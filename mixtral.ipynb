{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clarifai.client.model import Model\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "clarifai_pat = os.getenv('CLARIFAI_PAT')\n",
    "OpenAI_API_KEY = os.getenv('OpenAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is a topic of much discussion and debate among experts. Here are a few possibilities:\n",
      "\n",
      "1. Advancements in machine learning and deep learning will continue to drive improvements in AI's ability to understand and interpret data, leading to more sophisticated and capable AI systems.\n",
      "2. AI will become more integrated into our daily lives, from home appliances and personal devices to transportation and the workplace.\n",
      "3. The development of \"explainable AI\" will become a priority, as there is a growing recognition of the need for AI systems to be transparent and understandable to human users.\n",
      "4. AI will play an increasingly important role in scientific research and discovery, as it can process and analyze vast amounts of data more quickly and accurately than humans.\n",
      "5. Ethical and societal implications of AI will continue to be a major focus of debate and discussion, as AI becomes more prevalent and powerful.\n",
      "6. There will be a growing need for regulations\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<s> [INST] What’s the futuree of AI? [/INST]\"\n",
    "\n",
    "inference_params = dict(temperature=0.7, max_tokens=200, top_k = 50, top_p= 0.95)\n",
    "\n",
    "# Model Predict\n",
    "model_prediction = Model(\"https://clarifai.com/mistralai/completion/models/mixtral-8x7B-Instruct-v0_1\").predict_by_bytes(prompt.encode(), input_type=\"text\", inference_params=inference_params)\n",
    "\n",
    "print(model_prediction.outputs[0].data.text.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teams. class of 2023:\n",
      "\n",
      "- josh harris\n",
      "- benjamin nelson\n",
      "- jordan pendley\n",
      "- caleb petersen\n",
      "- tanner robinson\n",
      "- jaxon schroeder\n",
      "- conor taylor\n",
      "- tyler wright\n",
      "- cameron zimmerman\n",
      "\n",
      "class of 2022:\n",
      "\n",
      "- andrew koppenhaver\n",
      "- jacob pendergast\n",
      "- brandon richardson\n",
      "- evan schluter\n",
      "- jacob shaw\n",
      "\n",
      "class of 2021:\n",
      "\n",
      "- andrew guthrie\n",
      "- jake hartman\n",
      "- jacob koppenhaver\n",
      "- austin mccarty\n",
      "- austin stueve\n",
      "- tyler stueve\n",
      "- kyle taylor\n",
      "\n",
      "class of 2020\n"
     ]
    }
   ],
   "source": [
    "prompt = \"soccer\"\n",
    "\n",
    "inference_params = dict(temperature=0.7, max_tokens=200, top_k = 50, top_p= 0.95)\n",
    "\n",
    "# Model Predict\n",
    "model_prediction = Model(\"https://clarifai.com/mistralai/completion/models/mixtral-8x7B-Instruct-v0_1\").predict_by_bytes(prompt.encode(), \n",
    "                                                                                                                         input_type=\"text\", \n",
    "                                                                                                                         inference_params=inference_params)\n",
    "\n",
    "print(model_prediction.outputs[0].data.text.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nST-MOE: DESIGNING STABLE AND TRANSFERABLE\\nSPARSE EXPERT MODELS\\n\\nBarret Zoph∗\\nGoogle Brain\\n\\nIrwan Bello∗†\\nGoogle Brain\\n\\nSameer Kumar\\nGoogle\\n\\nNan Du\\nGoogle Brain\\n\\nYanping Huang\\nGoogle Brain\\n\\nJeff Dean\\nGoogle Research\\n\\nNoam Shazeer†\\nGoogle Brain\\n\\nWilliam Fedus∗\\nGoogle Brain\\n\\nABSTRACT\\n\\nScale has opened new frontiers in natural language processing – but at a high cost.\\nIn response, Mixture-of-Experts (MoE) and Switch Transformers have been pro-\\nposed as an energy efficient path to even larger and more capable language models.\\nBut advancing the state-of-the-art across a broad set of natural language tasks has\\nbeen hindered by training instabilities and uncertain quality during fine-tuning.\\nOur work focuses on these issues and acts as a design guide. We conclude by\\nscaling a sparse model to 269B parameters, with a computational cost comparable\\nto a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-\\nof-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-\\nof-the-art performance in transfer learning, across a diverse set of tasks includ-\\ning reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum,\\nCNN-DM), closed book question answering (WebQA, Natural Questions), and\\nadversarially constructed tasks (Winogrande, ANLI R3). 1\\n\\n∗Equal contribution. Correspondence to {barretzoph,liamfedus}@google.com.\\n†Work was done while at Google.\\n1Code for our models is available at https://github.com/tensorflow/mesh/blob/master/\\n\\nmesh_tensorflow/transformer/moe.py\\n\\n1\\n\\nar\\nX\\n\\niv\\n:2\\n\\n20\\n2.\\n\\n08\\n90\\n\\n6v\\n2 \\n\\n [\\ncs\\n\\n.C\\nL\\n\\n] \\n 2\\n\\n9 \\nA\\n\\npr\\n 2\\n\\n02\\n2\\n\\nhttps://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py\\nhttps://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py\\n\\n\\nCONTENTS\\n\\n1 Introduction 3\\n2 Background 3\\n3 Stabilizing Training of Sparse Models 5\\n\\n3.1 Stability and Quality Tradeoffs when Removing Multiplicative Interactions . . . . 6\\n3.2 Stability and Quality Tradeoffs when Adding Noise . . . . . . . . . . . . . . . . . 6\\n3.3 Stability and Quality Tradeoffs when Constraining Activations and Gradients . . . 7\\n3.4 Selecting a Precision Format: Trading Efficiency and Stability . . . . . . . . . . . 8\\n\\n4 Fine-Tuning Performance of Sparse Models 9\\n4.1 Hypothesis: A Generalization Problem . . . . . . . . . . . . . . . . . . . . . . . . 9\\n4.2 Fine-Tuning a Subset of Model Parameters to Improve Generalization . . . . . . . 11\\n4.3 Sparse and Dense Models Require Different Fine-Tuning Protocols . . . . . . . . . 11\\n4.4 Sparse Models Are Robust to Dropped Tokens During Fine-Tuning . . . . . . . . . 12\\n4.5 Inserting Sentinels Tokens During Fine-Tuning . . . . . . . . . . . . . . . . . . . 13\\n\\n5 Designing Sparse Models 13\\n5.1 Setting the Number of Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n5.2 Choosing the Capacity Factor and Routing Algorithm . . . . . . . . . . . . . . . . 14\\n\\n6 Experimental Results 16\\n6.1 ST-MoE-L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n6.2 ST-MoE-32B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n\\n7 Tracing Tokens Through the Model 19\\n7.1 Encoder Experts Exhibit Specialization . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.2 Decoder Experts Lack Specialization . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.3 Multilingual Experts Specialize, But Not by Language . . . . . . . . . . . . . . . 21\\n\\n8 Related Work 21\\n9 Discussion 22\\n10 Conclusion 24\\nA Token Load Balance Description 31\\nB Router Z-Loss Training Dynamics 31\\nC Improved Architectural Modifications 32\\nD Batch Prioritized Routing for Lower Capacity Factors 33\\nE Pre-Training Dataset Details 34\\nF Full Fine-tuning Sensitivity Data 35\\nG Optimally Setting the Routing Threshold 36\\nH Mesh Layout for Data, Model and Expert Parallelism with Few Experts 36\\nI Note on Communication Costs for Distributed Models 37\\nJ Negative Results 38\\n\\n2\\n\\n\\n\\n1 INTRODUCTION\\n\\nSparse expert neural networks showcase the advantage of sheer scale and offer an efficient alternative\\nto the static neural network architectures commonly used today (Raffel et al., 2019; Brown et al.,\\n2020; Rae et al., 2021). Rather than applying the same parameters to all inputs, sparse expert\\nnetworks dynamically select which parameters to use for each input (Shazeer et al., 2017). This\\nallows for networks to vastly expand their number of parameters, while keeping the FLOPs per\\ntoken roughly constant. These approaches have yielded state-of-the-art translation models (Lepikhin\\net al., 2020), 4-7x pre-training speed-ups (Fedus et al., 2021; Artetxe et al., 2021), and GPT-3 level\\none-shot performance using 1/3 the energy training cost (Du et al., 2021). And despite a shocking\\nnumber of parameters, sparse models reduce the carbon footprint for training large neural networks\\nby an order of magnitude (Patterson et al., 2021). However, difficulties remain.\\n\\nFedus et al. (2021) observed that a sparse 1.6T parameter model achieved a 4x pre-training speed-up\\nover the prior state-of-the-art (Raffel et al., 2019), but lagged smaller models when fine-tuned on\\ncommon benchmarks like SuperGLUE. Similar gaps were observed in Artetxe et al. (2021) when\\nMoE language models were fine-tuned on out-of-domain data. In response, Switch-XXL, a model\\nwith fewer parameters, but a 8x-larger computational footprint (FLOPs approximately equal to the\\nlargest T5 model), was proposed and improved quality on natural language understanding tasks.\\nHowever, necessary pre-training was hampered by training instabilities previously undetected during\\nsmaller scale studies. These instabilities were later identified in other sparse models (Du et al., 2021).\\nThese results revealed a necessary balance of parameters and computation, but left an open question\\non how to reliably train these types of models.\\n\\nOur aim in this paper is to increase the practicality and reliability of sparse models. We study these\\ntwo issues and pre-train a 269B sparse model that achieves state-of-the-art results when fine-tuned\\nacross many competitive NLP benchmarks, including SuperGLUE. We also put forth additional\\nanalysis and a design guide (or at least, our heuristics) for sparse expert models. Furthermore, this\\nwork emphasizes jointly optimizing both the upstream pre-training and the downstream fine-tuning\\nmetrics to avoid discrepancies (Tay et al., 2021).\\n\\nContributions\\n\\n1. A large-scale study of the quality-stability trade-offs of stability techniques.\\n2. An introduction of the router z-loss that resolves instability issues, while slightly improv-\\n\\ning model quality.\\n3. A fine-tuning analysis of sparse and dense models highlighting different hyperparameter\\n\\nsensitivity to the batch size and learning rate. We show bad hyperparameters result in\\nvirtually no fine-tuning gain over dense models, despite large pre-training speed-ups.\\n\\n4. Architectural, routing and model design principles for designing Pareto efficient sparse\\nmodels in a distributed setting.\\n\\n5. A qualitative analysis tracing token routing decisions across expert layers.\\n6. A 269B sparse model (the Stable Transferable Mixture-of-Experts or ST-MoE-32B)\\n\\nwhich achieves state-of-the-art performance across a diverse set of natural language\\nbenchmarks.\\n\\n2 BACKGROUND\\n\\nSparse expert models typically substitute a neural network layer with a set of experts, each having\\nunique weights (Jacobs et al., 1991; Jordan and Jacobs, 1994). Typically all the experts within a layer\\nare of the same type and shape (homogeneous), however, varied (heterogeneous) expert-types are\\npossible. Inputs are only processed by a subset of the experts to save computation, so a mechanism\\nmust be added to determine where to send each input. Usually a router or gating network determines\\nwhere to send inputs (i.e. words, sentences, image patches, etc.), but alternative schemes have been\\nproposed (Lewis et al., 2021; Roller et al., 2021; Zuo et al., 2021; Clark et al., 2022).\\n\\n3\\n\\n\\n\\nSpecifically, in natural language processing, Shazeer et al. (2017) proposed a Mixture-of-Experts\\n(MoE) layer which takes a token representation x as input and routes it to the best matched top-\\nk experts selected out of a set {Ei(x)}Ni=1 of N experts. The router variable Wr produces logits\\nh(x) = Wr · x which are normalized via a softmax distribution over the available N experts at that\\nlayer. The gate-value for expert i is given by\\n\\npi(x) =\\neh(x)i∑N\\nj eh(x)j\\n\\n(1)\\n\\nand the token x is routed to the experts with the highest top-k gate values (set of indices T ). The\\noutput of the layer is the weighted sum of each expert’s computation by the gate value\\n\\ny =\\n∑\\ni∈T\\n\\npi(x)Ei(x) (2)\\n\\nOriginally proposed in LSTMs (Hochreiter and Schmidhuber, 1997), expert layers were later used in\\nthe Transformer (Vaswani et al., 2017) by Shazeer et al. (2018) and Lepikhin et al. (2020). Follow-\\non work by Fedus et al. (2021) simplified the MoE further to route tokens to a single expert (top-1)\\nand reduced other costs to improve training efficiency.\\n\\nTo improve hardware utilization, most implementations of sparse models have static batch sizes\\nfor each expert (Shazeer et al., 2017; 2018; Lepikhin et al., 2020; Fedus et al., 2021). The expert\\ncapacity refers to the number of tokens that can be routed to each expert. If this capacity is exceeded\\n(the router sends too many inputs to that expert) then the overflowed tokens have no computation\\napplied to them and are passed to the next layer through a residual connection.\\n\\nTerminology Definition\\n\\nExpert An independently-learned neural network with unique weights.\\n\\nRouter A network that computes the probability of each token getting sent to each\\nexpert.\\n\\nTop-n Routing Routing algorithm where each token is routed to n experts.\\n\\nLoad Balancing Loss An auxiliary (aux) loss to encourage each group of tokens to evenly distribute\\nacross experts.\\n\\nGroup Size The global batch size is split into smaller groups, each of size Group Size. Each\\ngroup is considered separately for load balancing across experts. Increasing it\\nincreases memory, computation, and communication.\\n\\nCapacity Factor (CF) Each expert can only process up to a fixed number of tokens, which is often\\nset by evenly dividing across experts, tokens\\n\\nexperts . The capacity factor can expand or\\ncontract this amount to CF · tokens\\n\\nexperts .\\n\\nFFN Acronym of Feed Forward Network (FFN) layer of Transformer consisting of\\nlinear, activation, linear.\\n\\nEncoder-Decoder A Transformer architectural variant that all of our models are based on. Consists\\nof an encoder that does all-to-all attention on the inputs and a decoder that\\nattends to the encoder and to its own inputs in an autoregressive manner.\\n\\nallreduce Communication primitive which sums a subset of n tensors on n different de-\\nvices, then broadcasts the summed value to all n devices. This is used in dis-\\ntributed training for gradient accumulation and model parallelism.\\n\\nall2all Communication primitive where each device sends to every other device a part\\nof its tensor. Used in sparse Transformer models for token routing.\\n\\n(↑/↓) Indicates whether higher/lower values are better (e.g. accuracy/train loss).\\n\\nTable 1: Terminology used throughout the paper.\\n\\nThe batch B of input tokens is broken into G unique groups across the data-parallelism dimension2,\\neach with size B/G. The expert capacity is equal to CF · tokens/experts where CF represents the\\n\\n2Our implementation relies on einsums with one-hot tensors for dispatching and combining tensors to/from\\nexperts. The size of this one-hot tensor grows quadratically with the number of tokens being routed as a group\\nwhich motivates breaking the batch into smaller groups. This may be avoided with sparse lookup operations.\\n\\n4\\n\\n\\n\\ncapacity factor hyperparameter, experts is the number of experts and tokens is the group size. If the\\ncapacity factor is increased, it creates extra buffer so that fewer tokens will be dropped in case of load\\nimbalance. However, increasing the capacity factor also increases the memory and computational\\ncosts, so there exists a trade off3.\\n\\nFinally, an auxiliary load balancing loss encourages tokens to be roughly evenly distributed across\\nthe experts (Shazeer et al., 2017). This improves the hardware efficiency by ensuring that all accel-\\nerators are processing significant chunks of data in parallel as mentioned above. The details of the\\nloss are presented in Appendix A. However, alternatives exist: Lewis et al. (2021) and Clark et al.\\n(2022) treats balanced token allocation as an assignment problem and removes the auxiliary loss\\nentirely.\\n\\n3 STABILIZING TRAINING OF SPARSE MODELS\\n\\nSparse models often suffer from training instabilities (Figure 1) worse than those observed in stan-\\ndard densely-activated Transformers.\\n\\n0 2500 5000 7500 10000 12500 15000\\nStep\\n\\n0\\n\\n50\\n\\n100\\n\\n150\\n\\n200\\n\\n250\\n\\n300\\n\\n350\\n\\nTr\\nai\\n\\nni\\nng\\n\\n L\\nos\\n\\ns\\n\\n0 2500 5000 7500 10000 12500 15000\\nStep\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\nTr\\nai\\n\\nni\\nng\\n\\n L\\nos\\n\\ns\\n\\nFigure 1: Training instabilities for sparse models. We refer to training instabilities as divergences\\nin the training loss. Above are two runs from sparse models FLOP-matched to the T5-XL version\\n(Raffel et al., 2019) each trained with a batch size of 1M tokens using the Adafactor optimizer\\n(Shazeer and Stern, 2018). (Left) An unstable training run. (Right) A stable training run.\\n\\nIt’s straightforward to find changes that improve the stability, however, these often come at an un-\\ntenable expense to model quality (for instance, using an arbitrarily small learning rate or using tight\\ngradient clipping). We categorize and examine several approaches to improve stability. The sta-\\nbility techniques span generic fixes to Transformers as well as those specific to sparse models: (1)\\nRemove multiplicative interactions (2) Inject model noise (3) Constrain activations, and gradients.\\nWe conclude with our recommendation: a new auxiliary loss, the router z-loss, which significantly\\nimproves training stability with no quality degradation. This is an adaptation of the z-loss used for\\nfinal softmax logits in the Mesh Tensorflow codebase (Shazeer et al., 2018).\\n\\nStabilizing Sparse Models\\n\\n1. Many methods stabilize sparse models, but at the expense of worse quality.\\n2. The router z-loss stabilizes models without quality degradation.\\n3. Transformer modifications with more multiplicative components (GEGLU, RMS nor-\\n\\nmalization) worsen stability, but boost quality.\\n\\nDesigning a large-scale stability study. We design a large-scale stability study of sparse models\\nFLOP-matched to the T5-XL version (Raffel et al., 2019) pre-trained on the multilingual corpus\\nmC4 (Xue et al., 2020). Each sparse model has 32 experts and we introduce a sparse MoE layer for\\n\\n3See Fedus et al. (2021) for a graphical illustration of how the capacity factor works.\\n\\n5\\n\\n\\n\\nevery fourth FFN. The train capacity factor is 1.25 and the eval capacity factor is 2.0. See Table 11\\nfor a more detailed description of models used throughout this paper. For each stability technique,\\nwe record the fraction that are stable, the mean quality (negative log perplexity on English), and the\\nstandard deviation over seeds.\\n\\nThe primary issue in constructing this study is that small models are rarely unstable but large un-\\nstable models are too costly to run for sufficient steps and seeds. We found a sparse model FLOP-\\nmatched to T5-XL to be good object of study because it was unstable roughly 1/3 of the runs, but\\nwas still relatively cheap to train. Furthermore, we run our instability experiments on multilingual\\ndata since we find this exacerbates model instabilities, allowing us to experiment on slightly smaller\\nmodels. See Section 9 for more details. Our baseline configuration is trained using six random seeds\\nand each configuration with a stability technique uses three random seeds. We use six seeds for the\\nbaseline to better characterize the instability rate and three seeds for the variants to save compute.\\nEach model is pre-trained for 20k steps on mC4 using a masked language modeling objective (Fedus\\net al., 2018; Devlin et al., 2018).\\n\\n3.1 STABILITY AND QUALITY TRADEOFFS WHEN REMOVING MULTIPLICATIVE\\nINTERACTIONS\\n\\nSome architectural improvements involve more multiplications than additions or do not sum many\\nitems at once. For example, a matrix multiplication has one multiplication for each addition and\\nhence we do not refer to it as a “multiplicative” operation. We present and analyze the impact of\\ntwo instances of multiplicative interactions in Transformers here.\\n\\nGELU Gated Linear Units (GEGLU). Our first example is the Gated Linear Unit (Dauphin\\net al., 2017) which is a component-wise product of two linear projections, one of which is first\\npassed through a sigmoid function. Shazeer (2020) extends this to other variants and presents a\\nGELU-Linear (Hendrycks and Gimpel, 2016) FFN layer as a replacement the usual ReLU (Nair and\\nHinton, 2010) FFN in Transformer.\\n\\nFFNGEGLU (x,W, V, b, c) = GELU(xW + b)� (xV + c) (3)\\n\\nThis quality gain was corroborated in later work (Narang et al., 2021).\\n\\nRoot Mean Square Scale Parameters. Our second example is the scale parameter in root mean\\nsquare (RMS) normalization (Zhang and Sennrich, 2019). Within the Transformer, rather than call-\\ning layers back-to-back, there is an internal structure (referred to as sublayer calls) which improve\\ngradient propagation and training dynamics. Our sublayer calls match that of Raffel et al. (2019)\\nand consist of: (1) RMS normalization, (2) layer call (e.g. Self Attention), (3) dropout (Srivastava\\net al., 2014), (4) add residual (He et al., 2015). RMS normalization scales the input vector x ∈ Rd\\n\\nelement-wise per the root-mean-square. It then rescales the output element-wise by multiplying with\\na learned scale parameter g.\\n\\nyi =\\nxi√\\n\\n1\\nd\\n\\n∑d\\ni=1 x\\n\\n2\\ni\\n\\n· gi (4)\\n\\nTable 2 shows that both removing GEGLU layers or the RMS scale parameter improves stability, but\\nat a significant loss to model quality. We note that these scale parameters (g) have a disproportionate\\ngain to model quality versus parameters elsewhere (e.g. FFN). In line with our findings, Shleifer\\net al. (2021) found adding a learned multiplicative scalar to the residual connection in Transformers\\nmade them much more unstable.\\n\\nIn Appendix C, we further study the quality impact of adding new multiplicative interactions in\\nexpert layers. We find that this operation yields quality improvements with virtually no slow-down\\nin model step time.\\n\\n3.2 STABILITY AND QUALITY TRADEOFFS WHEN ADDING NOISE\\n\\nWe next explore a hypothesis that adding noise into the model can improve training stability (Nee-\\nlakantan et al., 2015). Taleb (2012) argues that certain systems exhibit the property of anti-fragility,\\nwhere they improve through noise. Inspired by the concept and by our observation that fine-tuning\\n\\n6\\n\\n\\n\\nMethod Fraction Stable Quality (↑)\\nBaseline 4/6 -1.755 ±0.02\\n\\nRemove GEGLU 3/3 -1.849 ±0.02\\nRemove RMS Norm. Scale Param 3/3 -2.020 ±0.06\\n\\nTable 2: Removing operations with more multiplicative interactions. Multiplicative interactions\\nimprove quality, but can destabilize training. Individually removing two sources of multiplicative\\ncomponents improves the stability, but worsens quality significantly. When we remove the GEGLU\\nlayer, we replace it with with an equivalent Dense-ReLU-Dense layer to match the FLOPs and\\nparameters.\\n\\n(which injects noise via dropout) was rarely unstable, we examined whether training noise might\\nimprove the stability of sparse models. Table 3 shows a stability improvement versus the baseline,\\nbut at the expense of lower quality. We also find that input-jitter, introduced by Fedus et al. (2021),\\ndiminishes quality at XL-scale, hence we ablate it in our models. Input-jitter multiplies the input\\nlogits to the router by a uniform random variable between [1 − 10−2, 1 + 10−2] . Dropout in our\\nablation is applied throughout the Transformer. As seen previously, improvements in small-scale\\nsettings may fail to generalize when scaled up and therefore trends should always be monitored and\\nre-assessed at increasing scale (Kaplan et al., 2020).\\n\\nMethod Fraction Stable Quality (↑)\\nBaseline 4/6 -1.755 ±0.02\\n\\nInput jitter (10−2) 3/3 -1.777 ±0.03\\nDropout (0.1) 3/3 -1.822 ±0.11\\n\\nTable 3: Injecting noise during training. Both input-jitter and dropout improve stability, but lead to\\na significant loss of model quality. There is a clear tradeoff with most methods: when one improves\\nstability, it then typically decreases model quality. Our work aims to find methods that fix stability\\nwithout hurting quality.\\n\\n3.3 STABILITY AND QUALITY TRADEOFFS WHEN CONSTRAINING ACTIVATIONS AND\\nGRADIENTS\\n\\nOne of the most successful approaches to stabilizing neural networks are constraints on activations,\\nand gradients (Pascanu et al., 2013; Ioffe and Szegedy, 2015; Salimans and Kingma, 2016; Ba et al.,\\n2016). A popular approach consists in the clipping of gradient norms to remedy exploding gradients\\nwhile backpropagating through deep networks (Pascanu et al., 2013).\\n\\nIn this work, we use the Adafactor optimizer due to its memory efficiency (though recently in-\\ntroduced 8-bit optimizers (Dettmers et al., 2021) may offer better trade-offs). Instead of gradient\\nclipping, Adafactor uses update clipping, where the changes to the weights are constrained to be\\nbelow a certain norm. We experiment with tightening the update clipping to a smaller value.\\n\\nNext, we study constraints on the logits going into the router. The router computes the probability\\ndistribution over the experts in float32 precision (i.e. selective precision) (Fedus et al., 2021).\\nHowever, at the largest scales, we find this is insufficient to yield reliable training. To fix this, we\\nintroduce the router z-loss,\\n\\nLz(x) =\\n1\\n\\nB\\n\\nB∑\\ni=1\\n\\n\\uf8eb\\uf8edlog\\n\\nN∑\\nj=1\\n\\nex\\n(i)\\nj\\n\\n\\uf8f6\\uf8f82\\n\\n(5)\\n\\nwhere B is the number of tokens, N is the number of experts, and x ∈ RB×N are the logits going\\ninto the router. This penalizes large logits into the gating network and Section 3.4 contains a more\\ndetailed explanation of why the z-loss before the router is useful.\\n\\n7\\n\\n\\n\\nTable 4 shows that both update clipping and the router z-loss stabilize the model in all 3 runs, but the\\nupdate clipping significantly hurts the model quality. Therefore we use the z-loss method for fixing\\nour model stability due to improved quality and stability4.\\n\\nMethod Fraction Stable Quality (↑)\\nBaseline 4/6 -1.755 ±0.02\\n\\nUpdate clipping (clip = 0.1) 3/3 -4.206 ±0.17\\nRouter Z-Loss 3/3 -1.741 ±0.02\\n\\nTable 4: Constraining weight updates and router logits. Constraining the update clipping in\\nAdafactor improves stability, but at a catastrophic loss of quality. Looser clipping values did not\\nreliably stabilize training so we exclude them here. The router z-loss stabilizes the model without\\nany quality degradation (in this case, we observe a slight quality boost).\\n\\nThe router z-loss introduces another hyperparameter (cz), which is the coefficient to weight this as\\npart of the total loss optimized. The total loss is a linearly weighted combination of the cross entropy\\nloss (LCE), the auxiliary load balance loss (LB), and the router z-loss (LZ), yielding a total loss\\n\\nLtot = LCE + cBLB + czLZ (6)\\n\\nWe choose a value of cz = 0.001 based on the best model quality after pre-training with a hyperpa-\\nrameter sweep. Appendix B logs the resulting losses over the course of pre-training.\\n\\n3.4 SELECTING A PRECISION FORMAT: TRADING EFFICIENCY AND STABILITY\\n\\nAs in most modern distributed Transformers we train with mixed precision (Micikevicius et al.,\\n2017) 5. Weights are stored in float32 for gradient updates and then converted to bfloat16\\nwhen doing matrix multiplications in the forward and backward pass6. Furthermore, all activations\\nare stored and operated on in bfloat16 and allreduce communications can be done in either\\nbfloat16 or float32 numerical precision. For the largest model explored in this work (ST-\\nMoE-32B presented later) we find speed-ups halving the numerical precision of the allreduce,\\nhowever this also can destabilize the training so we keep this as float32 throughout this work.\\n\\nA lower precision format enables more efficient models by reducing (a) communication costs be-\\ntween processors and memory, (b) computation costs, (c) memory for storing tensors (e.g. activa-\\ntions). However, lower precision formats come at the expense of larger roundoff errors which can\\nlead to irrecoverable training instabilities.\\n\\nMantissa (7 bits)Exponent (8 bits)\\n\\nExponent (8 bits) Mantissa (23 bits)\\n\\nPrecision Format: Float32\\n\\nPrecision Format: BFloat16\\n\\nNumber Range\\nMax BFloat16\\nRoundoff Error\\n\\nMax Float32\\nRoundoff Error\\n\\n[2, 4) 0.01563 2.34x10^(-7)\\n\\n[32, 64) 0.25 3.81x10^(-6)\\n\\n[1024, 2048) 8.0 0.00012\\n\\n[2^20, 2^21) 8192.0 0.125\\n\\n[2^30, 2^31) 8288608.0 128.0\\n\\nFigure 2: Numerical precision formats and roundoff errors. Larger numbers have larger roundoff\\nerrors. bfloat16 has up to 65,536x worse roundoff errors than float32. The router z-loss\\nencourages the absolute magnitude of numbers to be small, which doesn’t hinder model performance\\nand reduces roundoff errors. The router z-loss is most effective into functions where larger errors\\ncan drastically change the relative output (e.g. exponential and sinusoidal functions).\\n\\n4We also experimented with adding z-losses onto the attention logits which also improves model instability\\nwithout hurting model quality.\\n\\n5See Mesh Tensorflow for implementation details: https://github.com/tensorflow/mesh/\\nblob/master/mesh_tensorflow/\\n\\n6Matrix multiplications on TPUs perform multiplications in bfloat16 and accumulations in float32.\\n\\n8\\n\\nhttps://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/\\nhttps://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/\\n\\n\\nUnderstanding precision format and roundoff errors. Figure 2 reviews the properties of differ-\\nent precision formats and their corresponding roundoff errors for different number ranges. Numbers\\nin any range of two consecutive powers of 2 (e.g. [2,4) and [1024, 2048)) are represented by a fixed\\nnumber of mantissa bits (7 for bfloat16, 23 for float32). As a result, (1) bfloat16 will\\nhave about 65,536x (i.e. 23 − 7 = 16 additional bits and 216 = 65536) as large roundoff errors as\\nfloat32 and (2) larger numbers have larger roundoff errors. Due to the 8 exponent bits, number\\ncan get as large as ≈ 3e38, which leads to even float32 having some issues with roundoff errors.\\n\\nSparse expert models are sensitive to roundoff errors because they have more exponential\\nfunctions due to the routers. Sparse expert models introduce additional exponential functions –\\nthrough the router – which can exacerbate roundoff errors7 and lead to training instabilities. While\\na roundoff error does not change the ordering of probabilities within a softmax operation, it does\\nimpact the routing of the second token in MoE due to relative thresholding (e.g. a token is only\\nrouted to its second place expert if the gating probability for the second expert is 1/5 as large as\\nthat of the first expert). Additionally, roundoff errors can drastically change the probability that\\nscales the expert output – which we have found to be important. Finally, we conjecture that the\\nhigher stability we observed for decoder-only models (not shown here) was because they had fewer\\nexponential functions. Section 9 contains a more detailed discussion.\\n\\nAn aside on the router z-loss. One might think that the router z-loss is a convoluted method\\nreplaceable by clipping logits (Wu et al., 2016). We explain why this is not the case. The goal is to\\nminimize large roundoff errors going into exponential functions. Clipping the logits occurs after any\\nroundoff errors – resulting in even larger discontinuities. In one view, clipping in itself is a roundoff\\nerror; conversely, the z-loss naturally encourages the model to produce logits that are small in value\\nand thus more accurately modeled. Due to these dynamics, we ensure all exponentiated tensors are\\ncast to float32. This hints at the possibility of better number formats for neural networks because\\nof the unused exponent bits when z-losses are added throughout the network (see Section 9).\\n\\n4 FINE-TUNING PERFORMANCE OF SPARSE MODELS\\n\\nThe best performing language models are usually obtained by (1) pre-training on large amounts of\\ndata (e.g. the internet) followed by (2) fine-tuning on a task of interest (e.g. SuperGLUE). Promis-\\ning new techniques have emerged as an alternative, including few-shot inference (Brown et al.,\\n2020), prefix tuning (Li and Liang, 2021), prompt tuning (Lester et al., 2021), and adapter modules\\n(Houlsby et al., 2019) – however, a quality gap still persists compared to fine-tuning. Because of\\nthis, we focus on fine-tuning in this work, but highlight recent successes of sparse models in few-\\nshot settings from Du et al. (2021); Artetxe et al. (2021). Further, we leave as future work techniques\\nthat adapt large language models through reinforcement learning (Ouyang et al., 2022)\\n\\n4.1 HYPOTHESIS: A GENERALIZATION PROBLEM\\n\\nSparse models have performed remarkably well in the regime of large datasets, but have sometimes\\nperformed poorly when fine-tuning (Fedus et al., 2021; Artetxe et al., 2021). We present evidence\\nfor a (not so surprising) hypothesis that sparse models are prone to overfitting. We illustrate this\\nproblem through two tasks in SuperGLUE (Wang et al., 2019) – Commitment Bank (De Marneffe\\net al., 2019) and ReCORD (Zhang et al., 2018). Commitment Bank (CB) has 250 training examples\\nwhile ReCORD has over 100,000. This significant size discrepancy facilitates a natural study for\\noverfitting on two tasks selected as part of the same benchmark.\\n\\nIn Figure 3, we compare the fine-tuning characteristics of the Dense L and the ST-MoE-L model.\\nEach model was pre-trained on 500B tokens from the C4 corpus (Raffel et al., 2019). The models\\n\\n7Exponential functions have the property that a small input perturbation can lead to a large difference in\\nthe output. As an example, con'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tika import parser # pip install tika\n",
    "\n",
    "def get_pdf(path):\n",
    "    raw = parser.from_file(temp_file_path)\n",
    "    return(raw['content'])\n",
    "\n",
    "\n",
    "temp_file_path = 'paper.pdf'\n",
    "pdf=get_pdf(temp_file_path)\n",
    "pdf\n",
    "len(pdf[:30000])\n",
    "pdf[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ST-M OE: D ESIGNING STABLE AND TRANSFERABLE\\nSPARSE EXPERT MODELS\\nBarret Zoph\\x03\\nGoogle BrainIrwan Bello\\x03y\\nGoogle Brain\\nSameer Kumar\\nGoogleNan Du\\nGoogle BrainYanping Huang\\nGoogle Brain\\nJeff Dean\\nGoogle ResearchNoam Shazeery\\nGoogle BrainWilliam Fedus\\x03\\nGoogle Brain\\nABSTRACT\\nScale has opened new frontiers in natural language processing – but at a high cost.\\nIn response, Mixture-of-Experts (MoE) and Switch Transformers have been pro-\\nposed as an energy efﬁcient path to even larger and more capable language models.\\nBut advancing the state-of-the-art across a broad set of natural language tasks has\\nbeen hindered by training instabilities and uncertain quality during ﬁne-tuning.\\nOur work focuses on these issues and acts as a design guide. We conclude by\\nscaling a sparse model to 269B parameters, with a computational cost comparable\\nto a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-\\nof-Experts or ST-MoE-32B). For the ﬁrst time, a sparse model achieves state-\\nof-the-art performance in transfer learning, across a diverse set of tasks includ-\\ning reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum,\\nCNN-DM), closed book question answering (WebQA, Natural Questions), and\\nadversarially constructed tasks (Winogrande, ANLI R3).1\\n\\x03Equal contribution. Correspondence to fbarretzoph,liamfedus g@google.com .\\nyWork was done while at Google.\\n1Code for our models is available at https://github.com/tensorflow/mesh/blob/master/\\nmesh_tensorflow/transformer/moe.py\\n1arXiv:2202.08906v2  [cs.CL]  29 Apr 2022\\nCONTENTS\\n1 Introduction 3\\n2 Background 3\\n3 Stabilizing Training of Sparse Models 5\\n3.1 Stability and Quality Tradeoffs when Removing Multiplicative Interactions . . . . 6\\n3.2 Stability and Quality Tradeoffs when Adding Noise . . . . . . . . . . . . . . . . . 6\\n3.3 Stability and Quality Tradeoffs when Constraining Activations and Gradients . . . 7\\n3.4 Selecting a Precision Format: Trading Efﬁciency and Stability . . . . . . . . . . . 8\\n4 Fine-Tuning Performance of Sparse Models 9\\n4.1 Hypothesis: A Generalization Problem . . . . . . . . . . . . . . . . . . . . . . . . 9\\n4.2 Fine-Tuning a Subset of Model Parameters to Improve Generalization . . . . . . . 11\\n4.3 Sparse and Dense Models Require Different Fine-Tuning Protocols . . . . . . . . . 11\\n4.4 Sparse Models Are Robust to Dropped Tokens During Fine-Tuning . . . . . . . . . 12\\n4.5 Inserting Sentinels Tokens During Fine-Tuning . . . . . . . . . . . . . . . . . . . 13\\n5 Designing Sparse Models 13\\n5.1 Setting the Number of Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n5.2 Choosing the Capacity Factor and Routing Algorithm . . . . . . . . . . . . . . . . 14\\n6 Experimental Results 16\\n6.1 ST-MoE-L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n6.2 ST-MoE-32B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n7 Tracing Tokens Through the Model 19\\n7.1 Encoder Experts Exhibit Specialization . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.2 Decoder Experts Lack Specialization . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.3 Multilingual Experts Specialize, But Not by Language . . . . . . . . . . . . . . . 21\\n8 Related Work 21\\n9 Discussion 22\\n10 Conclusion 24\\nA Token Load Balance Description 31\\nB Router Z-Loss Training Dynamics 31\\nC Improved Architectural Modiﬁcations 32\\nD Batch Prioritized Routing for Lower Capacity Factors 33\\nE Pre-Training Dataset Details 34\\nF Full Fine-tuning Sensitivity Data 35\\nG Optimally Setting the Routing Threshold 36\\nH Mesh Layout for Data, Model and Expert Parallelism with Few Experts 36\\nI Note on Communication Costs for Distributed Models 37\\nJ Negative Results 38\\n2\\n1 I NTRODUCTION\\nSparse expert neural networks showcase the advantage of sheer scale and offer an efﬁcient alternative\\nto the static neural network architectures commonly used today (Raffel et al., 2019; Brown et al.,\\n2020; Rae et al., 2021). Rather than applying the same parameters to all inputs, sparse expert\\nnetworks dynamically select which parameters to use for each input (Shazeer et al., 2017). This\\nallows for networks to vastly expand their number of parameters, while keeping the FLOPs per\\ntoken roughly constant. These approaches have yielded state-of-the-art translation models (Lepikhin\\net al., 2020), 4-7x pre-training speed-ups (Fedus et al., 2021; Artetxe et al., 2021), and GPT-3 level\\none-shot performance using 1/3 the energy training cost (Du et al., 2021). And despite a shocking\\nnumber of parameters, sparse models reduce the carbon footprint for training large neural networks\\nby an order of magnitude (Patterson et al., 2021). However, difﬁculties remain.\\nFedus et al. (2021) observed that a sparse 1.6T parameter model achieved a 4x pre-training speed-up\\nover the prior state-of-the-art (Raffel et al., 2019), but lagged smaller models when ﬁne-tuned on\\ncommon benchmarks like SuperGLUE. Similar gaps were observed in Artetxe et al. (2021) when\\nMoE language models were ﬁne-tuned on out-of-domain data. In response, Switch-XXL, a model\\nwith fewer parameters, but a 8x-larger computational footprint (FLOPs approximately equal to the\\nlargest T5 model), was proposed and improved quality on natural language understanding tasks.\\nHowever, necessary pre-training was hampered by training instabilities previously undetected during\\nsmaller scale studies. These instabilities were later identiﬁed in other sparse models (Du et al., 2021).\\nThese results revealed a necessary balance of parameters andcomputation, but left an open question\\non how to reliably train these types of models.\\nOur aim in this paper is to increase the practicality andreliability of sparse models. We study these\\ntwo issues and pre-train a 269B sparse model that achieves state-of-the-art results when ﬁne-tuned\\nacross many competitive NLP benchmarks, including SuperGLUE. We also put forth additional\\nanalysis and a design guide (or at least, our heuristics) for sparse expert models. Furthermore, this\\nwork emphasizes jointly optimizing both the upstream pre-training and the downstream ﬁne-tuning\\nmetrics to avoid discrepancies (Tay et al., 2021).\\nContributions\\n1.A large-scale study of the quality-stability trade-offs of stability techniques.\\n2.An introduction of the router z-loss that resolves instability issues, while slightly improv-\\ning model quality.\\n3.A ﬁne-tuning analysis of sparse and dense models highlighting different hyperparameter\\nsensitivity to the batch size and learning rate. We show bad hyperparameters result in\\nvirtually no ﬁne-tuning gain over dense models, despite large pre-training speed-ups.\\n4.Architectural, routing and model design principles for designing Pareto efﬁcient sparse\\nmodels in a distributed setting.\\n5.A qualitative analysis tracing token routing decisions across expert layers.\\n6.A 269B sparse model (the Stable Transferable Mixture-of-Experts or ST-MoE-32B)\\nwhich achieves state-of-the-art performance across a diverse set of natural language\\nbenchmarks.\\n2 B ACKGROUND\\nSparse expert models typically substitute a neural network layer with a set of experts , each having\\nunique weights (Jacobs et al., 1991; Jordan and Jacobs, 1994). Typically all the experts within a layer\\nare of the same type and shape (homogeneous), however, varied (heterogeneous) expert-types are\\npossible. Inputs are only processed by a subset of the experts to save computation, so a mechanism\\nmust be added to determine where to send each input. Usually a router or gating network determines\\nwhere to send inputs (i.e. words, sentences, image patches, etc.), but alternative schemes have been\\nproposed (Lewis et al., 2021; Roller et al., 2021; Zuo et al., 2021; Clark et al., 2022).\\n3\\nSpeciﬁcally, in natural language processing, Shazeer et al. (2017) proposed a Mixture-of-Experts\\n(MoE) layer which takes a token representation xas input and routes it to the best matched top-\\nkexperts selected out of a set fEi(x)gN\\ni=1ofNexperts. The router variable Wrproduces logits\\nh(x) =Wr\\x01xwhich are normalized via a softmax distribution over the available Nexperts at that\\nlayer. The gate-value for expert iis given by\\npi(x) =eh(x)i\\nPN\\njeh(x)j(1)\\nand the token xis routed to the experts with the highest top- kgate values (set of indices T). The\\noutput of the layer is the weighted sum of each expert’s computation by the gate value\\ny=X\\ni2Tpi(x)Ei(x) (2)\\nOriginally proposed in LSTMs (Hochreiter and Schmidhuber, 1997), expert layers were later used in\\nthe Transformer (Vaswani et al., 2017) by Shazeer et al. (2018) and Lepikhin et al. (2020). Follow-\\non work by Fedus et al. (2021) simpliﬁed the MoE further to route tokens to a single expert (top-1)\\nand reduced other costs to improve training efﬁciency.\\nTo improve hardware utilization, most implementations of sparse models have static batch sizes\\nfor each expert (Shazeer et al., 2017; 2018; Lepikhin et al., 2020; Fedus et al., 2021). The expert\\ncapacity refers to the number of tokens that can be routed to each expert. If this capacity is exceeded\\n(the router sends too many inputs to that expert) then the overﬂowed tokens have no computation\\napplied to them and are passed to the next layer through a residual connection.\\nTerminology Deﬁnition\\nExpert An independently-learned neural network with unique weights.\\nRouter A network that computes the probability of each token getting sent to each\\nexpert.\\nTop-nRouting Routing algorithm where each token is routed to nexperts.\\nLoad Balancing Loss An auxiliary (aux) loss to encourage each group of tokens to evenly distribute\\nacross experts.\\nGroup Size The global batch size is split into smaller groups, each of size Group Size. Each\\ngroup is considered separately for load balancing across experts. Increasing it\\nincreases memory, computation, and communication.\\nCapacity Factor (CF) Each expert can only process up to a ﬁxed number of tokens, which is often\\nset by evenly dividing across experts,tokens\\nexperts. The capacity factor can expand or\\ncontract this amount to CF \\x01tokens\\nexperts.\\nFFN Acronym of Feed Forward Network (FFN) layer of Transformer consisting of\\nlinear, activation, linear.\\nEncoder-Decoder A Transformer architectural variant that all of our models are based on. Consists\\nof an encoder that does all-to-all attention on the inputs and a decoder that\\nattends to the encoder and to its own inputs in an autoregressive manner.\\nallreduce Communication primitive which sums a subset of ntensors on ndifferent de-\\nvices, then broadcasts the summed value to all ndevices. This is used in dis-\\ntributed training for gradient accumulation and model parallelism.\\nall2all Communication primitive where each device sends to every other device a part\\nof its tensor. Used in sparse Transformer models for token routing.\\n(\"/#) Indicates whether higher/lower values are better (e.g. accuracy/train loss).\\nTable 1: Terminology used throughout the paper.\\nThe batchBof input tokens is broken into Gunique groups across the data-parallelism dimension2,\\neach with sizeB=G. The expert capacity is equal to CF \\x01tokens=experts where CF represents the\\n2Our implementation relies on einsums with one-hot tensors for dispatching and combining tensors to/from\\nexperts. The size of this one-hot tensor grows quadratically with the number of tokens being routed as a group\\nwhich motivates breaking the batch into smaller groups. This may be avoided with sparse lookup operations.\\n4\\ncapacity factor hyperparameter, experts is the number of experts and tokens is the group size. If the\\ncapacity factor is increased, it creates extra buffer so that fewer tokens will be dropped in case of load\\nimbalance. However, increasing the capacity factor also increases the memory and computational\\ncosts, so there exists a trade off3.\\nFinally, an auxiliary load balancing loss encourages tokens to be roughly evenly distributed across\\nthe experts (Shazeer et al., 2017). This improves the hardware efﬁciency by ensuring that all accel-\\nerators are processing signiﬁcant chunks of data in parallel as mentioned above. The details of the\\nloss are presented in Appendix A. However, alternatives exist: Lewis et al. (2021) and Clark et al.\\n(2022) treats balanced token allocation as an assignment problem and removes the auxiliary loss\\nentirely.\\n3 S TABILIZING TRAINING OF SPARSE MODELS\\nSparse models often suffer from training instabilities (Figure 1) worse than those observed in stan-\\ndard densely-activated Transformers.\\n0 2500 5000 7500 10000 12500 15000\\nStep050100150200250300350Training Loss\\n0 2500 5000 7500 10000 12500 15000\\nStep2345678Training Loss\\nFigure 1: Training instabilities for sparse models. We refer to training instabilities as divergences\\nin the training loss. Above are two runs from sparse models FLOP-matched to the T5-XL version\\n(Raffel et al., 2019) each trained with a batch size of 1M tokens using the Adafactor optimizer\\n(Shazeer and Stern, 2018). (Left) An unstable training run. (Right) A stable training run.\\nIt’s straightforward to ﬁnd changes that improve the stability , however, these often come at an un-\\ntenable expense to model quality (for instance, using an arbitrarily small learning rate or using tight\\ngradient clipping). We categorize and examine several approaches to improve stability. The sta-\\nbility techniques span generic ﬁxes to Transformers as well as those speciﬁc to sparse models: (1)\\nRemove multiplicative interactions (2)Inject model noise (3)Constrain activations, and gradients.\\nWe conclude with our recommendation: a new auxiliary loss, the router z-loss , which signiﬁcantly\\nimproves training stability with no quality degradation. This is an adaptation of the z-loss used for\\nﬁnal softmax logits in the Mesh Tensorﬂow codebase (Shazeer et al., 2018).\\nStabilizing Sparse Models\\n1.Many methods stabilize sparse models, but at the expense of worse quality.\\n2.The router z-loss stabilizes models without quality degradation.\\n3.Transformer modiﬁcations with more multiplicative components (GEGLU, RMS nor-\\nmalization) worsen stability, but boost quality.\\nDesigning a large-scale stability study. We design a large-scale stability study of sparse models\\nFLOP-matched to the T5-XL version (Raffel et al., 2019) pre-trained on the multilingual corpus\\nmC4 (Xue et al., 2020). Each sparse model has 32 experts and we introduce a sparse MoE layer for\\n3See Fedus et al. (2021) for a graphical illustration of how the capacity factor works.\\n5\\nevery fourth FFN. The train capacity factor is 1.25 and the eval capacity factor is 2.0. See Table 11\\nfor a more detailed description of models used throughout this paper. For each stability technique,\\nwe record the fraction that are stable, the mean quality (negative log perplexity on English), and the\\nstandard deviation over seeds.\\nThe primary issue in constructing this study is that small models are rarely unstable but large un-\\nstable models are too costly to run for sufﬁcient steps and seeds. We found a sparse model FLOP-\\nmatched to T5-XL to be good object of study because it was unstable roughly 1=3of the runs, but\\nwas still relatively cheap to train. Furthermore, we run our instability experiments on multilingual\\ndata since we ﬁnd this exacerbates model instabilities, allowing us to experiment on slightly smaller\\nmodels. See Section 9 for more details. Our baseline conﬁguration is trained using six random seeds\\nand each conﬁguration with a stability technique uses three random seeds. We use six seeds for the\\nbaseline to better characterize the instability rate and three seeds for the variants to save compute.\\nEach model is pre-trained for 20k steps on mC4 using a masked language modeling objective (Fedus\\net al., 2018; Devlin et al., 2018).\\n3.1 S TABILITY AND QUALITY TRADEOFFS WHEN REMOVING MULTIPLICATIVE\\nINTERACTIONS\\nSome architectural improvements involve more multiplications than additions or do not sum many\\nitems at once. For example, a matrix multiplication has one multiplication for each addition and\\nhence we do not refer to it as a “multiplicative” operation. We present and analyze the impact of\\ntwo instances of multiplicative interactions in Transformers here.\\nGELU Gated Linear Units (GEGLU). Our ﬁrst example is the Gated Linear Unit (Dauphin\\net al., 2017) which is a component-wise product of two linear projections, one of which is ﬁrst\\npassed through a sigmoid function. Shazeer (2020) extends this to other variants and presents a\\nGELU-Linear (Hendrycks and Gimpel, 2016) FFN layer as a replacement the usual ReLU (Nair and\\nHinton, 2010) FFN in Transformer.\\nFFN GEGLU (x;W;V;b;c ) =GELU (xW+b)\\x0c(xV+c) (3)\\nThis quality gain was corroborated in later work (Narang et al., 2021).\\nRoot Mean Square Scale Parameters. Our second example is the scale parameter in root mean\\nsquare (RMS) normalization (Zhang and Sennrich, 2019). Within the Transformer, rather than call-\\ning layers back-to-back, there is an internal structure (referred to as sublayer calls) which improve\\ngradient propagation and training dynamics. Our sublayer calls match that of Raffel et al. (2019)\\nand consist of: (1)RMS normalization, (2)layer call (e.g. Self Attention), (3)dropout (Srivastava\\net al., 2014), (4)add residual (He et al., 2015). RMS normalization scales the input vector x2Rd\\nelement-wise per the root-mean-square. It then rescales the output element-wise by multiplying with\\na learned scale parameter g.\\nyi=xiq\\n1\\ndPd\\ni=1x2\\ni\\x01gi (4)\\nTable 2 shows that both removing GEGLU layers or the RMS scale parameter improves stability, but\\nat a signiﬁcant loss to model quality. We note that these scale parameters ( g) have a disproportionate\\ngain to model quality versus parameters elsewhere (e.g. FFN). In line with our ﬁndings, Shleifer\\net al. (2021) found adding a learned multiplicative scalar to the residual connection in Transformers\\nmade them much more unstable.\\nIn Appendix C, we further study the quality impact of adding new multiplicative interactions in\\nexpert layers. We ﬁnd that this operation yields quality improvements with virtually no slow-down\\nin model step time.\\n3.2 S TABILITY AND QUALITY TRADEOFFS WHEN ADDING NOISE\\nWe next explore a hypothesis that adding noise into the model can improve training stability (Nee-\\nlakantan et al., 2015). Taleb (2012) argues that certain systems exhibit the property of anti-fragility,\\nwhere they improve through noise. Inspired by the concept and by our observation that ﬁne-tuning\\n6\\nMethod Fraction Stable Quality ( \")\\nBaseline 4=6 -1.755\\x060:02\\nRemove GEGLU 3=3 -1.849\\x060:02\\nRemove RMS Norm. Scale Param 3=3 -2.020\\x060:06\\nTable 2: Removing operations with more multiplicative interactions . Multiplicative interactions\\nimprove quality, but can destabilize training. Individually removing two sources of multiplicative\\ncomponents improves the stability, but worsens quality signiﬁcantly. When we remove the GEGLU\\nlayer, we replace it with with an equivalent Dense-ReLU-Dense layer to match the FLOPs and\\nparameters.\\n(which injects noise via dropout) was rarely unstable, we examined whether training noise might\\nimprove the stability of sparse models. Table 3 shows a stability improvement versus the baseline,\\nbut at the expense of lower quality. We also ﬁnd that input-jitter, introduced by Fedus et al. (2021),\\ndiminishes quality at XL-scale, hence we ablate it in our models. Input-jitter multiplies the input\\nlogits to the router by a uniform random variable between [1\\x0010\\x002;1 + 10\\x002]. Dropout in our\\nablation is applied throughout the Transformer. As seen previously, improvements in small-scale\\nsettings may fail to generalize when scaled up and therefore trends should always be monitored and\\nre-assessed at increasing scale (Kaplan et al., 2020).\\nMethod Fraction Stable Quality ( \")\\nBaseline 4=6 -1.755\\x060:02\\nInput jitter ( 10\\x002) 3=3 -1.777\\x060:03\\nDropout (0.1) 3=3 -1.822\\x060:11\\nTable 3: Injecting noise during training . Both input-jitter and dropout improve stability, but lead to\\na signiﬁcant loss of model quality. There is a clear tradeoff with most methods: when one improves\\nstability, it then typically decreases model quality. Our work aims to ﬁnd methods that ﬁx stability\\nwithout hurting quality.\\n3.3 S TABILITY AND QUALITY TRADEOFFS WHEN CONSTRAINING ACTIVATIONS AND\\nGRADIENTS\\nOne of the most successful approaches to stabilizing neural networks are constraints on activations,\\nand gradients (Pascanu et al., 2013; Ioffe and Szegedy, 2015; Salimans and Kingma, 2016; Ba et al.,\\n2016). A popular approach consists in the clipping of gradient norms to remedy exploding gradients\\nwhile backpropagating through deep networks (Pascanu et al., 2013).\\nIn this work, we use the Adafactor optimizer due to its memory efﬁciency (though recently in-\\ntroduced 8-bit optimizers (Dettmers et al., 2021) may offer better trade-offs). Instead of gradient\\nclipping, Adafactor uses update clipping , where the changes to the weights are constrained to be\\nbelow a certain norm. We experiment with tightening the update clipping to a smaller value.\\nNext, we study constraints on the logits going into the router. The router computes the probability\\ndistribution over the experts in float32 precision (i.e. selective precision) (Fedus et al., 2021).\\nHowever, at the largest scales, we ﬁnd this is insufﬁcient to yield reliable training. To ﬁx this, we\\nintroduce the router z-loss ,\\nLz(x) =1\\nBBX\\ni=10\\n@logNX\\nj=1ex(i)\\nj1\\nA2\\n(5)\\nwhereBis the number of tokens, Nis the number of experts, and x2RB\\x02Nare the logits going\\ninto the router. This penalizes large logits into the gating network and Section 3.4 contains a more\\ndetailed explanation of why the z-loss before the router is useful.\\n7\\nTable 4 shows that both update clipping and the router z-loss stabilize the model in all 3 runs, but the\\nupdate clipping signiﬁcantly hurts the model quality. Therefore we use the z-loss method for ﬁxing\\nour model stability due to improved quality and stability4.\\nMethod Fraction Stable Quality ( \")\\nBaseline 4=6 -1.755\\x060:02\\nUpdate clipping (clip = 0:1) 3=3 -4.206\\x060:17\\nRouter Z-Loss 3=3 -1.741\\x060:02\\nTable 4: Constraining weight updates and router logits. Constraining the update clipping in\\nAdafactor improves stability, but at a catastrophic loss of quality. Looser clipping values did not\\nreliably stabilize training so we exclude them here. The router z-loss stabilizes the model without\\nany quality degradation (in this case, we observe a slight quality boost).\\nThe router z-loss introduces another hyperparameter (cz), which is the coefﬁcient to weight this as\\npart of the total loss optimized. The total loss is a linearly weighted combination of the cross entropy\\nloss (LCE), the auxiliary load balance loss ( LB), and the router z-loss ( LZ), yielding a total loss\\nLtot=LCE+cBLB+czLZ (6)\\nWe choose a value of cz= 0:001based on the best model quality after pre-training with a hyperpa-\\nrameter sweep. Appendix B logs the resulting losses over the course of pre-training.\\n3.4 S ELECTING A PRECISION FORMAT : TRADING EFFICIENCY AND STABILITY\\nAs in most modern distributed Transformers we train with mixed precision (Micikevicius et al.,\\n2017)5. Weights are stored in float32 for gradient updates and then converted to bfloat16\\nwhen doing matrix multiplications in the forward and backward pass6. Furthermore, all activations\\nare stored and operated on in bfloat16 andallreduce communications can be done in either\\nbfloat16 orfloat32 numerical precision. For the largest model explored in this work (ST-\\nMoE-32B presented later) we ﬁnd speed-ups halving the numerical precision of the allreduce ,\\nhowever this also can destabilize the training so we keep this as float32 throughout this work.\\nA lower precision format enables more efﬁcient models by reducing (a)communication costs be-\\ntween processors and memory, (b)computation costs, (c)memory for storing tensors (e.g. activa-\\ntions). However, lower precision formats come at the expense of larger roundoff errors which can\\nlead to irrecoverable training instabilities.\\nMantissa (7 bits) Exponent (8 bits)Exponent (8 bits) Mantissa (23 bits)Precision Format: Float32\\nPrecision Format: BFloat16Number RangeMax BFloat16\\nRoundoff ErrorMax Float32\\nRoundoff Error\\n[2, 4) 0.01563 2.34x10^(-7)\\n[32, 64) 0.25 3.81x10^(-6)\\n[1024, 2048) 8.0 0.00012\\n[2^20, 2^21) 8192.0 0.125\\n[2^30, 2^31) 8288608.0 128.0\\nFigure 2: Numerical precision formats and roundoff errors. Larger numbers have larger roundoff\\nerrors. bfloat16 has up to 65,536x worse roundoff errors than float32 . The router z-loss\\nencourages the absolute magnitude of numbers to be small, which doesn’t hinder model performance\\nand reduces roundoff errors. The router z-loss is most effective into functions where larger errors\\ncan drastically change the relative output (e.g. exponential and sinusoidal functions).\\n4We also experimented with adding z-losses onto the attention logits which also improves model instability\\nwithout hurting model quality.\\n5See Mesh Tensorﬂow for implementation details: https://github.com/tensorflow/mesh/\\nblob/master/mesh_tensorflow/\\n6Matrix multiplications on TPUs perform multiplications in bfloat16 and accumulations in float32 .\\n8\\nUnderstanding precision format and roundoff errors. Figure 2 reviews the properties of differ-\\nent precision formats and their corresponding roundoff errors for different number ranges. Numbers\\nin any range of two consecutive powers of 2 (e.g. [2,4) and [1024, 2048)) are represented by a ﬁxed\\nnumber of mantissa bits (7 for bfloat16 , 23 for float32 ). As a result, (1)bfloat16 will\\nhave about 65,536x (i.e. 23\\x007 = 16 additional bits and 216= 65536 ) as large roundoff errors as\\nfloat32 and(2)larger numbers have larger roundoff errors. Due to the 8 exponent bits, number\\ncan get as large as \\x193e38, which leads to even float32 having some issues with roundoff errors.\\nSparse expert models are sensitive to roundoff errors because they have more exponential\\nfunctions due to the routers. Sparse expert models introduce additional exponential functions –\\nthrough the router – which can exacerbate roundoff errors7and lead to training instabilities. While\\na roundoff error does not change the ordering of probabilities within a softmax operation, it does\\nimpact the routing of the second token in MoE due to relative thresholding (e.g. a token is only\\nrouted to its second place expert if the gating probability for the second expert is 1=5as large as\\nthat of the ﬁrst expert). Additionally, roundoff errors can drastically change the probability that\\nscales the expert output – which we have found to be important. Finally, we conjecture that the\\nhigher stability we observed for decoder-only models (not shown here) was because they had fewer\\nexponential functions. Section 9 contains a more detailed discussion.\\nAn aside on the router z-loss. One might think that the router z-loss is a convoluted method\\nreplaceable by clipping logits (Wu et al., 2016). We explain why this is not the case. The goal is to\\nminimize large roundoff errors going into exponential functions. Clipping the logits occurs after any\\nroundoff errors – resulting in even larger discontinuities. In one view, clipping in itself is a roundoff\\nerror; conversely, the z-loss naturally encourages the model to produce logits that are small in value\\nand thus more accurately modeled. Due to these dynamics, we ensure all exponentiated tensors are\\ncast to float32 . This hints at the possibility of better number formats for neural networks because\\nof the unused exponent bits when z-losses are added throughout the network (see Section 9).\\n4 F INE-TUNING PERFORMANCE OF SPARSE MODELS\\nThe best performing language models are usually obtained by (1) pre-training on large amounts of\\ndata (e.g. the internet) followed by (2) ﬁne-tuning on a task of interest (e.g. SuperGLUE). Promis-\\ning new techniques have emerged as an alternative, including few-shot inference (Brown et al.,\\n2020), preﬁx tuning (Li and Liang, 2021), prompt tuning (Lester et al., 2021), and adapter modules\\n(Houlsby et al., 2019) – however, a quality gap still persists compared to ﬁne-tuning. Because of\\nthis, we focus on ﬁne-tuning in this work, but highlight recent successes of sparse models in few-\\nshot settings from Du et al. (2021); Artetxe et al. (2021). Further, we leave as future work techniques\\nthat adapt large language models through reinforcement learning (Ouyang et al., 2022)\\n4.1 H YPOTHESIS : A G ENERALIZATION PROBLEM\\nSparse models have performed remarkably well in the regime of large datasets, but have sometimes\\nperformed poorly when ﬁne-tuning (Fedus et al., 2021; Artetxe et al., 2021). We present evidence\\nfor a (not so surprising) hypothesis that sparse models are prone to overﬁtting. We illustrate this\\nproblem through two tasks in SuperGLUE (Wang et al., 2019) – Commitment Bank (De Marneffe\\net al., 2019) and ReCORD (Zhang et al., 2018). Commitment Bank (CB) has 250 training examples\\nwhile ReCORD has over 100,000. This signiﬁcant size discrepancy facilitates a natural study for\\noverﬁtting on two tasks selected as part of the same benchmark.\\nIn Figure 3, we compare the ﬁne-tuning characteristics of the Dense L and the ST-MoE-L model.\\nEach model was pre-trained on 500B tokens from the C4 corpus (Raffel et al., 2019). The models\\n7Exponential functions have the property that a small input perturbation can lead to a large difference in\\nthe output. As an example, consider inputting 10 logits to a softmax function with values of 128 and one logit\\nwith a value 128.5. A roundoff error of 0.5 in bfloat16 will alter the softmax output by 36% and incorrectly\\nmake all logits equal. The calculation goes fromexp(0)\\nexp(0)+10\\x01exp(\\x000:5)\\x190:142toexp(0)\\nexp(0)+10\\x01exp(0)\\x190:091.\\nThis occurs because the max is subtracted from all logits (for numerical stability) in softmax operations and\\nthe roundoff error changes the number from 128.5 to 128. This example was in bfloat16 , but analogous\\nsituations occur in float32 with larger logit values.\\n9\\nStep80.082.585.087.590.092.595.097.5100.0MetricSuperGLUE CB Task\\nSparse train_eval\\nSparse validation_eval\\nDense train_eval\\nDense validation_eval\\nStep8486889092949698100102MetricSuperGLUE ReCoRD Task\\nSparse train_eval\\nSparse validation_eval\\nDense train_eval\\nDense validation_evalFigure 3: Sparse models are prone to overﬁt. We plot train and validation curves for our ST-MoE-\\nL and a dense-L models ﬁne-tuned on the CB task (250 train sequences) and ReCoRD (138k train\\nsequences). In both cases, the sparse model learns more quickly on the train partition (blue exceeds\\ngreen line). However, for the smaller CB task, the dense model outperforms the sparse model on the\\nheld-out validation set (red vs. orange). In contrast, on the larger ReCoRD task, the sparse model\\noutperforms the dense model by several percentage points.\\nare designed to be roughly FLOP matched variants of the T5-Large encoder-decoder models from\\nRaffel et al. (2019) with 770M parameters. The ST-MoE models have 32 experts with an expert\\nlayer frequency of 1=4(every fourth FFN layer is replaced by an MoE layer). The pre-training and\\nﬁne-tuning train capacity factor is 1.25 and the eval is 2.0. We evaluate performance on the held-out\\nvalidation and train dataset partitions.\\nAcross both tasks, the sparse model converges faster to 100% train set accuracy supporting that\\nsparse models optimize effectively under a data distribution shift. On the larger task, ReCORD, the\\nvalidation quality of the sparse model follows the boost in training and signiﬁcantly exceeds the\\ndense model. However, on the smaller task, CB, the sparse model lags its dense counterpart on held-\\nout data. As per the recommendation of Fedus et al. (2021), we consider increasing the dropout\\nwithin the expert hidden state (i.e. expert dropout), but ﬁnd that at this scale, higher values only\\nmoderately improve quality (Figure 4). We study further improvements to ﬁne-tuning in Section 4.2\\nand hyperparameter sensitivity in Section 4.3.\\n0.0 0.1 0.2 0.3\\nDropout Probability80828486SuperGLUE Score\\n0.0 0.1 0.2 0.3 0.4 0.5\\nExpert Dropout Probability80828486SuperGLUE Score\\nFigure 4: Regularization studies of sparse models for ﬁne-tuning . For each setting, we train\\nthree random seeds till convergence on SuperGLUE. We ﬁnd that increased regularization through\\ndropout provides modest boosts. (Left) demonstrates peak SuperGLUE ﬁne-tuning quality at a\\nglobal dropout rate of 0.1. Higher values over-regularize and severely hurt quality. (Right) Starting\\nwith the best known global dropout rate of 0.1, we selectively increase the expert dropout (an in-\\ndependent dropout rate on the expert hidden activation). This yields further generalization beneﬁts\\nand is in line with the ﬁndings of Fedus et al. (2021).\\n10\\n4.2 F INE-TUNING A SUBSET OF MODEL PARAMETERS TO IMPROVE GENERALIZATION\\nTo combat overﬁtting we experiment updating only a subset of models parameters during ﬁne-\\ntuning. Figure 5 measures quality for updating 5 different subsets of parameters: all parameters\\n(All), only non MoE parameters (Non MoE), only MoE parameters (MoE), only the self-attention\\nand enc-dec attention parameters (Attention) and only the non MoE FFN parameters (FFN).\\nAll Non MoE MoE Attention FFN\\nParameters Being Updated81828384858687SuperGLUE Score\\nFigure 5: Updating only a subset of model parameters during ﬁne-tuning . To improve the gener-\\nalization of sparse models and combat overﬁtting, we ﬁne-tune a subset of the model parameters. All\\nresults are with the ST-MoE-L model and are an average of 5 different random seeds. We observe\\nthat updating 3=5of the subsets of parameters appear to work about the same, while ﬁne-tuning only\\nthe MoE parameters results in a drastic quality reduction.\\nWe observe that updating the non MoE parameters works about as well as updating all the param-\\neters and updating only the FFN parameters works a bit better. Updating only the MoE parameters\\nsigniﬁcantly degrades ﬁne-tuning performance, which is where \\x1980% of model parameters are.\\nOnly updating the non MoE parameters can be an effective way to speedup and reduce memory for\\nﬁne-tuning.\\nWe hypothesize that ﬁne-tuning only the MoE parameters leads to bad performance since expert\\nlayers only occur every 1=4layers and a token will see at most two experts per layer. Therefore,\\nupdating the MoE parameters will affect much fewer layers and FLOPs than updating any other\\nsubset of the parameters we tried. Updating only the MoE parameters resulted in a much larger\\ntraining loss than updating the non MoE parameters, even though there are signiﬁcantly more pa-\\nrameters. We further observe that updating all the non-MoE parameters results in a higher training\\nloss than updating all the parameters, but unfortunately this regularization effect didn’t translate to\\nbetter validation performance.\\nFurther, one regularizer we tried was a dropout variant where entire experts were masked out\\nstochastically during training. However, this failed to improve generalization in our preliminary\\nstudies. Appendix J expands on this experiment and contains other negative results.\\n4.3 S PARSE AND DENSE MODELS REQUIRE DIFFERENT FINE-TUNING PROTOCOLS\\nHow sensitive are sparse and dense models to the ﬁne-tuning protocol? We study two hyperparam-\\neters: the batch size and the learning rate. We pretrain a Dense-L and ST-MoE-L on 500B tokens\\nof C4 and then ﬁne-tune on SuperGLUE. Figure 6 summarizes our experiments with the full data\\npresented in Table 20 (Appendix F). Across all hyperparameter settings, the sparse models (orange)\\noutperform the dense (blue) counterparts – however, the best setting for each can materially change\\nresults. Sparse and dense models have vastly different performance across different batch sizes and\\nlearning rates. Sparse models beneﬁt from smaller batch sizes and a higher learning rate. Consis-\\ntent with the overﬁtting hypothesis (Section 4.1), both these changes might improve generalization\\nthrough higher noise in the ﬁne-tuning process. Finally, we point out the importance of correctly\\ntuning the batch size and learning rate during ﬁne-tuning. Simply using the same ﬁne-tuning hyper-\\n11\\nparameters that worked well for the dense model can mask any pre-training improvements obtained\\nby the sparse model.\\n65k 262k 1M\\nBatch Size81828384858687SuperGLUE ScoreDense\\nSparse\\n1e-4 5e-4 1e-3\\nLearning Rate81828384858687SuperGLUE ScoreDense\\nSparse\\nFigure 6: Batch size and learning rate sensitivity. We measure differences and sensitivity to ﬁne-\\ntuning protocols between dense (blue) and sparse (orange) models. Each bar is an average across 6\\ndifferent runs with different hyperparameters. On SuperGLUE, sparse models beneﬁt from noisier\\nhyperparameters including small batch sizes and high learning rates. Dense models behave nearly\\noppositely. See Appendix F for all data.\\n4.4 S PARSE MODELS AREROBUST TO DROPPED TOKENS DURING FINE-TUNING\\nSparse models route tokens to one or more experts at each layer. To make these models efﬁcient in\\nthe SPMD paradigm with modern hardware, the expert capacity (the number of tokens each expert\\nprocesses) needs to be ﬁxed ahead of time (see Section 2 for more details). When an expert receives\\nmore tokens than its capacity, the extra tokens are dropped — no computation is applied to those\\ntokens. We again try to prevent this by (1)pre-training with an auxiliary loss that promotes equal\\namounts of tokens getting sent to each expert and (2)a capacity factor (a hyperparameter) that adds\\nroom for extra tokens at each expert. We experiment with turning off the auxiliary loss during\\nﬁne-tuning and using different capacity factors. Tables 5 reveals a surprising result that ﬁne-tuning\\nquality is not materially impacted by dropping up to 10-15% of tokens8. Studies on ST-MoE-32B\\ncorroborate that high capacity factors do not improve ﬁne-tuning quality. This is in-line with ﬁndings\\nof Yang et al. (2021) that unequal load balance may not signiﬁcantly impact model quality.\\nModel Train CF Eval CF Aux Loss Percent Tokens Dropped SuperGLUE (\")\\nSparse 0.75 2.0 Yes 10.6% 86.5\\x060.21\\nSparse 1.25 2.0 Yes 0.3% 86.7\\nSparse 2.0 3.0 Yes 0.0% 85.8\\nSparse 4.0 5.0 Yes 0.0% 86.4\\nSparse 0.75 2.0 No 15.6% 85.7\\nSparse 1.25 2.0 No 2.9% 85.8\\nSparse 2.0 3.0 No 0.4% 85.9\\nSparse 4.0 5.0 No 0.0% 86.4\\nTable 5: Sparse models are robust to dropped tokens when ﬁne-tuning. We ﬁnd the ﬁne-tuning\\nquality on SuperGLUE is not impacted signiﬁcantly across the values explored. Interestingly, drop-\\nping 10-15% of tokens can perform approximately as well as models that drop <1%. We also\\nobserve that load balance losses (Aux Loss) improve ﬁne-tuning. The dropped token percentage\\ncorresponds to the fraction of dropped tokens across all expert layers at peak validation accuracy.\\n8Token dropping may be a form of regularization and a more extensive study may be an interesting direction\\nfor future work.\\n12\\n4.5 I NSERTING SENTINELS TOKENS DURING FINE-TUNING\\nSentinel tokens denote masked sequences in the span-corruption objective (Fedus et al., 2018; Devlin\\net al., 2018). This differs from any ﬁne-tuning task we would likely encounter, leading to a domain\\nmismatch between pre-training and ﬁne-tuning. Table 6 illustrates the difference. We examine\\nwhether modifying the ﬁne-tuning task to look more like the pre-training task effects results.\\nObjective Inputs Targets\\nSpan Corruption I like <X> the pool <Y> day . <X> going to <Y> on a sunny\\nFine-Tuning What is the capital of Illinois ? Springﬁeld\\nFine-Tuning + Sentinels What is the capital of Illinois ? <X> <X> Springﬁeld\\nTable 6: Inserting sentinels during ﬁne-tuning mimics the pre-training span objective . We\\nhighlight the typical difference between span corruption and ﬁne-tuning. We propose modifying the\\nﬁne-tuning task to resemble pre-training by inserting sentinel tokens.\\nIn Table 7 we ﬁnd that adding sentinel tokens while ﬁne-tuning only improves Grammar Error\\nCorrection (GEC) (Rothe et al., 2021), but not SuperGLUE. We tried to further reduce the data\\ndistribution shift by inserting multiple sentinel tokens (as would be encountered by the model while\\npre-training), but again found no universal beneﬁt. However, despite no consistent beneﬁt on held-\\nout data, we ﬁnd that training convergence is accelerated for both dense and sparse models.\\nModel Insert Sentinel Tokens SuperGLUE (\") GEC (\")\\nDense 84.9\\x060.33 22.3\\x060.25\\nDense X 85.1\\x060.25 22.1\\x060.42\\nSparse 86.6\\x060.18 22.2\\x060.04\\nSparse X 86.6\\x060.24 22.9\\x060.09\\nTable 7: Impact of sentinel tokens for ﬁne-tuning . The addition of sentinel tokens (a similar\\nconcept used in Lester et al. (2021)) during ﬁne-tuning has mixed performance on the two tasks we\\nconsider. SuperGLUE records the average score and GEC records the exact match. While we ﬁnd it\\ndoesn’t improve generalization, sentinel tokens can accelerate training convergence.\\n5 D ESIGNING SPARSE MODELS\\nThe design of dense models has been guided by the foundational work of Kaplan et al. (2020). But\\nsparse models pose a myriad of additional questions: (1) How many experts to use? (2) Which\\nrouting algorithm? (3) What value for the capacity factor? (4) How does hardware change these\\ndecisions? In this section, we comment on these and offer recommendations for building Pareto ef-\\nﬁcient sparse models. Concurrently, Clark et al. (2022) provides additional design recommendations\\nincluding higher layer frequency and top-1 routing as per Fedus et al. (2021).\\nDesigning Sparse Models\\n1.In our setup, we recommend top-2 routing with 1.25 capacity factor and at most one\\nexpert per core.\\n2.The capacity factor can be changed during evaluation to adjust to new memory/compute\\nrequirements.\\n3.Dense layer stacking and a multiplicative bias can boost quality (Appendix C).\\n5.1 S ETTING THE NUMBER OF EXPERTS\\nOne of the ﬁrst questions is the number of experts to use. Fedus et al. (2021) presented the scaling-\\nproperties of Switch Transformer which yielded monotonic pre-training beneﬁts (on a step basis) on\\n13\\nC4 up to 512-experts, Kim et al. (2021) up to 64-experts and Clark et al. (2022) up to 512-experts.\\nBut the incremental beneﬁt quickly diminishes with many experts ( >256) or equivalently, with very\\nsparse models ( <1% of experts activated).\\nHowever, reﬂecting on the speciﬁc hardware system can further guide this choice. The compute-to-\\nmemory ratio (operational intensity) can serve as an estimate of the efﬁciency of different operations\\n(Williams et al., 2009; Shazeer, 2019). A model is memory bound if the time to load tensors to\\nthe computing core (e.g. ALU/MMU) greatly exceeds the time required to do the computation on\\nthe tensors. On modern GPUs and TPUs, increasing this compute to memory ratio improves the\\nefﬁciency.\\nReturning to sparse expert models, using more than one expert per core increases memory transfer,\\npotentially hurting efﬁciency. Increasing the number of experts does not change the computation\\ndone (sparse models apply a ﬁxed amount of computation to each input), but increases the mem-\\nory transfer requirement (additional expert variables must be loaded from device memory). This\\ndecreases the compute-to-memory ratio9.\\nOn our TPU system, we recommend to one expert (or less) per core. Our largest models use both data\\nand model parallelism where data parallelism is over “rows” and model-parallelism over “columns”\\nof the logical mesh. We use \\x141 expert per data parallelism row to ensure the compute-to-memory\\nratio is high and to reduce the cores needed for evaluation and inference. Furthermore, using less\\nexperts lets us allocate more cores to the model parallelism “column” to have more FLOPs in our\\nmodel. Appendix H explains our mesh layouts for when we have fewer experts than data parallelism\\nrows.\\n5.2 C HOOSING THE CAPACITY FACTOR AND ROUTING ALGORITHM\\nWe generalize top-1 routing (Fedus et al., 2021; Roller et al., 2021) and top-2 (Shazeer et al., 2017;\\nLepikhin et al., 2020) to study top- nrouting where each token is processed by at most nexperts. In\\nthis study, all models are pre-trained for 100k steps with 1M tokens per batch and sparse models have\\n32 experts and are FLOP matched to T5-Large Raffel et al. (2019). We draw two key conclusions.\\nFirst, increasing both the train and eval capacity factors (CF) improves quality as seen by comparing\\nacross the segmented blocks of Table 8. For instance, top-1 routing improves by +0.011 neg. log\\nperp. when increasing from 1.0 !1.25 train CF and top-2 routing improves +0.009 increasing\\nfrom 1.25!2.0 train CF. To provide context for these numbers: tripling the size of a dense model\\n(Dense-L to Dense-XL) yields a +0.090 neg. log perp. boost. Therefore, these CF boosts are \\x18\\n1=10thof that magnitude. But this comes at a cost. Increasing the capacity factor linearly increases\\nthe einsums costs, memory for activations, all2all communication costs, and model-parallelism\\nallreduce communication costs for expert layers10.\\nSecond, there are small gains of top- (n+1) over top-nrouting given a ﬁxed capacity factor (Table 8).\\nFor instance, top-2 routing improves +0.004 over top-1 at train CF of 1.25 or about 1=20ththe boost\\nof a dense model tripling. This revises an earlier recommendation from Fedus et al. (2021). The\\nprimary difference between these experimental setups was scale of compute. Fedus et al. (2021)\\ntrained 220M-FLOP matched models for 50B tokens. We ﬁnd at an 8x larger scale of training\\n(1B-FLOP matched models for 100B tokens) there is instead a small gain to route to more than\\none expert. Furthermore, at the larger experimental scale, the speed difference of top- nversus top-\\n(n+ 1) routing is negligible. Speed differences were observed in Fedus et al. (2021) because the\\nrouter computation was a larger fraction of the total model computation.\\n9As an exercise to the reader, verify the operational intensity of the ﬁrst expert computation isb\\x01h\\nb+h\\x01ewithb\\nbatch size, hhidden dimension, enumber of experts.\\n10all2all andallreduce costs depend on the number of devices, batch size, dmodel and capacity factor,\\nbut not on the number of experts.\\n14\\nAlgorithm Train CF Eval CF Neg. Log Perp. (\")\\nDense-L — — -1.474\\nDense-XL — — -1.384\\nTop-1 0.75 0.75 -1.428\\nTop-1 0.75 2.0 -1.404\\nTop-2 0.75 0.75 -1.424\\nTop-2 0.75 2.0 -1.402\\nTop-1 1.0 1.0 -1.397\\nTop-1 1.0 2.0 -1.384\\nTop-2 1.0 1.0 -1.392\\nTop-2 1.0 2.0 -1.378\\nTop-1 1.25 1.25 -1.378\\nTop-1 1.25 2.0 -1.373\\nTop-2 1.25 1.25 -1.375\\nTop-2 1.25 2.0 -1.369\\nTop-2 2.0 2.0 -1.360\\nTop-2 2.0 3.0 -1.359\\nTop-3 2.0 2.0 -1.360\\nTop-3 2.0 3.0 -1.356\\nTable 8: Comparing capacity factors (CF) and routing algorithms. Increasing both train and eval\\nCF improves performance. Increasing or decreasing the eval CF gives an additional lever if you have\\nmore or less compute at eval time. Next, there are smaller gains of top- (n+ 1) over top-nrouting\\nacross capacity factors. Because the quality improves, but the speed slows as the CF increases, the\\nPareto efﬁcient CF must be determined by the speciﬁc hardware system.\\nThe speciﬁc hardware-software system will determine the optimal nand capacity factor. For in-\\nstance, if the system supports fast all2all andallreduce communications, larger capacity\\nfactors and larger nin top-nrouting may be optimal. However, if the all2all and/or allreduce\\ncommunications are slow, smaller capacity factors may dominate. In our case, the hardware-\\nsoftware stack is the TPU and Mesh Tensorﬂow. We record the training speed of both our ST-MoE-L\\nand ST-MoE-32B model in Table 9 as we increase the train capacity factor. As the models scale,\\na higher capacity factor makes the models increasingly slower. The ST-MoE-L does not require\\nmodel parallelism (it ﬁts within accelerators memory, which implies no additional allreduce\\ncommunications) making it better suited for high capacity factors than our ST-MoE-32B model. For\\nour largest model, we therefore continue to use the smaller train capacity factor of 1.25 advocated\\nby Fedus et al. (2021) for Pareto efﬁciency, differing from other work which use a larger and more\\nexpensive 2.0 capacity factor (Lepikhin et al., 2020; Du et al., 2021).\\nModel Train CF Step Time (s) ( #)\\nST-MoE-L 1.25 2.397\\nST-MoE-L 2.0 2.447 ( +7%)\\nST-MoE-32B 1.25 4.244\\nST-MoE-32B 2.0 4.819 ( +14%)\\nTable 9: Proﬁling sparse models on TPUs. Increasing the train capacity factor from 1.25 to 2.0\\nincreases the step-time by +7% for the large (1B) model but by +14% for our 32B model. As the\\nmodel size increases, we ﬁnd the small quality gains of higher train capacity factors from Table 8\\nare more than offset by the signiﬁcant 14% slow-down. Note: the step time between ST-MoE-L and\\nST-MoE-32B are not comparable because they used a different number of cores.\\nOur results in this section focus on top- nrouting, but we also experimented with a variety of other\\nrouting techniques in Appendix J. We found most performed similarity or worse compared to top- n\\nrouting. However we found Batch Prioritized Routing (BPR), introduced in Riquelme et al. (2021),\\nsigniﬁcantly helps performance for capacity factors less than one (Appendix D). We recommend\\n15\\nBPR for larger models where all2all andallreduce are more expensive and lower capacity\\nfactors are optimal.\\n6 E XPERIMENTAL RESULTS\\nGiven our improvements to training stability, ﬁne-tuning and model design, we start by validating\\na sparse model approximately FLOP-matched to T5-Large (Raffel et al., 2019). We conclude this\\nsection by designing and training a 269B sparse parameter model (FLOP matched to a 32B dense\\nmodel) which achieves state-of-the-art quality across a wide set of NLP tasks.\\nWe studied the SuperGLUE (Wang et al., 2019) benchmark throughout this work which consists\\nof tasks including sentiment analysis (SST-2), word sense disambiguation (WIC), sentence similar-\\nity (MRPC, STS-B, QQP), natural language inference (MNLI, QNLI, RTE, CB), question answer-\\ning (MultiRC, RECORD, BoolQ), coreference resolution (WNLI, WSC) and sentence completion\\n(COPA) and sentence acceptability (CoLA). We often observe good performance on SuperGLUE to\\ncorrelate with (but not guarantee) performance across many NLP tasks. We also include a divers set\\nof additional benchmarks. The CNN-DM (Hermann et al., 2015) and BBC XSum (Narayan et al.,\\n2018) datasets are used to measure the ability to summarize articles. Question answering is probed\\nwith the SQuAD dataset (Rajpurkar et al., 2016) as well as on grade-school science questions in\\nARC Easy and ARC Reasoning Challenge (Clark et al., 2018). And as in Roberts et al. (2020), we\\nevaluate the knowledge of our models by ﬁne-tuning on three closed-book question answer datasets:\\nNatural Questions (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013) and Trivia QA\\n(Joshi et al., 2017). Closed-book simply refers to questions posed with no supplemental reference or\\ncontext material. To gauge the model’s common sense reasoning we evaluate it on the Winogrande\\nSchema Challenge (Sakaguchi et al., 2020). And ﬁnally, we test our model’s natural language infer-\\nence capabilities on the Adversarial NLI Benchmark (Nie et al., 2019).\\n6.1 ST-M OE-L\\nFor simplicity and to cover dozens of tasks easily, we train on mixtures of the tasks listed rather than\\nseparately ﬁne-tuning a model on each task. However, because the tasks vary in size considerably,\\nequally sampling per the number of examples would over-sample large tasks and under-sample small\\nones. We therefore mix each task in proportion to the number of examples in its ‘train’ split (up to\\nsome maxnumexamples=65536 ) as in Raffel et al. (2019). This means that tasks containing\\nmore than 65536 training examples are weighted as if they only contain maxnumexamples .\\nTable 10 summarizes the quality of a dense T5-Large (L) model and sparse model with approxi-\\nmately the same number of FLOPs pre-trained for 500k steps with a 1M batch size (524B tokens)\\non the C4 dataset (Raffel et al., 2019). The sequence length for the encoder was 512 and 114 for\\nthe decoder. We observe improvements on the validation (dev) sets across a wide array of tasks ex-\\namining natural language understanding, question answering, and summarization. As seen in Fedus\\net al. (2021), striking gains are observed in closed book question answering (Roberts et al., 2020).\\nAlso, in support of the overﬁtting hypothesis presented in Section 4.1, we observe two of the smallest\\ntasks CB and WSC (250 and 259 training examples, respectively), are the only ones where the sparse\\nmodel does not yield gains over its dense counterpart. This again suggests that improved forms of\\nregularization for sparse models may unleash greater performance.\\n6.2 ST-M OE-32B\\nWith quality validated at the scale of T5-Large, we seek to push the capabilities of sparse models\\nthrough the ST-MoE-32B. When designing this, we sought a balance between FLOPs and parame-\\nters. High-FLOP sparse models were previously unstable in Fedus et al. (2021) in our setting (i.e.\\nencoder-decoder models, Adafactor optimizer), but the router z-loss enabled us to proceed. For com-\\nputational efﬁciency, we expanded the hidden size of the experts ( dffin Table 11 below)11. Finally,\\nwe increased the dkvto 128 for better performance on our hardware. The most salient changes are\\nfewer overall parameters and more FLOPs per token relative to both Switch-C and Switch-XXL. Our\\n11allreduce activation communications introduced through model parallelism are independent of the\\nhidden size, but not the model dimension, making it a good choice to increase.\\n16\\nName Metric Split Dense-L ( \") ST-MoE-L (\") Gain (%)\\nSQuADv2 F1 dev 94.0 94.5 +1%\\nSQuADv2 acc dev 87.6 88.1 +1%\\nSuperGLUE avg dev 85.1 87.4 +3%\\nBoolQ acc dev 87.1 88.6 +2%\\nCopa acc dev 83.0 91.0 +10%\\nRTE acc dev 91.0 92.1 +1%\\nWiC acc dev 70.4 74.0 +5%\\nMultiRC F1 dev 83.9 86.0 +3%\\nWSC acc dev 95.2 93.3\\x002%\\nReCoRD acc dev 85.7 88.9 +4%\\nCB acc dev 100 98.2\\x002%\\nXSum ROUGE-2 dev 19.9 21.8 +10%\\nCNN-DM ROUGE-2 dev 20.3 20.7 +2%\\nWinoGrande (XL) acc dev 75.4 81.7 +8%\\nANLI (R3) acc dev 54.3 57.3 +6%\\nARC-Easy acc dev 63.5 75.4 +19%\\nARC-Challenge acc dev 50.2 56.9 +13%\\nClosed Book TriviaQA acc dev 28.1 33.8 +20%\\nClosed Book NatQA acc dev 27.2 29.5 +8%\\nClosed Book WebQA acc dev 30.5 33.2 +9%\\nTable 10: Fine-tuning performance of FLOP-matched dense and sparse models. Comparison\\nof the dense-L baseline and the sparse FLOP-matched version (higher numbers better). We observe\\nconsistent gains across diverse tasks, using approximately the same amount of computation. The\\nonly two tasks without improvement from the sparse model are the two smallest: CB with 250\\ntraining examples and WSC with 259.\\nST-MoE-32B has “only” 269B parameters and is approximately FLOP-matched to a dense Trans-\\nformer with 32B parameters. The reduced parameter count from Switch-C and Switch-XXL eases\\nthe burden for both serving and ﬁne-tuning. Finally, we use the sparse-dense stacking described in\\nAppendix C.\\nWe pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the\\ndataset from GLaM (Du et al., 2021) summarized in Appendix E. We use 1M tokens per batch,\\nthe Adafactor optimizer with default hyperparameters, and a learning rate warm-up of 10k steps\\nfollowed by inverse square root decay. Our model follows the initialization scheme proposed in\\nFedus et al. (2021).\\nTable 12 evaluates our ST-MoE-32B model against previous state-of-the-art approaches using\\ninference-only (zero-shot, one-shot) as well as ﬁne-tuning. On SuperGLUE, our model improves\\nupon the prior state-of-the-art model, achieving an average score of 91.2 on the test server (93.2\\nvalidation accuracy) which is over one percentage point beyond estimated human capability. For\\nboth summarization datasets, XSum and CNN-DM, our model achieves state-of-the-art without ad-\\nditional changes to training or ﬁne-tuning (Raffel et al., 2019; Liang et al., 2021). ST-MoE-32B\\nimproves the current state-of-the-art on the test server submissions for both ARC Easy (92.7 !\\n94.8) and ARC Challenge (81.4 !86.5). On two of the three closed book QA tasks, we improve\\nover the prior state-of-the-art. Closed book WebQA achieves a 47.4 accuracy (prior best of 42.8 from\\nRoberts et al. (2020) and exceeds results from the zero-shot performance of the ERNIE 3.0 Titan\\n260B dense parameter model (Wang et al., 2021)). Closed book NatQA improves to 41.9 accuracy\\n(prior best of 41.5 from Karpukhin et al. (2020)). We ﬁnd signiﬁcant improvements on adversari-\\nally constructed datasets (ANLI R3 and WinoGrande XL). ANLI R3 (Nie et al., 2019) improves the\\nstate-of-the-art to 74.7 (prior best of 53.4).\\nWe note some weaknesses in our model. ST-MoE-32B has lackluster performance on the small\\nSQuAD dataset, with an exact match score of 90.8 which falls short of the older benchmark set by\\nthe T5-XXL of 91.3. Furthermore, while setting a new state-of-the-art for SuperGLUE in aggregate,\\n17\\nModel Parameters FLOPs/seq dmodel FFN GEGLU dffdkv\\nDense-L 0.8B 645B 1024 X 2816 64\\nT5-XXL 11.1B 6.3T 4096 X 10240 64\\nSwitch-XXL 395B 6.3T 4096 X 10240 64\\nSwitch-C 1571B 890B 2080 6144 64\\nST-MoE-L 4.1B 645B 1024 X 2816 64\\nST-MoE-32B 269B 20.2T 5120 X 20480 128\\nModel Num. Heads Num. Layers Num. Experts Expert Layer Freq. Sparse-Dense\\nDense-L 16 27 – –\\nT5-XXL 64 24 – –\\nSwitch-XXL 64 24 64 1=4\\nSwitch-C 32 15 2048 1=1\\nST-MoE-L 16 27 32 1=4 X\\nST-MoE-32B 64 27 64 1=4 X\\nTable 11: Model comparisons. A comparison of the Dense-L and T5-XXL, the two largest Switch\\nTransformer variants (Switch-XXL and Switch-C), and the ST-MoE-L and ST-MoE-32B. dmodel\\nrefers to the model hiddenstate size and dffis the internal size of the FFN layer. dkvis the dimension\\nof each attention head. Expert Layer Freq. is the fraction of FFN layers replaced with a sparse layer.\\nSparse-Dense refers to the architectural variant described in Appendix C.\\nPrevious Best (\") Ours ( \")\\nName Metric Split Zero-Shot One-Shot Fine-Tune Fine-Tune\\nSQuADv2 F1 dev 68.3e70.0e96:2a96.3\\nSQuADv2 acc dev 62.1e64.6e91.3a90.8\\nSuperGLUE avg test – – 90.9 91.2\\nBoolQ acc dev/test 83.0e82.8e92.0 92.4\\nCopa acc dev/test 91.0d92.0e98.2 99.2\\nRTE acc dev/test 68.8e71.5e94.1 93.5\\nWiC acc dev/test 50.5e52.7e77.9 77.7\\nMultiRC F1 dev/test 72.9d72.9d88.6 89.6\\nWSC acc dev/test 84.9e83.9e97.3 96.6\\nReCoRD acc dev/test 90.3e90.8e96.4 95.1\\nCB acc dev/test 46.4d73.2e99.2 98.0\\nXSum ROUGE-2 test – – 24.6h27.1\\nCNN-DM ROUGE-2 test – – 21.6a21.7\\nWinoGrande XL acc dev 73.4e73.2d– 96.1\\nANLI R3 acc test 40.9e40.8e53.4 74.7\\nARC-Easy acc test 71.9e76.6e92.7g95.2\\nARC-Challenge acc test 51.4 53.2 81.4g86.5\\nCB TriviaQA em dev 68.0e74.8e61.6b62.3\\nCB NatQA em test 21.5e23.9e41:5c41.9\\nCB WebQA em test 38.0f25.3 42:8b47.4\\nTable 12: ST-MoE-32B versus previous best for inference-only techniques and ﬁne-tuned mod-\\nels.A split of “dev/test” refers to dev split for Zero-Shot and One-Shot and test split for Fine-Tune\\nquality. Data not available ﬁlled in with “–”. Superscript letters denote the result:a: Raffel et al.\\n(2019)b: Roberts et al. (2020)c: Karpukhin et al. (2020),d: Brown et al. (2020),e: Du et al. (2021),\\nf: Wang et al. (2021),g: UniﬁedQA + ARC MC/DA + IR,h: Zhang et al. (2020).\\n18\\ncertain tasks, including small ones like CB, WSC, fail to improve. Finally, on closed book Trivia\\nQA, our model improves over the ﬁne-tuned baseline with SSM from Roberts et al. (2020), but fails\\nto produce gains over both GPT-3 and GLAM.\\nWhile not the focus of this paper, we present the quality differential between recent advances in\\ninference-only techniques like few-shot learning and ﬁne-tuning on these tasks (GPT-3 (Brown\\net al., 2020), GLAM (Du et al., 2021) and Gopher (Rae et al., 2021)). As expected and observed\\npreviously, ﬁne-tuning outperforms zero/one-shot learning, but has the disadvantage of requiring\\nadditional training and different models for each task.\\n7 T RACING TOKENS THROUGH THE MODEL\\nThus far we have presented quantitative measures and performance metrics. We change tack to\\nexplore qualitative features by visualizing how tokens are routed among the experts. We do so by\\npassing a batch of tokens to the model and manually inspecting token assignment at each layer. We\\nconsider our ST-MoE-L model pre-trained either on the monolingual C4 corpus (Raffel et al., 2019)\\nor on the multilingual mC4 corpus (Xue et al., 2020). On both the encoder and the decoder, the\\nmodel has six sparse layers, each with 32 experts.\\nPreliminaries\\nThe span corruption objective is to recover spans of variable-length contiguous segments\\nmasked out in the inputs. This is formatted as:\\nInputs: I went to<extra id0>to buy<extra id1>\\nTargets:<extra id0>the store<extra id1>milk\\nIn our encoder-decoder architecture, the inputs will be passed to the encoder and targets to\\nthe decoder.\\nEach group of tokens is routed jointly with load balancing across experts incentivized by an auxiliary\\nloss as proposed in Shazeer et al. (2017) (see Appendix A for details). Tokens compete for expert\\nassignment against other tokens in their group, rather than the entire batch, and expert specialization\\nis heavily inﬂuenced by the distribution of tokens in each group. The notion of groups is introduced\\nto limit the cost of dispatching and gathering the correct tokens to the correct experts.\\n7.1 E NCODER EXPERTS EXHIBIT SPECIALIZATION\\nOur ﬁrst observation is that, at each layer, at least one expert specializes in sentinel tokens (mask to-\\nkens that represent blanks to ﬁll-in). Additionally, some encoder experts exhibit clear specialization,\\nwith some experts primarily operating on punctuation, verbs, proper names, counting, etc. Table 13\\npresents a few notable example of specialization across encoder experts. And while we ﬁnd many\\ninstances of specialization, these have been speciﬁcally extracted from many examples without a\\nclear semantic or syntactic specialization.\\n7.2 D ECODER EXPERTS LACK SPECIALIZATION\\nIn contrast, expert specialization is far less noticeable in the decoder . Not only are sentinel to-\\nkens routed somewhat uniformly across decoder experts (see Table 14), but we also do not observe\\nmeaningful specialization (semantics or syntax) in decoder experts.\\nWe hypothesize that this lack of meaningful expert specialization is caused by the distribution of\\ntarget tokens induced by the span corruption objective . In particular, (a)a smaller number of tokens\\nare routed jointly in the decoder due to longer sequence lengths in the encoder (e.g. group size\\nis 2048 in the encoder vs 456 in the decoder in our setup) and (b)a higher proportion of tokens\\nare sentinel tokens in the decoder. As a result, target tokens in each group typically cover a smaller\\nsemantic space (compared to the encoder), perhaps explaining the lack of expert specialization in the\\ndecoder. This intricate interplay between the architecture and the training objective invites further\\n19\\nExpert specialization Expert position Routed tokens\\nSentinel tokens Layer 1 been<extra id4><extra id7>ﬂoral to\\n<extra id10><extra id12><extra id15>\\n<extra id17><extra id18><extra id19>...\\nLayer 4 <extra id0><extra id1><extra id2>\\n<extra id4><extra id6><extra id7>\\n<extra id12><extra id13><extra id14>...\\nLayer 6 <extra id0><extra id4><extra id5>\\n<extra id6><extra id7><extra id14>\\n<extra id16><extra id17><extra id18>...\\nPunctuation Layer 2 , , , , , , , , , - , , , , , ). )\\nLayer 6 , , , , , : . : , & , & & ? & - , , ? , , , . <extra id27>\\nConjunctions and articles Layer 3 The the the the the the the the the The the the\\nthe the the The the the the\\nLayer 6 a and and and and and and and or and a and .\\nthe the if ? a designed does been is not\\nVerbs Layer 1 died falling identiﬁed fell closed left posted lost felt\\nleft said read miss place struggling falling signed died\\nfalling designed based disagree submitted develop\\nVisual descriptions Layer 0 her over her know dark upper dark outer\\ncolor, spatial position center upper blue inner yellow raw mama\\nbright bright over open your dark blue\\nProper names Layer 1 A Mart Gr Mart Kent Med Cor Tri Ca Mart\\nR Mart Lorraine Colin Ken Sam Ken Gr Angel A\\nDou Now Ga GT Q Ga C Ko C Ko Ga G\\nCounting and numbers Layer 1 after 37 19. 6. 27 I I Seven 25 4, 54 I two dead we\\nwritten and numerical forms Some 2012 who we few lower each\\nTable 13: Notable examples of specialization in encoder experts . We ﬁnd experts that specialize\\nin punctuation, conjunctions & articles, verbs, visual descriptions, proper names, counting & num-\\nbers. Across all layers (not shown), we observe experts that primarily operate on sentinel tokens\\n(marked as<extra idx>). Note that a SentencePiece model (Kudo and Richardson, 2018) will\\nsplit a token if it doesn’t exist in the vocabulary, e.g. Kenneth may become Ken, ne, th.\\nresearch on better leveraging sparsity and expert specialization in the decoder. Alternatively, future\\nwork could study simply removing the experts in the decoder layer, which also confers beneﬁts\\nduring autoregressive decoding (Kudugunta et al., 2021a).\\nLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Uniform (32-experts)\\nEncoder 2.2 1.8 1.6 1.7 1.7 1.2 3.5\\nDecoder 3.4 3.4 3.4 3.4 3.4 3.4 3.5\\nTable 14: Entropy of routed sentinel tokens across encoder and decoder layers. We support our\\nqualitative observation that encoder experts specialize, but decoder expert don’t by computing the\\nentropy over the routing for sentinel tokens. The encoder routing entropy is low, but the decoder\\nrouter is high entropy, and nearly equal to uniform routing. Because each layer has 32-experts, a\\ncompletely uniform distribution has entropy of 3.5.\\n20\\n7.3 M ULTILINGUAL EXPERTS SPECIALIZE , BUTNOT BY LANGUAGE\\nWe next consider a multilingual sparse model pretrained on a mixture of different languages and\\ninspect the expert specialization in the encoder. As in the monolingual case, we ﬁnd strong evidence\\nof expert specialization. Table 15 presents some examples of experts specializing in sentinel tokens,\\nnumbers, conjunctions & articles and proper names.\\nExpert specialization Routed tokens\\nSentinel tokens to<extra id6>to til<extra id9>\\n<extra id10>to<extra id14><extra id17>\\n<extra id19><extra id20><extra id21>...\\nNumbers $50 comment .10.2016 ! 20 20 3 ! 5 1. ! 91 ? n ´e ?\\n2 17 4 17 11 17 8 & 11 & 22:30 02 2016. ) iOS\\nConjunctions & Articles of of of their their of any this this your your am von\\nthis of Do of of This these our 的的于的在的在的\\nle les Le la di la sur sur 136 sur ののするのというのし\\nPrepositions & Conjunctions For for or for for or for from because https during https\\n并与和par c Pour `a a par tr `e pour pour pour pour pour c とやのに\\nでででなので - and and + c between and and\\nProper names Life Apple iOS A IGT 众莫HB\\nF HB A K A OPP OK HB A Gia C Gia C P Scand Wi\\nG H Z PC G Z ハイ PC G Ti CPU PC PC A キット OS\\nTable 15: Examples of specialization in multilingual experts (encoder) . Multilingual ex-\\nperts also exhibit specialization, which sometimes spans across different languages (e.g. ”for” and\\n”pour”). Experts trained on multilingual mixtures do not exhibit language specialization.\\nOne might expect experts to specialize in languages, which appears as a natural criterion for divvying\\nup batches of data among experts. However, we ﬁnd no evidence of language specialization (see\\nTable 15). Routers instead pass tokens from English, Japanese, French and Chinese indiscriminately\\nand the experts appear to be multilingual . But this lack of language specialization is less surprising\\nwhen considering the mechanism of token routing and load balancing. Since each group of tokens\\nmay only contain one, to at most a few, languages (a group usually consists of 2-4 sequences in\\nour setup), then all experts are encouraged to handle tokens from all languages. We experimented\\nwith a global load balance loss, however, this usually results in worse load-balance and worse model\\nperformance, so we leave further improving multilingual expert models as an area of open work\\n(Section 9).\\nOur visualization reveals apparent specialization learned in our models (Tables 13, 15) for the en-\\ncoder layers. Other expert specializations were also observed in the appendix of Shazeer et al.\\n(2017). However, this leads to an interesting question of how architectures that eliminate learned\\nrouting Roller et al. (2021); Zuo et al. (2021) appear to perform well. An extensive study of the\\nscaling properties of learned versus random routing could prove helpful as future work and help\\nguide us to a better understanding of routing behavior.\\n8 R ELATED WORK\\nMixture-of-Experts (MoE) date back at least three decade history to the work of Jacobs et al. (1991);\\nJordan and Jacobs (1994). In initial concepts, the MoE deﬁned the entire neural network akin to\\nensemble methods. But later Eigen et al. (2013) extended the idea of including MoE as a component\\nas part of deeper networks. Shazeer et al. (2017) then scaled this idea to a 137B parameter model to\\nachieve state-of-the-art in machine translation. Most of the later work (including ours) follows this\\nMoE as a component approach.\\nScale in natural language processing. The remarkable success of scale in natural language pro-\\ncessing (Kaplan et al., 2020; Brown et al., 2020) has reinvigorated MoE research evidenced by a\\n21\\nsurge of recent work (Lepikhin et al., 2020; Fedus et al., 2021; Yang et al., 2021; Kim et al., 2021;\\nDu et al., 2021; Artetxe et al., 2021; Zuo et al., 2021; Clark et al., 2022). Sparse expert models\\nhave been proposed as a method to achieve the results of large-scale dense models, more efﬁciently.\\nFedus et al. (2021) showed a 4x pre-train speed-up over T5-XXL (Raffel et al., 2019) and Du et al.\\n(2021) matched the quality of GPT-3 (Brown et al., 2020) using only 1=3of the energy. And in the\\nspan of the last twelve months, a milestone of efﬁciently training trillion parameter deep neural net-\\nworks has been achieved by multiple groups (Fedus et al., 2021; Yang et al., 2021; Du et al., 2021),\\nand most recently, Lin et al. (2021) introduced techniques to train a 10T parameter model. One side\\nnote is that the recent signiﬁcant successes of sparse expert models have often been in settings with\\na lot of data and no distribution shift – two examples being language modeling/span corruption and\\nmachine translation (Shazeer et al., 2017; Lepikhin et al., 2020; Kim et al., 2021; Fedus et al., 2021).\\nIn contrast, discrepancies between strong pre-training quality and poor ﬁne-tuning quality for sparse\\nmodels have been observed in Fedus et al. (2021); Narang et al. (2021); Artetxe et al. (2021), but we\\nexpect advances in regularization techniques to continue to improve downstream quality.\\nTowards better routing algorithms. BASE layers (Lewis et al., 2021) recasts token routing as a\\nlinear assignment problem – removing the need for load balancing auxiliary losses. This work also\\ndemonstrated the efﬁcacy of a single expert layer. Clark et al. (2022) studies in depth the scaling\\nproperties of a few different routing algorithms and propose their own variant of BASE layers that\\nuses an optimal transport formulation. Yang et al. (2021) introduces the M6-T architecture and\\nexpert prototyping which splits experts into different groups and applies ktop-1 routing procedures\\n(contrasting with the top- krouting commonly used elsewhere). Hazimeh et al. (2021) proposed a\\ncontinuously differentiable sparse gate with demonstrated improvements over vanilla top- kgating.\\nOther work (Bengio et al., 2016) considered casting the routing selection as a reinforcement learning\\nproblem. More radical versions remove learning the routing entirely. Hash layers (Roller et al.,\\n2021) shows random ﬁxed routing (per hash functions) led to competitive performance with learned\\nrouting. Zuo et al. (2021) also proposed an algorithm which randomly selects experts during training\\nand inference and found gains of 2 BLEU points over Switch Transformers and competitive scores\\nwith the larger models of Kim et al. (2021). Finally, Fan et al. (2021) designs an architecture with\\nexplicit language-speciﬁc sublayers (rather than allowing arbitrary routing as done in Lepikhin et al.\\n(2020)) to yield gains of +1 BLEU.\\nSparse expert models in other modalities. MoE and sparse experts model have also advanced\\nresults in modalities aside from language. Riquelme et al. (2021) designed a 15B parameter V-MoE\\nto match state-of-the-art ImageNet (Deng et al., 2009) models with fewer computational resources.\\nLou et al. (2021) similarly showed a beneﬁt over dense vision models by using MoE layers across\\nboth image patch and channel dimensions. Additionally, Automatic Speech Recognition has been\\nimproved by the SpeechMoE variants (You et al., 2021a;b). Kumatani et al. (2021) reduced word\\nerror rates using MoE models in Sequence-to-Sequence Transformer and Transformer Transducer.\\nImproving deployment of sparse models. Initial expert designs (including this work) route each\\ntoken separately to experts at that layer. One issue is that these type of architectures may be burden-\\nsome to serve since it requires sufﬁcient memory for storing the parameters. Distillation was shown\\nin Fedus et al. (2021) to be moderately effective, but recent approaches modiﬁed the routing to in-\\nstead route full sentences or tasks (Kudugunta et al., 2021b; Zuo et al., 2021) which then permits\\nextraction of sub-networks at time of serving (e.g. deploy only the network associated with the new\\ntask). As an alternative to distillation, Kim et al. (2021) considers directly pruning away experts not\\nessential to the task of interest.\\nMultitask learning with MoE. We conclude our tour of recent MoE research with successes in\\nmultitask settings. Ma et al. (2018) recommended using a separate gating or router network for each\\ntask, an idea that may soon be revisited for Transformer architectures. Finally, Gururangan et al.\\n(2021) recommends even greater modularity of language models and conditionally activates experts\\nbased on the domain/task label or by an inferred label.\\n9 D ISCUSSION\\nWhile this work is on sparse models, these models intersect with many other interesting topics\\nin machine learning such as adaptive computation, low-precision training, scaling principles, and\\n22\\nneural network architecture advances. Our discussion therefore covers a broader range of topics\\nsurfaced during this research.\\nUnpredictable dynamics when pre-training on multilingual data. We often observe that the\\nsame model pre-trained on multilingual data will yield smaller pre-training speed-ups and be more\\nunstable. One hypothesis is that this is due to the variance of sequences per group across batches.\\nAs a reminder, we encourage tokens in a group to be load-balanced. There are usually only 2-8\\nsequences per group (higher becomes expensive) where each sequence is written in a single lan-\\nguage. Therefore, at most 2-8 languages must be balanced across experts – even when training with\\nover 100 languages. This leads to high variance across groups and batches, resulting in chaotic and\\nunpredictable routing. In a follow-up experiment (just highlighted for brevity), we pre-trained on\\na mixture of English C4 plus a small fraction of a ﬁne-tuning task which similarly resulted in an\\nunstable model.\\nThe robustness of sparse models. Despite a paper focused on the details of sparse model-\\nparticulars, zooming out we ﬁnd them to be robust to a wide set of hyperparameters and architectural\\nchanges. Sparse models obtain great performance under a variety of routing algorithms, dropping\\nhigh fractions of tokens, and different hyperparameters. While we did point out the importance of\\ntuning the batch size and learning rate for ﬁne-tuning, our intuition, in-line with Kaplan et al. (2020),\\nis that the real winner is scale. For instance, Table 8 shows larger gains to be had by simply increas-\\ning the capacity factor (i.e. FLOPs) rather than by more sophisticated routing (i.e. algorithms).\\nAdaptive computation. Sparse models are a subclass of adaptive computation models since each\\ninput gets different computation applied to it. In sparse models a token is routed to the expert(s)\\nof its choosing. When capacity factors are less than one, the model learns to not apply computa-\\ntion to certain tokens. This has shown promise in computer vision (Riquelme et al., 2021) and our\\nlanguage experiments (Appendix D). We envision future models expanding this through heteroge-\\nneous experts (e.g. each expert applies differing computation). Intuitively, different input examples\\nwill likely require different amounts of processing depending on difﬁculty. Future models in this\\ndirection will be efﬁciently enabled through emerging computing infrastructures (Dean, 2021).\\nGeneralizing ﬁndings from small to large scale. A key issue we faced throughout our work was\\nidentifying small scale models and training setups that reﬂect larger scale experiments. This was\\nevident in our stability studies in Section 3 where experiments had to be run with XL sized models\\nto surface relevant dynamics. For our architecture and routing algorithm experiments, we often ﬁnd\\nimprovements vanish, or even reverse, when models are trained for longer or made larger. As one\\nexample, the top- nﬁndings of Fedus et al. (2021) were reversed in our 8x larger-scale experiments\\npresented here, which revealed small boosts of top- (n+ 1) routing over top- nrouting (see Table 8).\\nTraining models with even lower precision. The best method we found to stabilize our models\\nwithout hurting (and sometimes improving) quality was the router z-loss. This is an auxiliary loss\\nthat encourages the model logits to have values smaller in absolute magnitude. Given the max range\\nof numbers float32 andbfloat16 can support (\\x183e38), this leads us to believe most of this\\nrange is not needed, and compressing it actually might improve model training dynamics. Therefore,\\nfuture precision formats might take into account more compressed exponential ranges to train certain\\nclasses of models.\\nDesigning new operations with more multiplicative interactions. Section 3.1 shows that op-\\nerations with more multiplicative interactions than additions, or those that don’t accumulate over\\nmany numbers, improve model performance. We test this further by injecting more multiplicative\\ninteractions into expert layers which speedup pre-training by 4% without any change to step-time\\n(Appendix C). We think this hints at promising architectural improvements for models and could be a\\ngood design principle. Recently depthwise convolutions, which only accumulate 3-5 elements, have\\nalso been shown to greatly improve Transformer performance (So et al., 2021). These operations\\nare especially exciting as elementwise multiplications typically do not introduce any communication\\noverhead when using model parallelism (which makes operations like depthwise convolutions and\\nour multiplicative interactions very efﬁcient). While we did note these methods to increase model\\ninstabilities in Section 3.1, using the router z-loss in our models prevented any further instabilities.\\n23\\nConstrain activations to alleviate other undesirable model scaling dynamics. We observed\\ntwo additional sources of training instability. (1)Encoder-decoder models are more unstable than\\ndecoder only models (for ﬁxed amount of FLOPs). Encoder-decoder models have a higher ratio\\nof attention layers (e.g. more exponential functions) due to having both self-attention and enc-dec\\nattention layers for each FFN on the decoder. (2)Deeper models are more unstable than shallower\\nmodels for a ﬁxed amount of FLOPs. Deeper models also introduce more exponential functions\\nthrough additional attention layers. We hypothesize that a contributing factor to both of these obser-\\nvations is simply the increased number of exponential functions found in the network. Future work\\ncould look at resolving these training dynamics by adding z-loss penalties to the attention softmaxes\\nfor non-sparse models, especially since we observed adding them didn’t change model quality.\\nDense and sparse models depend differently on hyperparameters. Our ﬁne-tuning analysis\\nin Section 4.3 shows optimal ﬁne-tuning hyperparameters differ signiﬁcantly between dense and\\nsparse models. In certain settings, ﬁne-tuning hyperparamters that worked well for the dense model\\nmasked any improvements from the sparse model (despite large pre-training speedups). For new\\nmodel classes, we recommend researchers and practitioners to extensively test key hyperparameters\\nbefore prematurely abandoning a method.\\n10 C ONCLUSION\\nWe temper the over-exuberance for scale in Fedus et al. (2021) by showing how a model with 1/5th\\nthe size, but with a better balance of computation (FLOPs) to parameters – is a more effective sparse\\nlearner. Furthermore, this improves the usability of sparse models since it can be deployed with less\\nmemory overhead. Using our sparse model variant, we achieve SOTA across a wide range of the\\nmost competitive public benchmarks. We hope this work shows the power of model sparsity and\\naccelerates the adoption of such models.\\nACKNOWLEDGEMENTS\\nWe would like to thank Alex Passos, Ekin Cubuk, Margaret Li, Noah Constant, Oriol Vinyals,\\nBasil Mustafa, Joan Puigcerver, Diego de Las Casas, Mike Lewis, and Ryan Sepassi for detailed\\ncomments and feedback on early versions of the draft. We also thank the Google Brain Team for\\nuseful discussions throughout the course of this work.\\n24\\nREFERENCES\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria\\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui\\nChen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo,\\nJeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. Efﬁcient large\\nscale language modeling with mixtures of experts, 2021.\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation\\nin neural networks for faster models, 2016.\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural\\nlanguage processing , pages 1533–1544, 2013.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv preprint arXiv:2005.14165 , 2020.\\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,\\nBogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Uniﬁed scaling laws for\\nrouted language models. arXiv preprint arXiv:2202.01169 , 2022.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457 , 2018.\\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\\ngated convolutional networks. In International conference on machine learning , pages 933–941.\\nPMLR, 2017.\\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: In-\\nvestigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung ,\\nvolume 23, pages 107–124, 2019.\\nJeff Dean. Introducing pathways: A next-generation ai architecture. Google AI Blog , 2021.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\\npages 248–255. Ieee, 2009.\\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise\\nquantization, 2021.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,\\nZongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy\\nMeier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen,\\nand Claire Cui. Glam: Efﬁcient scaling of language models with mixture-of-experts, 2021.\\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\\nmixture of experts. arXiv preprint arXiv:1312.4314 , 2013.\\n25\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Man-\\ndeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond english-centric\\nmultilingual machine translation. Journal of Machine Learning Research , 22(107):1–48, 2021.\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: Better text generation via ﬁlling in\\nthe.arXiv preprint arXiv:1801.07736 , 2018.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961 , 2021.\\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. Demix\\nlayers: Disentangling domains for modular language modeling, 2021.\\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\\nRahul Mazumder, Lichan Hong, and Ed H. Chi. Dselect-k: Differentiable selection in the mixture\\nof experts with applications to multi-task learning, 2021.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition, 2015.\\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\\narXiv:1606.08415 , 2016.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.\\nIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\\nin Neural Information Processing Systems , volume 28, pages 1693–1701. Curran Asso-\\nciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\\nafdec7005cc9f14302cd0474fd0f3c96-Paper.pdf .\\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\\n1735–1780, 1997.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp.\\nInInternational Conference on Machine Learning , pages 2790–2799. PMLR, 2019.\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. In International conference on machine learning , pages 448–\\n456. PMLR, 2015.\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\\nlocal experts. Neural computation , 3(1):79–87, 1991.\\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.\\nNeural computation , 6(2):181–214, 1994.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361 , 2020.\\nVladimir Karpukhin, Barlas Ouz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen tau Yih. Dense passage retrieval for open-domain question answering, 2020.\\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu,\\nAmr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efﬁ-\\ncient moe training for multitask multilingual models, 2021.\\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 , 2018.\\n26\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efﬁcient inference.\\narXiv preprint arXiv:2110.03742 , 2021a.\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efﬁcient inference.\\narXiv preprint arXiv:2110.03742 , 2021b.\\nKenichi Kumatani, Robert Gmyr, Felipe Cruz Salinas, Linquan Liu, Wei Zuo, Devang Patel, Eric\\nSun, and Yu Shi. Building a great multi-lingual teacher with sparsely-gated mixture of experts for\\nspeech recognition, 2021.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 7:453–466, 2019.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt\\ntuning. arXiv preprint arXiv:2104.08691 , 2021.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\\nSimplifying training of large, sparse models. arXiv preprint arXiv:2103.16716 , 2021.\\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv\\npreprint arXiv:2101.00190 , 2021.\\nXiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, and\\nTie-Yan Liu. R-drop: Regularized dropout for neural networks, 2021.\\nJunyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang,\\nYong Li, Wei Lin, Jingren Zhou, and Hongxia Yang. M6-10t: A sharing-delinking paradigm for\\nefﬁcient multi-trillion parameter pretraining, 2021.\\nYuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. Sparse-mlp: A fully-mlp architecture\\nwith conditional computation, 2021.\\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relation-\\nships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining , pages 1930–1939,\\n2018.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\\ntraining. arXiv preprint arXiv:1710.03740 , 2017.\\nVinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.\\nInIcml, 2010.\\nSharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Kar-\\nishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modiﬁcations\\ntransfer across implementations and applications? arXiv preprint arXiv:2102.11972 , 2021.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum-\\nmary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint\\narXiv:1808.08745 , 2018.\\nArvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and\\nJames Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint\\narXiv:1511.06807 , 2015.\\n27\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversar-\\nial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599 ,\\n2019.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\\ninstructions with human feedback. 2022.\\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\\nnetworks. In International conference on machine learning , pages 1310–1318. PMLR, 2013.\\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,\\nDavid So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv\\npreprint arXiv:2104.10350 , 2021.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kun-\\ncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Men-\\nsch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yu-\\njia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Au-\\nrelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training go-\\npher, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683 , 2019.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250 , 2016.\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr ´e Su-\\nsano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\\narXiv preprint arXiv:2106.05974 , 2021.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\\nparameters of a language model? arXiv preprint arXiv:2002.08910 , 2020.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse\\nmodels. arXiv preprint arXiv:2106.04426 , 2021.\\nSascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. A simple\\nrecipe for multilingual grammatical error correction. arXiv preprint arXiv:2106.03830 , 2021.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\\nsarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence , volume 34, pages 8732–8740, 2020.\\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\\ntraining of deep neural networks. Advances in neural information processing systems , 29:901–\\n909, 2016.\\nNoam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\\narXiv:1911.02150 , 2019.\\n28\\nNoam Shazeer. Glu variants improve transformer, 2020.\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\\nInInternational Conference on Machine Learning , pages 4596–4604. PMLR, 2018.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538 , 2017.\\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep\\nlearning for supercomputers. In Advances in Neural Information Processing Systems , pages\\n10414–10423, 2018.\\nSam Shleifer, Jason Weston, and Myle Ott. Normformer: Improved transformer pretraining with\\nextra normalization. arXiv preprint arXiv:2110.09456 , 2021.\\nDavid R So, Wojciech Ma ´nke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Primer:\\nSearching for efﬁcient transformers for language modeling. arXiv preprint arXiv:2109.08668 ,\\n2021.\\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\\nDropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning\\nResearch , 15(1):1929–1958, 2014. URL http://www.cs.toronto.edu/ ˜rsalakhu/\\npapers/srivastava14a.pdf .\\nNassim Nicholas Taleb. Antifragile: Things that gain from disorder , volume 3. Random House\\nIncorporated, 2012.\\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan\\nNarang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efﬁciently: Insights from\\npre-training and ﬁne-tuning transformers. arXiv preprint arXiv:2109.10686 , 2021.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\\nprocessing systems , pages 5998–6008, 2017.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language\\nunderstanding systems. In Advances in Neural Information Processing Systems , pages 3266–\\n3280, 2019.\\nShuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Jun-\\nyuan Shang, Yanbin Zhao, Chao Pang, et al. Ernie 3.0 titan: Exploring larger-scale knowledge en-\\nhanced pre-training for language understanding and generation. arXiv preprint arXiv:2112.12731 ,\\n2021.\\nSamuel Williams, Andrew Waterman, and David Patterson. Rooﬂine: an insightful visual perfor-\\nmance model for multicore architectures. Communications of the ACM , 52(4):65–76, 2009.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-\\nlation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\\nBarua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv\\npreprint arXiv:2010.11934 , 2020.\\nAn Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jia-\\nmang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, and Hongxia Yang. M6-t:\\nExploring sparse expert models and beyond, 2021.\\n29\\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe: Scaling to large acoustic models with\\ndynamic routing mixture of experts, 2021a.\\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe2: Mixture-of-experts model with im-\\nproved routing, 2021b.\\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. arXiv preprint\\narXiv:1910.07467 , 2019.\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with extracted\\ngap-sentences for abstractive summarization, 2020.\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\\narXiv preprint arXiv:1810.12885 , 2018.\\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and\\nJianfeng Gao. Taming sparsely activated transformer with stochastic experts, 2021.\\n30\\nA T OKEN LOAD BALANCE DESCRIPTION\\nThe auxiliary load balancing loss from Shazeer et al. (2017) is also used to here to balance tokens\\nacross experts. Assume we have Nexperts indexed by i= 1toNand a batchBwithTtokens. The\\nauxiliary loss is computed as the scaled dot-product between vectors f and P,\\nloss=\\x0b\\x01N\\x01NX\\ni=1fi\\x01Pi (7)\\nwherefiis the fraction of tokens dispatched to expert i,\\nfi=1\\nTX\\nx2B1fargmaxp(x);ig (8)\\nandPiis the fraction of the router probability allocated for expert i,2\\nPi=1\\nTX\\nx2Bpi(x) (9)\\nSince we seek uniform routing of the batch of tokens across the Nexperts, we desire both vectors\\nto have values of 1=N. The auxiliary loss of Equation 7 encourages uniform routing since it is\\nminimized under a uniform distribution. The objective can also be differentiated as the P-vector is\\ndifferentiable, but the f-vector is not. The ﬁnal loss is multiplied by expert count Nto keep the loss\\nconstant as the number of experts varies since under uniform routingPN\\n1(fi\\x01Pi) =PN\\n1(1\\nN\\x011\\nN) =\\n1\\nN. Finally, a hyperparameter \\x0bis a multiplicative coefﬁcient for these auxiliary losses; throughout\\nthis work we use an \\x0b= 10\\x002which was sufﬁciently large to ensure load balancing while small\\nenough to not to overwhelm the primary cross-entropy objective.\\nB R OUTER Z-L OSSTRAINING DYNAMICS\\nFigure 7 plots the router z-loss from Equation 5 across a coefﬁcient sweep where the best value of\\ncz= 0:001is plotted in green for the encoder and decoder.\\n0 25000 50000 75000 100000 125000 150000\\nStep0510152025303540Z-Loss0.0\\n1e-4\\n1e-3\\n1e-2\\n1e-1\\n0 25000 50000 75000 100000 125000 150000\\nStep0102030405060Z-Loss0.0\\n1e-4\\n1e-3\\n1e-2\\n1e-1\\nFigure 7: Sweeping loss coefﬁcient ( cz) for Router Z-Loss. We plot the router z-losses over the\\ncourse of pre-training without router z-loss (blue) and with increasing values of cz(we selected\\ncoefﬁcient associated with green curve for all later experiments). With values of 1e-2 , or larger,\\nthe z-loss shrinks near to zero. The left plot shows an encoder layer and the right plot shows a\\ndecoder layer.\\n2A potential source of confusion: pi(x)is the probability of routing token xto expert i.Piis the probability\\nfraction to expert iacross all tokens in the batchB.\\n31\\nC I MPROVED ARCHITECTURAL MODIFICATIONS\\nWe consider a few small architecture variations here. The ﬁrst modiﬁcation was adding additional\\nFFN layers (feed-forward network, see Table 1 for more details) immediately before or after each\\nMoE layer (referred to as Sparse-Dense). Table 16 reveals the effectiveness of an FFN layer im-\\nmediately preceding or following each sparse layer and that these extra FFN layers help less when\\nadded elsewhere in the network. Guaranteeing all tokens have at least one FFN applied to them\\nbetween each attention layer appears useful.\\nModel Neg. Log Perp. (\") \\x01\\nDense model (baseline) -1.474 -\\nDense model w/ extra FFN layers -1.452 0.022\\nSparse model (baseline) -1.383 -\\nSparse model w/ extra FFN layer after each sparse layer -1.369 0.014\\nSparse model w/ extra FFN layer before each sparse layer -1.369 0.014\\nSparse model w/ extra FNN layers placed randomly in the network -1.376 0.007\\nTable 16: A dense FFN immediately before or after each sparse layer improves quality . Insert-\\ning an extra dense FFN immediately before or after each sparse layer improves quality 2x as much\\nas placing the dense layers (randomly) elsewhere in the network. All of the non-baseline models\\nhave the same amount of FFN layers added for fair comparisons. Note that improving perplexity\\nbecomes harder as the model gets better.\\nSecond, we introduce an additional bias in the expert layers. All our models use the GELU-Linear\\nFFN (Shazeer, 2020), rather than the ReLU FFN:\\nFFN ReLU(x) = ( ReLU (xW 1))W2\\nFFN GEGLU (x) = ( GELU (xW 11)\\x0cxW 12)W2\\nThe additive bias is a learned weight ( B) added after the ﬁrst matrix multiplication in the FFN layer\\nof shape [batch;d ff]. The multiplicative bias (also referred to as a scale parameter) is a learned\\nweight of the same shape, but does an elementwise multiplication. We initialize the additive bias to\\nzeros and the multiplicative bias to ones.\\nFFN GEGLU +Add Bias (x) = [( GELU (xW 11)\\x0cxW 12) +B]W2\\nFFN GEGLU +Mult Bias (x) = [( GELU (xW 11)\\x0cxW 12)\\x0cB]W2\\nTable 17 shows the results of our different methods. Both the additive and multiplicative biases are\\nessentially free: cheap to compute, adds few new parameters, and incurs no additional communi-\\ncation costs with model and expert parallelism. When using our router z-loss from Section 3.1, we\\nobserve no instabilities from the multiplicative bias. We do see that the multiplicative interactions\\nimprove performance, achieving a 4% speedup in convergence time over our strong sparse baseline.\\nThis hints that a promising avenue for future architectural research is ﬁnding new ways of adding\\nmore multiplicative interactions into networks.\\nModel Neg. Log. Perp. (\") \\x01\\nDense Baseline -1.474 -\\nSparse Baseline -1.369 -\\nSparse +Additive Bias -1.371 -0.002\\nSparse +Multiplicative Bias -1.361 0.008\\nTable 17: More multiplicative interactions improve sparse model quality. Both the additive and\\nthe multiplicative bias add virtually no parameters or compute.\\nFinally, motivated by the work of Roller et al. (2021), we explored similar methods, but did not ﬁnd\\nimprovements in our setting. We tried routing using the word embedding exclusively, as well as\\n32\\nan additional input to the layer embedding for routing decisions. We toggled stopping the gradient\\nthrough the word embedding or allowing it to have gradients propagated from the router. Using only\\nthe word embedding hurt quality, while using it in addition to the normal layer hidden activation was\\ninitially positive, but after pre-training for 50B+ tokens on models of scale 1B+ dense parameters it\\nhad a neutral effect. Appendix J has further details on the experiments with negative results.\\nD B ATCH PRIORITIZED ROUTING FOR LOWER CAPACITY FACTORS\\nSurprisingly, top-1 and top-2 routing work well with CF less than 1.0 despite token routing being\\ndone in a left to right order over the sequence. If Ntokens are sent to an expert with only Mspaces\\nthenN >M tokens will dropped. The ordering of the dropping is important: we drop tokens going\\nleft to right (e.g. tokens earlier in the sentence will be routed ﬁrst over the end tokens). This is done\\nto avoid the model cheating. If we dropped tokens in another ordering, the model gets information\\non what tokens are occurring later in the sequence based on if tokens are being dropped or not.\\nBatch Prioritized Routing (BPR) from Riquelme et al. (2021) was introduced in Vision Transformers\\n(Dosovitskiy et al., 2020) for image classiﬁcation. Our work explores BPR with top-1 routing in the\\ncontext of language modeling. BPR aims to have a global view of all tokens to determine which\\ntokens should be dropped instead of the left-to-right ordering. The algorithm works by looking at\\nall N tokens getting sent to Expert iand then only routing the Mones with the highest probabilities\\nfrom the router. Table 18 shows that BPR top-1 routing improves performance over top-2 routing,\\nespecially when capacity factors are less than 1.0. We leave it to future work to try top- nBPR\\nrouting, which will hopefully yield larger improvments for higher capacity factors.\\nImportantly, BPR routing can only be done on the encoder side of the encoder-decoder model. On\\nthe encoder side there are not autoregressive predictions and all tokens can see each other. If you use\\nBPR on the decoder, it learns to cheat by using future token information to improve current token\\npredictions.\\n33\\nAlgorithm Train CF Eval CF Neg. Log. Perp. (\")\\nDense — — -1.474\\nDense-L — — -1.384\\nBPR Top-1 0.5 0.5 -1.433\\nBPR Top-1 0.5 2.0 -1.416\\nTop-1 0.75 0.75 -1.428\\nTop-1 0.75 2.0 -1.404\\nTop-2 0.75 0.75 -1.424\\nTop-2 0.75 2.0 -1.402\\nBPR Top-1 0.75 0.75 -1.409\\nBPR Top-1 0.75 2.0 -1.397\\nTop-1 1.0 1.0 -1.397\\nTop-1 1.0 2.0 -1.384\\nTop-2 1.0 1.0 -1.392\\nTop-2 1.0 2.0 -1.378\\nBPR Top-1 1.0 1.0 -1.386\\nBPR Top-1 1.0 2.0 -1.379\\nTop-1 1.25 1.25 -1.378\\nTop-1 1.25 2.0 -1.373\\nTop-2 1.25 1.25 -1.375\\nTop-2 1.25 2.0 -1.369\\nBPR Top-1 1.25 1.25 -1.376\\nBPR Top-1 1.25 2.0 -1.375\\nTable 18: Batch Prioritized Top-1 Routing (BPR) performance. BPR top-1 routing improves\\nquality when capacity factors are \\x141. However, once the capacity factor reaches 1.25, the improve-\\nments greatly diminish and it underperforms top-2 routing. Future work can try BPR with top-2\\nrouting, which should hopefully further improve the performance.\\nE P RE-TRAINING DATASET DETAILS\\nThe pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and\\nthe dataset introduced in GLaM (Du et al., 2021).\\nDataset Tokens (B) Weight in Mixture\\nFiltered C4 183 0.17\\nFiltered Webpages 143 0.34\\nWikipedia 3 0.05\\nConversations 174 0.23\\nForums 247 0.02\\nBooks 390 0.17\\nNews 650 0.02\\nTable 19: Data and mixture weights in the training set . We sample from different dataset sources\\nwith probability proportional to “weight in mixture”. The number of tokens listed are in billions (B).\\nFor more details on the C4 corpus see Raffel et al. (2019) and for the other datasets see Du et al.\\n(2021).\\n34\\nF F ULL FINE-TUNING SENSITIVITY DATA\\nTable 20 contains the raw data for Figure 6 measuring the ﬁne-tuning protocol sensitivity. Dense\\nand Sparse are encoder-decoder models FLOP matched to T5-Large that were pre-trained for 500k\\nsteps with a batch size of 1M tokens on the C4 corpus.\\nModel Learning Rate Batch Size Reset Optimizer Slot Vars SuperGLUE (\")\\nDense 1e-3 1M 84.8\\nDense 1e-3 1M X 84.3\\nDense 5e-4 1M 84.8\\nDense 5e-4 1M X 84.2\\nDense 1e-4 1M 84.0\\nDense 1e-4 1M X 84.8\\nDense 1e-3 262k 84.9\\nDense 1e-3 262k X 83.7\\nDense 5e-4 262k 84.9\\nDense 5e-4 262k X 84.0\\nDense 1e-4 262k 85.1\\nDense 1e-4 262k X 85.0\\nDense 1e-3 65k 83.7\\nDense 1e-3 65k X 82.5\\nDense 5e-4 65k 84.4\\nDense 5e-4 65k X 84.1\\nDense 1e-4 65k 84.9\\nDense 1e-4 65k X 84.6\\nSparse 1e-3 1M 86.9\\nSparse 1e-3 1M X 85.9\\nSparse 5e-4 1M 86.1\\nSparse 5e-4 1M X 83.5\\nSparse 1e-4 1M 84.3\\nSparse 1e-4 1M X 84.3\\nSparse 1e-3 262k 86.2\\nSparse 1e-3 262k X 85.2\\nSparse 5e-4 262k 85.5\\nSparse 5e-4 262k X 84.8\\nSparse 1e-4 262k 85.1\\nSparse 1e-4 262k X 85.5\\nSparse 1e-3 65k 85.8\\nSparse 1e-3 65k X 85.5\\nSparse 5e-4 65k 86.5\\nSparse 5e-4 65k X 85.1\\nSparse 1e-4 65k 85.6\\nSparse 1e-4 65k X 84.5\\nTable 20: Fine-tuning protocol sensitivity. We vary the batch size, learning rate and whether we\\nreset the optimizer slot variables for both dense and sparse models. Resetting the optimizer state\\nduring ﬁne-tuning hurts performance. We observe a difference in optimal batch size and learning\\nrate for sparse vs. dense models. Certain hyperparameter ﬁne-tuning settings make the sparse and\\ndense models perform almost exactly the same, showing the importance of correctly tuning the\\nhyperparameters.\\n35\\nG O PTIMALLY SETTING THE ROUTING THRESHOLD\\nTop-nRouting Algorithm\\n1.Route each token xto the expert with the highest router probability (gate1(x)).\\n2.Normalize the top- nexpert router scores for each token x, so gatei=gatei(x)Pn\\ni=1gatei(x).\\n3.Route the token to the other n-1 experts (indexed by i) with probability min (1:0;gatei(x)\\nthreshold).\\nThreshold is a predeﬁned hyperparameter that is typically set to 0.2.\\nWe describe the MoE hyperparameters and how they should change as the routing algorithm\\nchanges. The MoE top-2 routing algorithm (Shazeer et al., 2017; 2018; Lepikhin et al., 2020) works\\nas follows: ﬁrst the router ﬁnds the expert that is assigned the higher router score (gate1) and always\\nsends the token to that expert. The token is also sent to its second highest expert with probability\\nmin(1:0;gate2=threshold ). The threshold is a hyperparameter that is typically set to 0.2, and gate2is\\nthe token’s router probability for the second highest expert. Note that gate1and gate2get normalized\\nby the sum of their two scores, so they sum to one.\\nWe trivially extend the top-2 algorithm to work for top- nrouting here. Take the scores of the top- n\\nexperts per token and sum them, then renormalize each expert router score based on that sum. If the\\nspeciﬁc renormalized expert score has a higher value than the threshold (e.g. 0.2), then the token will\\nbe routed, otherwise it will be routed with probabilityscore\\nthreshold. At a high level this only routes the\\ntoken to the next n-1 experts if their scores are not too much lower than the highest scored expert.\\nFor top-3 routing vs top-2, the sum that the expert scores are normalized by is larger, therefore\\nwe experimented with decreasing the threshold. Our experimental results are shown in Table 21.\\nInterestingly, we do observe the top-3 routing to slightly beneﬁt from the lower threshold, while the\\nopposite is true for top-2 routing.\\nWe also experimented with an absolute threshold policy instead of a relative one. This is where the\\nnextn-1 tokens will be routed only if their router score is great than some pre-deﬁned value (e.g.\\n0.2). We found it can achieve as good of performance if the threshold value is tuned.\\nAlgorithm Train CF Threshold Neg. Log. Perp. (\")\\nDense — — -1.474\\nDense-L — — -1.384\\nTop-2 3.0 0.2 -1.354\\nTop-2 3.0 0.05 -1.356\\nTop-3 3.0 0.2 -1.351\\nTop-3 3.0 0.05 -1.349\\nTable 21: Performance of top-2 and top-3 routing with different thresholds. Top-3 routing does\\nslightly better with lower thresholds than top-2 routing.\\nH M ESH LAYOUT FOR DATA, MODEL AND EXPERT PARALLELISM WITH\\nFEWEXPERTS\\nWe use data and model parallelism partitioning with Mesh-Tensorﬂow (Shazeer et al., 2018). The\\npartitioning strategy works by ﬁrst forming a logical 2D mesh of size dxm, with the rows corre-\\nsponding to the data dimension ( d) and the columns as the model dimension ( m) and the product\\nequal to the total number of cores, n=dxm. This mesh is only an abstraction. Each logical core\\nmust be mapped to a physical core, which is optimized through performance tuning.\\nAs a refresher, each row in the mesh will have its own unique slice of the data and each column will\\nhave a unique slice of the model weights. The ﬁnal gradient allreduce communication occurs\\nacross each individual column. The model parallelism allreduce communications occur across\\neach row in the mesh. One constraint from this approach is that the number of rows must evenly\\n36\\nModel DimensionData Dimension\\nOuter Data DimensionInner Data Dimension\\nModel DimensionExperts >= Data Dimension Experts < Data DimensionFigure 8: Data and model parallelism meshes used for distributing models. In this example there\\nare a total of 32 processors (e.g. n= 32 ).(Left) A valid 2D mesh if the number of experts is greater\\nthan or equal to the data parallelism dimension. The data dimension has 8 rows ( d) and the model\\ndimension has 4 columns ( m).(Right) A valid 3D mesh when we have fewer experts than the data\\nparallelism dimension. The batch dimension is factorized into two new dimensions: inner data and\\nouter data dimensions. Now we have 1 expert per inner data dimension ( i). The 8 data rows in the\\nleft ﬁgure become 4 in the outer batch ( o) and 2 in the inner batch ( i) with 2 experts instead of 8.\\ndivide the number of data sequences and the number of columns must evenly divide the model\\ndimensions being partitioned.\\nBut if we have fewer thandexperts then this layout will not work. To allow for fewer experts than\\ndata parallelism rows in our mesh, we factorize the data dimension into two new dimensions: inner\\n(i) and outer ( o) whereixo=dand the number of experts equals i. This transforms the logical 2D\\nmesh of shape dxminto a 3D mesh of shape oxixm. See Figure 8 for a visualization of both\\nmeshes12.\\nI N OTE ON COMMUNICATION COSTS FOR DISTRIBUTED MODELS\\nCommunication operations ( allreduce andall2all ) can signiﬁcantly impact sparse model\\ntraining throughput (see Table 1 for a description of the communication operations). allreduce\\ncalls are executed along model and batch dimensions, typically dominated by the model dimension\\nallreduce calls that sum results of partial matrix multiplication operations from the workers.\\nThese calls are needed when matrix multiplications are partitioned across multiple cores (e.g. model\\nparallelism). The gradient summation allreduce calls can be amortized away by training models\\nwith larger batch sizes since the gradient accumulation allreduce communication cost is inde-\\npendent of the batch size. To alleviate the memory issues of larger batch sizes, microbatches can\\nbe used. Microbatches do this by splitting the batch into nevenly divisible chunks and computing\\ngradients on each sequentially, then summing.\\nTo increase the allreduce throughput, more workers may need to be assigned to the model di-\\nmension (instead of batch dimension). However, increasing the number of workers may reduce\\ncompute per worker resulting in higher communication overheads that cancel some of the gains\\nfrom higher communication throughput from allreduce . For the results in this paper, ﬁrst we\\nexplored various model partitioning strategies. Next the shapes of the pre-training jobs were al-\\nlocated based on performance benchmarking which showed the lowest cumulative communication\\noverheads in allreduce andall2all .\\n12See Mesh Tensorﬂow for more details on the inner and outer batch: https://github.com/\\ntensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py\\n37\\nJ N EGATIVE RESULTS\\nWe conclude with some ideas that yielded negative results in our setting.\\nAdding information if tokens were dropped to the router. We experimented with having the\\nexpert layer have information of whether the token was routed or dropped in the previous expert\\nlayers. We implemented this through counting the number of times a token was routed in all pre-\\nvious expert layers, having embeddings for each possible value and then adding this to the router\\nembedding. We found that this made no difference in performance.\\nAdding explicit expert positional information. We experimented with adding explicit positional\\ninformation into the outputs of the expert layer. We wanted to see if it either improved performance\\nor sped up convergence during the beginning of training when expert layers were drastically chang-\\ning. We did this through adding an embedding corresponding to what expert each token was sent\\n(including an embedding if the token was dropped), but this did not improve performance.\\nAdding pre-training noise to ﬁx pre-training and ﬁne-tuning discrepancies. To help ﬁx the\\npre-training perplexity and ﬁne-tuning gap we tried pre-training the sparse models with a variety of\\ndifferent types of noise. The goal was to help pre-training match the ﬁne-tuning conditions where\\ndropout is used and more tokens can be dropped. Some of the noise types we tried adding during\\npre-training were dropout, dropping out full experts for a batch of tokens, and adding an entropy\\nmaximization auxiliary loss to the router. Unfortunately, all of the methods either hurt the pre-\\ntraining quality too much or didn’t end up helping the ﬁne-tuning.\\nLoad balancing in top-n routing over lower n-1 experts. In the standard top- nMoE formaliza-\\ntion there is only loading balancing over the top expert a token is sent to. We experimented with\\nadding an auxiliary load balancing term to the other n\\x001experts in top- nrouting, but found this to\\nprovide minimal beneﬁts.\\nMixing pre-training and ﬁne-tuning data to prevent overﬁtting. To help combat the overﬁtting\\nof sparse models during ﬁne-tuning, we tried mixing in pre-training span corruption data at varying\\namounts (e.g. 1%, 5%, 25%, ...) during ﬁne-tuning. This ended up not helping the ﬁne-tuning\\nperformance, but did increase the training loss.\\n38']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "def get_pdf(path):\n",
    "\n",
    "    text =\"\"\n",
    "    with open(temp_file_path, \"rb\") as f:\n",
    "        pdf = PdfReader(f)\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            text += page_text + \"\\n\"  # Append the text of each page to the list\n",
    "    return text\n",
    "\n",
    "temp_file_path = 'paper.pdf'\n",
    "text=get_pdf(temp_file_path)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "chunks = text_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ST-M OE: D ESIGNING STABLE AND TRANSFERABLE\\nSPARSE EXPERT MODELS\\nBarret Zoph\\x03\\nGoogle BrainIrwan Bello\\x03y\\nGoogle Brain\\nSameer Kumar\\nGoogleNan Du\\nGoogle BrainYanping Huang\\nGoogle Brain\\nJeff Dean\\nGoogle ResearchNoam Shazeery\\nGoogle BrainWilliam Fedus\\x03\\nGoogle Brain\\nABSTRACT\\nScale has opened new frontiers in natural language processing – but at a high cost.\\nIn response, Mixture-of-Experts (MoE) and Switch Transformers have been pro-\\nposed as an energy efﬁcient path to even larger and more capable language models.\\nBut advancing the state-of-the-art across a broad set of natural language tasks has\\nbeen hindered by training instabilities and uncertain quality during ﬁne-tuning.\\nOur work focuses on these issues and acts as a design guide. We conclude by\\nscaling a sparse model to 269B parameters, with a computational cost comparable\\nto a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-\\nof-Experts or ST-MoE-32B). For the ﬁrst time, a sparse model achieves state-\\nof-the-art performance in transfer learning, across a diverse set of tasks includ-\\ning reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum,\\nCNN-DM), closed book question answering (WebQA, Natural Questions), and\\nadversarially constructed tasks (Winogrande, ANLI R3).1\\n\\x03Equal contribution. Correspondence to fbarretzoph,liamfedus g@google.com .\\nyWork was done while at Google.\\n1Code for our models is available at https://github.com/tensorflow/mesh/blob/master/\\nmesh_tensorflow/transformer/moe.py\\n1arXiv:2202.08906v2  [cs.CL]  29 Apr 2022\\nCONTENTS\\n1 Introduction 3\\n2 Background 3\\n3 Stabilizing Training of Sparse Models 5\\n3.1 Stability and Quality Tradeoffs when Removing Multiplicative Interactions . . . . 6\\n3.2 Stability and Quality Tradeoffs when Adding Noise . . . . . . . . . . . . . . . . . 6\\n3.3 Stability and Quality Tradeoffs when Constraining Activations and Gradients . . . 7\\n3.4 Selecting a Precision Format: Trading Efﬁciency and Stability . . . . . . . . . . . 8\\n4 Fine-Tuning Performance of Sparse Models 9\\n4.1 Hypothesis: A Generalization Problem . . . . . . . . . . . . . . . . . . . . . . . . 9\\n4.2 Fine-Tuning a Subset of Model Parameters to Improve Generalization . . . . . . . 11\\n4.3 Sparse and Dense Models Require Different Fine-Tuning Protocols . . . . . . . . . 11\\n4.4 Sparse Models Are Robust to Dropped Tokens During Fine-Tuning . . . . . . . . . 12\\n4.5 Inserting Sentinels Tokens During Fine-Tuning . . . . . . . . . . . . . . . . . . . 13\\n5 Designing Sparse Models 13\\n5.1 Setting the Number of Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n5.2 Choosing the Capacity Factor and Routing Algorithm . . . . . . . . . . . . . . . . 14\\n6 Experimental Results 16\\n6.1 ST-MoE-L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n6.2 ST-MoE-32B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n7 Tracing Tokens Through the Model 19\\n7.1 Encoder Experts Exhibit Specialization . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.2 Decoder Experts Lack Specialization . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n7.3 Multilingual Experts Specialize, But Not by Language . . . . . . . . . . . . . . . 21\\n8 Related Work 21\\n9 Discussion 22\\n10 Conclusion 24\\nA Token Load Balance Description 31\\nB Router Z-Loss Training Dynamics 31\\nC Improved Architectural Modiﬁcations 32\\nD Batch Prioritized Routing for Lower Capacity Factors 33\\nE Pre-Training Dataset Details 34\\nF Full Fine-tuning Sensitivity Data 35\\nG Optimally Setting the Routing Threshold 36\\nH Mesh Layout for Data, Model and Expert Parallelism with Few Experts 36\\nI Note on Communication Costs for Distributed Models 37\\nJ Negative Results 38\\n2\\n1 I NTRODUCTION\\nSparse expert neural networks showcase the advantage of sheer scale and offer an efﬁcient alternative\\nto the static neural network architectures commonly used today (Raffel et al., 2019; Brown et al.,\\n2020; Rae et al., 2021). Rather than applying the same parameters to all inputs, sparse expert\\nnetworks dynamically select which parameters to use for each input (Shazeer et al., 2017). This\\nallows for networks to vastly expand their number of parameters, while keeping the FLOPs per\\ntoken roughly constant. These approaches have yielded state-of-the-art translation models (Lepikhin\\net al., 2020), 4-7x pre-training speed-ups (Fedus et al., 2021; Artetxe et al., 2021), and GPT-3 level\\none-shot performance using 1/3 the energy training cost (Du et al., 2021). And despite a shocking\\nnumber of parameters, sparse models reduce the carbon footprint for training large neural networks\\nby an order of magnitude (Patterson et al., 2021). However, difﬁculties remain.\\nFedus et al. (2021) observed that a sparse 1.6T parameter model achieved a 4x pre-training speed-up\\nover the prior state-of-the-art (Raffel et al., 2019), but lagged smaller models when ﬁne-tuned on\\ncommon benchmarks like SuperGLUE. Similar gaps were observed in Artetxe et al. (2021) when\\nMoE language models were ﬁne-tuned on out-of-domain data. In response, Switch-XXL, a model\\nwith fewer parameters, but a 8x-larger computational footprint (FLOPs approximately equal to the\\nlargest T5 model), was proposed and improved quality on natural language understanding tasks.\\nHowever, necessary pre-training was hampered by training instabilities previously undetected during\\nsmaller scale studies. These instabilities were later identiﬁed in other sparse models (Du et al., 2021).\\nThese results revealed a necessary balance of parameters andcomputation, but left an open question\\non how to reliably train these types of models.\\nOur aim in this paper is to increase the practicality andreliability of sparse models. We study these\\ntwo issues and pre-train a 269B sparse model that achieves state-of-the-art results when ﬁne-tuned\\nacross many competitive NLP benchmarks, including SuperGLUE. We also put forth additional\\nanalysis and a design guide (or at least, our heuristics) for sparse expert models. Furthermore, this\\nwork emphasizes jointly optimizing both the upstream pre-training and the downstream ﬁne-tuning\\nmetrics to avoid discrepancies (Tay et al., 2021).\\nContributions\\n1.A large-scale study of the quality-stability trade-offs of stability techniques.\\n2.An introduction of the router z-loss that resolves instability issues, while slightly improv-\\ning model quality.\\n3.A ﬁne-tuning analysis of sparse and dense models highlighting different hyperparameter\\nsensitivity to the batch size and learning rate. We show bad hyperparameters result in\\nvirtually no ﬁne-tuning gain over dense models, despite large pre-training speed-ups.\\n4.Architectural, routing and model design principles for designing Pareto efﬁcient sparse\\nmodels in a distributed setting.\\n5.A qualitative analysis tracing token routing decisions across expert layers.\\n6.A 269B sparse model (the Stable Transferable Mixture-of-Experts or ST-MoE-32B)\\nwhich achieves state-of-the-art performance across a diverse set of natural language\\nbenchmarks.\\n2 B ACKGROUND\\nSparse expert models typically substitute a neural network layer with a set of experts , each having\\nunique weights (Jacobs et al., 1991; Jordan and Jacobs, 1994). Typically all the experts within a layer\\nare of the same type and shape (homogeneous), however, varied (heterogeneous) expert-types are\\npossible. Inputs are only processed by a subset of the experts to save computation, so a mechanism\\nmust be added to determine where to send each input. Usually a router or gating network determines\\nwhere to send inputs (i.e. words, sentences, image patches, etc.), but alternative schemes have been\\nproposed (Lewis et al., 2021; Roller et al., 2021; Zuo et al., 2021; Clark et al., 2022).\\n3\\nSpeciﬁcally, in natural language processing, Shazeer et al. (2017) proposed a Mixture-of-Experts\\n(MoE) layer which takes a token representation xas input and routes it to the best matched top-\\nkexperts selected out of a set fEi(x)gN\\ni=1ofNexperts. The router variable Wrproduces logits\\nh(x) =Wr\\x01xwhich are normalized via a softmax distribution over the available Nexperts at that\\nlayer. The gate-value for expert iis given by\\npi(x) =eh(x)i\\nPN\\njeh(x)j(1)\\nand the token xis routed to the experts with the highest top- kgate values (set of indices T). The\\noutput of the layer is the weighted sum of each expert’s computation by the gate value\\ny=X\\ni2Tpi(x)Ei(x) (2)\\nOriginally proposed in LSTMs (Hochreiter and Schmidhuber, 1997), expert layers were later used in\\nthe Transformer (Vaswani et al., 2017) by Shazeer et al. (2018) and Lepikhin et al. (2020). Follow-\\non work by Fedus et al. (2021) simpliﬁed the MoE further to route tokens to a single expert (top-1)\\nand reduced other costs to improve training efﬁciency.\\nTo improve hardware utilization, most implementations of sparse models have static batch sizes\\nfor each expert (Shazeer et al., 2017; 2018; Lepikhin et al., 2020; Fedus et al., 2021). The expert\\ncapacity refers to the number of tokens that can be routed to each expert. If this capacity is exceeded\\n(the router sends too many inputs to that expert) then the overﬂowed tokens have no computation\\napplied to them and are passed to the next layer through a residual connection.\\nTerminology Deﬁnition\\nExpert An independently-learned neural network with unique weights.\\nRouter A network that computes the probability of each token getting sent to each\\nexpert.\\nTop-nRouting Routing algorithm where each token is routed to nexperts.\\nLoad Balancing Loss An auxiliary (aux) loss to encourage each group of tokens to evenly distribute\\nacross experts.\\nGroup Size The global batch size is split into smaller groups, each of size Group Size. Each\\ngroup is considered separately for load balancing across experts. Increasing it\\nincreases memory, computation, and communication.\\nCapacity Factor (CF) Each expert can only process up to a ﬁxed number of tokens, which is often\\nset by evenly dividing across experts,tokens\\nexperts. The capacity factor can expand or\\ncontract this amount to CF \\x01tokens\\nexperts.\\nFFN Acronym of Feed Forward Network (FFN) layer of Transformer consisting of\\nlinear, activation, linear.\\nEncoder-Decoder A Transformer architectural variant that all of our models are based on. Consists\\nof an encoder that does all-to-all attention on the inputs and a decoder that\\nattends to the encoder and to its own inputs in an autoregressive manner.\\nallreduce Communication primitive which sums a subset of ntensors on ndifferent de-\\nvices, then broadcasts the summed value to all ndevices. This is used in dis-\\ntributed training for gradient accumulation and model parallelism.\\nall2all Communication primitive where each device sends to every other device a part\\nof its tensor. Used in sparse Transformer models for token routing.\\n(\"/#) Indicates whether higher/lower values are better (e.g. accuracy/train loss).\\nTable 1: Terminology used throughout the paper.\\nThe batchBof input tokens is broken into Gunique groups across the data-parallelism dimension2,\\neach with sizeB=G. The expert capacity is equal to CF \\x01tokens=experts where CF represents the\\n2Our implementation relies on einsums with one-hot tensors for dispatching and combining tensors to/from\\nexperts. The size of this one-hot tensor grows quadratically with the number of tokens being routed as a group\\nwhich motivates breaking the batch into smaller groups. This may be avoided with sparse lookup operations.\\n4\\ncapacity factor hyperparameter, experts is the number of experts and tokens is the group size. If the\\ncapacity factor is increased, it creates extra buffer so that fewer tokens will be dropped in case of load\\nimbalance. However, increasing the capacity factor also increases the memory and computational\\ncosts, so there exists a trade off3.\\nFinally, an auxiliary load balancing loss encourages tokens to be roughly evenly distributed across\\nthe experts (Shazeer et al., 2017). This improves the hardware efﬁciency by ensuring that all accel-\\nerators are processing signiﬁcant chunks of data in parallel as mentioned above. The details of the\\nloss are presented in Appendix A. However, alternatives exist: Lewis et al. (2021) and Clark et al.\\n(2022) treats balanced token allocation as an assignment problem and removes the auxiliary loss\\nentirely.\\n3 S TABILIZING TRAINING OF SPARSE MODELS\\nSparse models often suffer from training instabilities (Figure 1) worse than those observed in stan-\\ndard densely-activated Transformers.\\n0 2500 5000 7500 10000 12500 15000\\nStep050100150200250300350Training Loss\\n0 2500 5000 7500 10000 12500 15000\\nStep2345678Training Loss\\nFigure 1: Training instabilities for sparse models. We refer to training instabilities as divergences\\nin the training loss. Above are two runs from sparse models FLOP-matched to the T5-XL version\\n(Raffel et al., 2019) each trained with a batch size of 1M tokens using the Adafactor optimizer\\n(Shazeer and Stern, 2018). (Left) An unstable training run. (Right) A stable training run.\\nIt’s straightforward to ﬁnd changes that improve the stability , however, these often come at an un-\\ntenable expense to model quality (for instance, using an arbitrarily small learning rate or using tight\\ngradient clipping). We categorize and examine several approaches to improve stability. The sta-\\nbility techniques span generic ﬁxes to Transformers as well as those speciﬁc to sparse models: (1)\\nRemove multiplicative interactions (2)Inject model noise (3)Constrain activations, and gradients.\\nWe conclude with our recommendation: a new auxiliary loss, the router z-loss , which signiﬁcantly\\nimproves training stability with no quality degradation. This is an adaptation of the z-loss used for\\nﬁnal softmax logits in the Mesh Tensorﬂow codebase (Shazeer et al., 2018).\\nStabilizing Sparse Models\\n1.Many methods stabilize sparse models, but at the expense of worse quality.\\n2.The router z-loss stabilizes models without quality degradation.\\n3.Transformer modiﬁcations with more multiplicative components (GEGLU, RMS nor-\\nmalization) worsen stability, but boost quality.\\nDesigning a large-scale stability study. We design a large-scale stability study of sparse models\\nFLOP-matched to the T5-XL version (Raffel et al., 2019) pre-trained on the multilingual corpus\\nmC4 (Xue et al., 2020). Each sparse model has 32 experts and we introduce a sparse MoE layer for\\n3See Fedus et al. (2021) for a graphical illustration of how the capacity factor works.\\n5\\nevery fourth FFN. The train capacity factor is 1.25 and the eval capacity factor is 2.0. See Table 11\\nfor a more detailed description of models used throughout this paper. For each stability technique,\\nwe record the fraction that are stable, the mean quality (negative log perplexity on English), and the\\nstandard deviation over seeds.\\nThe primary issue in constructing this study is that small models are rarely unstable but large un-\\nstable models are too costly to run for sufﬁcient steps and seeds. We found a sparse model FLOP-\\nmatched to T5-XL to be good object of study because it was unstable roughly 1=3of the runs, but\\nwas still relatively cheap to train. Furthermore, we run our instability experiments on multilingual\\ndata since we ﬁnd this exacerbates model instabilities, allowing us to experiment on slightly smaller\\nmodels. See Section 9 for more details. Our baseline conﬁguration is trained using six random seeds\\nand each conﬁguration with a stability technique uses three random seeds. We use six seeds for the\\nbaseline to better characterize the instability rate and three seeds for the variants to save compute.\\nEach model is pre-trained for 20k steps on mC4 using a masked language modeling objective (Fedus\\net al., 2018; Devlin et al., 2018).\\n3.1 S TABILITY AND QUALITY TRADEOFFS WHEN REMOVING MULTIPLICATIVE\\nINTERACTIONS\\nSome architectural improvements involve more multiplications than additions or do not sum many\\nitems at once. For example, a matrix multiplication has one multiplication for each addition and\\nhence we do not refer to it as a “multiplicative” operation. We present and analyze the impact of\\ntwo instances of multiplicative interactions in Transformers here.\\nGELU Gated Linear Units (GEGLU). Our ﬁrst example is the Gated Linear Unit (Dauphin\\net al., 2017) which is a component-wise product of two linear projections, one of which is ﬁrst\\npassed through a sigmoid function. Shazeer (2020) extends this to other variants and presents a\\nGELU-Linear (Hendrycks and Gimpel, 2016) FFN layer as a replacement the usual ReLU (Nair and\\nHinton, 2010) FFN in Transformer.\\nFFN GEGLU (x;W;V;b;c ) =GELU (xW+b)\\x0c(xV+c) (3)\\nThis quality gain was corroborated in later work (Narang et al., 2021).\\nRoot Mean Square Scale Parameters. Our second example is the scale parameter in root mean\\nsquare (RMS) normalization (Zhang and Sennrich, 2019). Within the Transformer, rather than call-\\ning layers back-to-back, there is an internal structure (referred to as sublayer calls) which improve\\ngradient propagation and training dynamics. Our sublayer calls match that of Raffel et al. (2019)\\nand consist of: (1)RMS normalization, (2)layer call (e.g. Self Attention), (3)dropout (Srivastava\\net al., 2014), (4)add residual (He et al., 2015). RMS normalization scales the input vector x2Rd\\nelement-wise per the root-mean-square. It then rescales the output element-wise by multiplying with\\na learned scale parameter g.\\nyi=xiq\\n1\\ndPd\\ni=1x2\\ni\\x01gi (4)\\nTable 2 shows that both removing GEGLU layers or the RMS scale parameter improves stability, but\\nat a signiﬁcant loss to model quality. We note that these scale parameters ( g) have a disproportionate\\ngain to model quality versus parameters elsewhere (e.g. FFN). In line with our ﬁndings, Shleifer\\net al. (2021) found adding a learned multiplicative scalar to the residual connection in Transformers\\nmade them much more unstable.\\nIn Appendix C, we further study the quality impact of adding new multiplicative interactions in\\nexpert layers. We ﬁnd that this operation yields quality improvements with virtually no slow-down\\nin model step time.\\n3.2 S TABILITY AND QUALITY TRADEOFFS WHEN ADDING NOISE\\nWe next explore a hypothesis that adding noise into the model can improve training stability (Nee-\\nlakantan et al., 2015). Taleb (2012) argues that certain systems exhibit the property of anti-fragility,\\nwhere they improve through noise. Inspired by the concept and by our observation that ﬁne-tuning\\n6\\nMethod Fraction Stable Quality ( \")\\nBaseline 4=6 -1.755\\x060:02\\nRemove GEGLU 3=3 -1.849\\x060:02\\nRemove RMS Norm. Scale Param 3=3 -2.020\\x060:06\\nTable 2: Removing operations with more multiplicative interactions . Multiplicative interactions\\nimprove quality, but can destabilize training. Individually removing two sources of multiplicative\\ncomponents improves the stability, but worsens quality signiﬁcantly. When we remove the GEGLU\\nlayer, we replace it with with an equivalent Dense-ReLU-Dense layer to match the FLOPs and\\nparameters.\\n(which injects noise via dropout) was rarely unstable, we examined whether training noise might\\nimprove the stability of sparse models. Table 3 shows a stability improvement versus the baseline,\\nbut at the expense of lower quality. We also ﬁnd that input-jitter, introduced by Fedus et al. (2021),\\ndiminishes quality at XL-scale, hence we ablate it in our models. Input-jitter multiplies the input\\nlogits to the router by a uniform random variable between [1\\x0010\\x002;1 + 10\\x002]. Dropout in our\\nablation is applied throughout the Transformer. As seen previously, improvements in small-scale\\nsettings may fail to generalize when scaled up and therefore trends should always be monitored and\\nre-assessed at increasing scale (Kaplan et al., 2020).\\nMethod Fraction Stable Quality ( \")\\nBaseline 4=6 -1.755\\x060:02\\nInput jitter ( 10\\x002) 3=3 -1.777\\x060:03\\nDropout (0.1) 3=3 -1.822\\x060:11\\nTable 3: Injecting noise during training . Both input-jitter and dropout improve stability, but lead to\\na signiﬁcant loss of model quality. There is a clear tradeoff with most methods: when one improves\\nstability, it then typically decreases model quality. Our work aims to ﬁnd methods that ﬁx stability\\nwithout hurting quality.\\n3.3 S TABILITY AND QUALITY TRADEOFFS WHEN CONSTRAINING ACTIVATIONS AND\\nGRADIENTS\\nOne of the most successful approaches to stabilizing neural networks are constraints on activations,\\nand gradients (Pascanu et al., 2013; Ioffe and Szegedy, 2015; Salimans and Kingma, 2016; Ba et al.,\\n2016). A popular approach consists in the clipping of gradient norms to remedy exploding gradients\\nwhile backpropagating through deep networks (Pascanu et al., 2013).\\nIn this work, we use the Adafactor optimizer due to its memory efﬁciency (though recently in-\\ntroduced 8-bit optimizers (Dettmers et al., 2021) may offer better trade-offs). Instead of gradient\\nclipping, Adafactor uses update clipping , where the changes to the weights are constrained to be\\nbelow a certain norm. We experiment with tightening the update clipping to a smaller value.\\nNext, we study constraints on the logits going into the router. The router computes the probability\\ndistribution over the experts in float32 precision (i.e. selective precision) (Fedus et al., 2021).\\nHowever, at the largest scales, we ﬁnd this is insufﬁcient to yield reliable training. To ﬁx this, we\\nintroduce the router z-loss ,\\nLz(x) =1\\nBBX\\ni=10\\n@logNX\\nj=1ex(i)\\nj1\\nA2\\n(5)\\nwhereBis the number of tokens, Nis the number of experts, and x2RB\\x02Nare the logits going\\ninto the router. This penalizes large logits into the gating network and Section 3.4 contains a more\\ndetailed explanation of why the z-loss before the router is useful.\\n7\\nTable 4 shows that both update clipping and the router z-loss stabilize the model in all 3 runs, but the\\nupdate clipping signiﬁcantly hurts the model quality. Therefore we use the z-loss method for ﬁxing\\nour model stability due to improved quality and stability4.\\nMethod Fraction Stable Quality ( \")\\nBaseline 4=6 -1.755\\x060:02\\nUpdate clipping (clip = 0:1) 3=3 -4.206\\x060:17\\nRouter Z-Loss 3=3 -1.741\\x060:02\\nTable 4: Constraining weight updates and router logits. Constraining the update clipping in\\nAdafactor improves stability, but at a catastrophic loss of quality. Looser clipping values did not\\nreliably stabilize training so we exclude them here. The router z-loss stabilizes the model without\\nany quality degradation (in this case, we observe a slight quality boost).\\nThe router z-loss introduces another hyperparameter (cz), which is the coefﬁcient to weight this as\\npart of the total loss optimized. The total loss is a linearly weighted combination of the cross entropy\\nloss (LCE), the auxiliary load balance loss ( LB), and the router z-loss ( LZ), yielding a total loss\\nLtot=LCE+cBLB+czLZ (6)\\nWe choose a value of cz= 0:001based on the best model quality after pre-training with a hyperpa-\\nrameter sweep. Appendix B logs the resulting losses over the course of pre-training.\\n3.4 S ELECTING A PRECISION FORMAT : TRADING EFFICIENCY AND STABILITY\\nAs in most modern distributed Transformers we train with mixed precision (Micikevicius et al.,\\n2017)5. Weights are stored in float32 for gradient updates and then converted to bfloat16\\nwhen doing matrix multiplications in the forward and backward pass6. Furthermore, all activations\\nare stored and operated on in bfloat16 andallreduce communications can be done in either\\nbfloat16 orfloat32 numerical precision. For the largest model explored in this work (ST-\\nMoE-32B presented later) we ﬁnd speed-ups halving the numerical precision of the allreduce ,\\nhowever this also can destabilize the training so we keep this as float32 throughout this work.\\nA lower precision format enables more efﬁcient models by reducing (a)communication costs be-\\ntween processors and memory, (b)computation costs, (c)memory for storing tensors (e.g. activa-\\ntions). However, lower precision formats come at the expense of larger roundoff errors which can\\nlead to irrecoverable training instabilities.\\nMantissa (7 bits) Exponent (8 bits)Exponent (8 bits) Mantissa (23 bits)Precision Format: Float32\\nPrecision Format: BFloat16Number RangeMax BFloat16\\nRoundoff ErrorMax Float32\\nRoundoff Error\\n[2, 4) 0.01563 2.34x10^(-7)\\n[32, 64) 0.25 3.81x10^(-6)\\n[1024, 2048) 8.0 0.00012\\n[2^20, 2^21) 8192.0 0.125\\n[2^30, 2^31) 8288608.0 128.0\\nFigure 2: Numerical precision formats and roundoff errors. Larger numbers have larger roundoff\\nerrors. bfloat16 has up to 65,536x worse roundoff errors than float32 . The router z-loss\\nencourages the absolute magnitude of numbers to be small, which doesn’t hinder model performance\\nand reduces roundoff errors. The router z-loss is most effective into functions where larger errors\\ncan drastically change the relative output (e.g. exponential and sinusoidal functions).\\n4We also experimented with adding z-losses onto the attention logits which also improves model instability\\nwithout hurting model quality.\\n5See Mesh Tensorﬂow for implementation details: https://github.com/tensorflow/mesh/\\nblob/master/mesh_tensorflow/\\n6Matrix multiplications on TPUs perform multiplications in bfloat16 and accumulations in float32 .\\n8\\nUnderstanding precision format and roundoff errors. Figure 2 reviews the properties of differ-\\nent precision formats and their corresponding roundoff errors for different number ranges. Numbers\\nin any range of two consecutive powers of 2 (e.g. [2,4) and [1024, 2048)) are represented by a ﬁxed\\nnumber of mantissa bits (7 for bfloat16 , 23 for float32 ). As a result, (1)bfloat16 will\\nhave about 65,536x (i.e. 23\\x007 = 16 additional bits and 216= 65536 ) as large roundoff errors as\\nfloat32 and(2)larger numbers have larger roundoff errors. Due to the 8 exponent bits, number\\ncan get as large as \\x193e38, which leads to even float32 having some issues with roundoff errors.\\nSparse expert models are sensitive to roundoff errors because they have more exponential\\nfunctions due to the routers. Sparse expert models introduce additional exponential functions –\\nthrough the router – which can exacerbate roundoff errors7and lead to training instabilities. While\\na roundoff error does not change the ordering of probabilities within a softmax operation, it does\\nimpact the routing of the second token in MoE due to relative thresholding (e.g. a token is only\\nrouted to its second place expert if the gating probability for the second expert is 1=5as large as\\nthat of the ﬁrst expert). Additionally, roundoff errors can drastically change the probability that\\nscales the expert output – which we have found to be important. Finally, we conjecture that the\\nhigher stability we observed for decoder-only models (not shown here) was because they had fewer\\nexponential functions. Section 9 contains a more detailed discussion.\\nAn aside on the router z-loss. One might think that the router z-loss is a convoluted method\\nreplaceable by clipping logits (Wu et al., 2016). We explain why this is not the case. The goal is to\\nminimize large roundoff errors going into exponential functions. Clipping the logits occurs after any\\nroundoff errors – resulting in even larger discontinuities. In one view, clipping in itself is a roundoff\\nerror; conversely, the z-loss naturally encourages the model to produce logits that are small in value\\nand thus more accurately modeled. Due to these dynamics, we ensure all exponentiated tensors are\\ncast to float32 . This hints at the possibility of better number formats for neural networks because\\nof the unused exponent bits when z-losses are added throughout the network (see Section 9).\\n4 F INE-TUNING PERFORMANCE OF SPARSE MODELS\\nThe best performing language models are usually obtained by (1) pre-training on large amounts of\\ndata (e.g. the internet) followed by (2) ﬁne-tuning on a task of interest (e.g. SuperGLUE). Promis-\\ning new techniques have emerged as an alternative, including few-shot inference (Brown et al.,\\n2020), preﬁx tuning (Li and Liang, 2021), prompt tuning (Lester et al., 2021), and adapter modules\\n(Houlsby et al., 2019) – however, a quality gap still persists compared to ﬁne-tuning. Because of\\nthis, we focus on ﬁne-tuning in this work, but highlight recent successes of sparse models in few-\\nshot settings from Du et al. (2021); Artetxe et al. (2021). Further, we leave as future work techniques\\nthat adapt large language models through reinforcement learning (Ouyang et al., 2022)\\n4.1 H YPOTHESIS : A G ENERALIZATION PROBLEM\\nSparse models have performed remarkably well in the regime of large datasets, but have sometimes\\nperformed poorly when ﬁne-tuning (Fedus et al., 2021; Artetxe et al., 2021). We present evidence\\nfor a (not so surprising) hypothesis that sparse models are prone to overﬁtting. We illustrate this\\nproblem through two tasks in SuperGLUE (Wang et al., 2019) – Commitment Bank (De Marneffe\\net al., 2019) and ReCORD (Zhang et al., 2018). Commitment Bank (CB) has 250 training examples\\nwhile ReCORD has over 100,000. This signiﬁcant size discrepancy facilitates a natural study for\\noverﬁtting on two tasks selected as part of the same benchmark.\\nIn Figure 3, we compare the ﬁne-tuning characteristics of the Dense L and the ST-MoE-L model.\\nEach model was pre-trained on 500B tokens from the C4 corpus (Raffel et al., 2019). The models\\n7Exponential functions have the property that a small input perturbation can lead to a large difference in\\nthe output. As an example, consider inputting 10 logits to a softmax function with values of 128 and one logit\\nwith a value 128.5. A roundoff error of 0.5 in bfloat16 will alter the softmax output by 36% and incorrectly\\nmake all logits equal. The calculation goes fromexp(0)\\nexp(0)+10\\x01exp(\\x000:5)\\x190:142toexp(0)\\nexp(0)+10\\x01exp(0)\\x190:091.\\nThis occurs because the max is subtracted from all logits (for numerical stability) in softmax operations and\\nthe roundoff error changes the number from 128.5 to 128. This example was in bfloat16 , but analogous\\nsituations occur in float32 with larger logit values.\\n9\\nStep80.082.585.087.590.092.595.097.5100.0MetricSuperGLUE CB Task\\nSparse train_eval\\nSparse validation_eval\\nDense train_eval\\nDense validation_eval\\nStep8486889092949698100102MetricSuperGLUE ReCoRD Task\\nSparse train_eval\\nSparse validation_eval\\nDense train_eval\\nDense validation_evalFigure 3: Sparse models are prone to overﬁt. We plot train and validation curves for our ST-MoE-\\nL and a dense-L models ﬁne-tuned on the CB task (250 train sequences) and ReCoRD (138k train\\nsequences). In both cases, the sparse model learns more quickly on the train partition (blue exceeds\\ngreen line). However, for the smaller CB task, the dense model outperforms the sparse model on the\\nheld-out validation set (red vs. orange). In contrast, on the larger ReCoRD task, the sparse model\\noutperforms the dense model by several percentage points.\\nare designed to be roughly FLOP matched variants of the T5-Large encoder-decoder models from\\nRaffel et al. (2019) with 770M parameters. The ST-MoE models have 32 experts with an expert\\nlayer frequency of 1=4(every fourth FFN layer is replaced by an MoE layer). The pre-training and\\nﬁne-tuning train capacity factor is 1.25 and the eval is 2.0. We evaluate performance on the held-out\\nvalidation and train dataset partitions.\\nAcross both tasks, the sparse model converges faster to 100% train set accuracy supporting that\\nsparse models optimize effectively under a data distribution shift. On the larger task, ReCORD, the\\nvalidation quality of the sparse model follows the boost in training and signiﬁcantly exceeds the\\ndense model. However, on the smaller task, CB, the sparse model lags its dense counterpart on held-\\nout data. As per the recommendation of Fedus et al. (2021), we consider increasing the dropout\\nwithin the expert hidden state (i.e. expert dropout), but ﬁnd that at this scale, higher values only\\nmoderately improve quality (Figure 4). We study further improvements to ﬁne-tuning in Section 4.2\\nand hyperparameter sensitivity in Section 4.3.\\n0.0 0.1 0.2 0.3\\nDropout Probability80828486SuperGLUE Score\\n0.0 0.1 0.2 0.3 0.4 0.5\\nExpert Dropout Probability80828486SuperGLUE Score\\nFigure 4: Regularization studies of sparse models for ﬁne-tuning . For each setting, we train\\nthree random seeds till convergence on SuperGLUE. We ﬁnd that increased regularization through\\ndropout provides modest boosts. (Left) demonstrates peak SuperGLUE ﬁne-tuning quality at a\\nglobal dropout rate of 0.1. Higher values over-regularize and severely hurt quality. (Right) Starting\\nwith the best known global dropout rate of 0.1, we selectively increase the expert dropout (an in-\\ndependent dropout rate on the expert hidden activation). This yields further generalization beneﬁts\\nand is in line with the ﬁndings of Fedus et al. (2021).\\n10\\n4.2 F INE-TUNING A SUBSET OF MODEL PARAMETERS TO IMPROVE GENERALIZATION\\nTo combat overﬁtting we experiment updating only a subset of models parameters during ﬁne-\\ntuning. Figure 5 measures quality for updating 5 different subsets of parameters: all parameters\\n(All), only non MoE parameters (Non MoE), only MoE parameters (MoE), only the self-attention\\nand enc-dec attention parameters (Attention) and only the non MoE FFN parameters (FFN).\\nAll Non MoE MoE Attention FFN\\nParameters Being Updated81828384858687SuperGLUE Score\\nFigure 5: Updating only a subset of model parameters during ﬁne-tuning . To improve the gener-\\nalization of sparse models and combat overﬁtting, we ﬁne-tune a subset of the model parameters. All\\nresults are with the ST-MoE-L model and are an average of 5 different random seeds. We observe\\nthat updating 3=5of the subsets of parameters appear to work about the same, while ﬁne-tuning only\\nthe MoE parameters results in a drastic quality reduction.\\nWe observe that updating the non MoE parameters works about as well as updating all the param-\\neters and updating only the FFN parameters works a bit better. Updating only the MoE parameters\\nsigniﬁcantly degrades ﬁne-tuning performance, which is where \\x1980% of model parameters are.\\nOnly updating the non MoE parameters can be an effective way to speedup and reduce memory for\\nﬁne-tuning.\\nWe hypothesize that ﬁne-tuning only the MoE parameters leads to bad performance since expert\\nlayers only occur every 1=4layers and a token will see at most two experts per layer. Therefore,\\nupdating the MoE parameters will affect much fewer layers and FLOPs than updating any other\\nsubset of the parameters we tried. Updating only the MoE parameters resulted in a much larger\\ntraining loss than updating the non MoE parameters, even though there are signiﬁcantly more pa-\\nrameters. We further observe that updating all the non-MoE parameters results in a higher training\\nloss than updating all the parameters, but unfortunately this regularization effect didn’t translate to\\nbetter validation performance.\\nFurther, one regularizer we tried was a dropout variant where entire experts were masked out\\nstochastically during training. However, this failed to improve generalization in our preliminary\\nstudies. Appendix J expands on this experiment and contains other negative results.\\n4.3 S PARSE AND DENSE MODELS REQUIRE DIFFERENT FINE-TUNING PROTOCOLS\\nHow sensitive are sparse and dense models to the ﬁne-tuning protocol? We study two hyperparam-\\neters: the batch size and the learning rate. We pretrain a Dense-L and ST-MoE-L on 500B tokens\\nof C4 and then ﬁne-tune on SuperGLUE. Figure 6 summarizes our experiments with the full data\\npresented in Table 20 (Appendix F). Across all hyperparameter settings, the sparse models (orange)\\noutperform the dense (blue) counterparts – however, the best setting for each can materially change\\nresults. Sparse and dense models have vastly different performance across different batch sizes and\\nlearning rates. Sparse models beneﬁt from smaller batch sizes and a higher learning rate. Consis-\\ntent with the overﬁtting hypothesis (Section 4.1), both these changes might improve generalization\\nthrough higher noise in the ﬁne-tuning process. Finally, we point out the importance of correctly\\ntuning the batch size and learning rate during ﬁne-tuning. Simply using the same ﬁne-tuning hyper-\\n11\\nparameters that worked well for the dense model can mask any pre-training improvements obtained\\nby the sparse model.\\n65k 262k 1M\\nBatch Size81828384858687SuperGLUE ScoreDense\\nSparse\\n1e-4 5e-4 1e-3\\nLearning Rate81828384858687SuperGLUE ScoreDense\\nSparse\\nFigure 6: Batch size and learning rate sensitivity. We measure differences and sensitivity to ﬁne-\\ntuning protocols between dense (blue) and sparse (orange) models. Each bar is an average across 6\\ndifferent runs with different hyperparameters. On SuperGLUE, sparse models beneﬁt from noisier\\nhyperparameters including small batch sizes and high learning rates. Dense models behave nearly\\noppositely. See Appendix F for all data.\\n4.4 S PARSE MODELS AREROBUST TO DROPPED TOKENS DURING FINE-TUNING\\nSparse models route tokens to one or more experts at each layer. To make these models efﬁcient in\\nthe SPMD paradigm with modern hardware, the expert capacity (the number of tokens each expert\\nprocesses) needs to be ﬁxed ahead of time (see Section 2 for more details). When an expert receives\\nmore tokens than its capacity, the extra tokens are dropped — no computation is applied to those\\ntokens. We again try to prevent this by (1)pre-training with an auxiliary loss that promotes equal\\namounts of tokens getting sent to each expert and (2)a capacity factor (a hyperparameter) that adds\\nroom for extra tokens at each expert. We experiment with turning off the auxiliary loss during\\nﬁne-tuning and using different capacity factors. Tables 5 reveals a surprising result that ﬁne-tuning\\nquality is not materially impacted by dropping up to 10-15% of tokens8. Studies on ST-MoE-32B\\ncorroborate that high capacity factors do not improve ﬁne-tuning quality. This is in-line with ﬁndings\\nof Yang et al. (2021) that unequal load balance may not signiﬁcantly impact model quality.\\nModel Train CF Eval CF Aux Loss Percent Tokens Dropped SuperGLUE (\")\\nSparse 0.75 2.0 Yes 10.6% 86.5\\x060.21\\nSparse 1.25 2.0 Yes 0.3% 86.7\\nSparse 2.0 3.0 Yes 0.0% 85.8\\nSparse 4.0 5.0 Yes 0.0% 86.4\\nSparse 0.75 2.0 No 15.6% 85.7\\nSparse 1.25 2.0 No 2.9% 85.8\\nSparse 2.0 3.0 No 0.4% 85.9\\nSparse 4.0 5.0 No 0.0% 86.4\\nTable 5: Sparse models are robust to dropped tokens when ﬁne-tuning. We ﬁnd the ﬁne-tuning\\nquality on SuperGLUE is not impacted signiﬁcantly across the values explored. Interestingly, drop-\\nping 10-15% of tokens can perform approximately as well as models that drop <1%. We also\\nobserve that load balance losses (Aux Loss) improve ﬁne-tuning. The dropped token percentage\\ncorresponds to the fraction of dropped tokens across all expert layers at peak validation accuracy.\\n8Token dropping may be a form of regularization and a more extensive study may be an interesting direction\\nfor future work.\\n12\\n4.5 I NSERTING SENTINELS TOKENS DURING FINE-TUNING\\nSentinel tokens denote masked sequences in the span-corruption objective (Fedus et al., 2018; Devlin\\net al., 2018). This differs from any ﬁne-tuning task we would likely encounter, leading to a domain\\nmismatch between pre-training and ﬁne-tuning. Table 6 illustrates the difference. We examine\\nwhether modifying the ﬁne-tuning task to look more like the pre-training task effects results.\\nObjective Inputs Targets\\nSpan Corruption I like <X> the pool <Y> day . <X> going to <Y> on a sunny\\nFine-Tuning What is the capital of Illinois ? Springﬁeld\\nFine-Tuning + Sentinels What is the capital of Illinois ? <X> <X> Springﬁeld\\nTable 6: Inserting sentinels during ﬁne-tuning mimics the pre-training span objective . We\\nhighlight the typical difference between span corruption and ﬁne-tuning. We propose modifying the\\nﬁne-tuning task to resemble pre-training by inserting sentinel tokens.\\nIn Table 7 we ﬁnd that adding sentinel tokens while ﬁne-tuning only improves Grammar Error\\nCorrection (GEC) (Rothe et al., 2021), but not SuperGLUE. We tried to further reduce the data\\ndistribution shift by inserting multiple sentinel tokens (as would be encountered by the model while\\npre-training), but again found no universal beneﬁt. However, despite no consistent beneﬁt on held-\\nout data, we ﬁnd that training convergence is accelerated for both dense and sparse models.\\nModel Insert Sentinel Tokens SuperGLUE (\") GEC (\")\\nDense 84.9\\x060.33 22.3\\x060.25\\nDense X 85.1\\x060.25 22.1\\x060.42\\nSparse 86.6\\x060.18 22.2\\x060.04\\nSparse X 86.6\\x060.24 22.9\\x060.09\\nTable 7: Impact of sentinel tokens for ﬁne-tuning . The addition of sentinel tokens (a similar\\nconcept used in Lester et al. (2021)) during ﬁne-tuning has mixed performance on the two tasks we\\nconsider. SuperGLUE records the average score and GEC records the exact match. While we ﬁnd it\\ndoesn’t improve generalization, sentinel tokens can accelerate training convergence.\\n5 D ESIGNING SPARSE MODELS\\nThe design of dense models has been guided by the foundational work of Kaplan et al. (2020). But\\nsparse models pose a myriad of additional questions: (1) How many experts to use? (2) Which\\nrouting algorithm? (3) What value for the capacity factor? (4) How does hardware change these\\ndecisions? In this section, we comment on these and offer recommendations for building Pareto ef-\\nﬁcient sparse models. Concurrently, Clark et al. (2022) provides additional design recommendations\\nincluding higher layer frequency and top-1 routing as per Fedus et al. (2021).\\nDesigning Sparse Models\\n1.In our setup, we recommend top-2 routing with 1.25 capacity factor and at most one\\nexpert per core.\\n2.The capacity factor can be changed during evaluation to adjust to new memory/compute\\nrequirements.\\n3.Dense layer stacking and a multiplicative bias can boost quality (Appendix C).\\n5.1 S ETTING THE NUMBER OF EXPERTS\\nOne of the ﬁrst questions is the number of experts to use. Fedus et al. (2021) presented the scaling-\\nproperties of Switch Transformer which yielded monotonic pre-training beneﬁts (on a step basis) on\\n13\\nC4 up to 512-experts, Kim et al. (2021) up to 64-experts and Clark et al. (2022) up to 512-experts.\\nBut the incremental beneﬁt quickly diminishes with many experts ( >256) or equivalently, with very\\nsparse models ( <1% of experts activated).\\nHowever, reﬂecting on the speciﬁc hardware system can further guide this choice. The compute-to-\\nmemory ratio (operational intensity) can serve as an estimate of the efﬁciency of different operations\\n(Williams et al., 2009; Shazeer, 2019). A model is memory bound if the time to load tensors to\\nthe computing core (e.g. ALU/MMU) greatly exceeds the time required to do the computation on\\nthe tensors. On modern GPUs and TPUs, increasing this compute to memory ratio improves the\\nefﬁciency.\\nReturning to sparse expert models, using more than one expert per core increases memory transfer,\\npotentially hurting efﬁciency. Increasing the number of experts does not change the computation\\ndone (sparse models apply a ﬁxed amount of computation to each input), but increases the mem-\\nory transfer requirement (additional expert variables must be loaded from device memory). This\\ndecreases the compute-to-memory ratio9.\\nOn our TPU system, we recommend to one expert (or less) per core. Our largest models use both data\\nand model parallelism where data parallelism is over “rows” and model-parallelism over “columns”\\nof the logical mesh. We use \\x141 expert per data parallelism row to ensure the compute-to-memory\\nratio is high and to reduce the cores needed for evaluation and inference. Furthermore, using less\\nexperts lets us allocate more cores to the model parallelism “column” to have more FLOPs in our\\nmodel. Appendix H explains our mesh layouts for when we have fewer experts than data parallelism\\nrows.\\n5.2 C HOOSING THE CAPACITY FACTOR AND ROUTING ALGORITHM\\nWe generalize top-1 routing (Fedus et al., 2021; Roller et al., 2021) and top-2 (Shazeer et al., 2017;\\nLepikhin et al., 2020) to study top- nrouting where each token is processed by at most nexperts. In\\nthis study, all models are pre-trained for 100k steps with 1M tokens per batch and sparse models have\\n32 experts and are FLOP matched to T5-Large Raffel et al. (2019). We draw two key conclusions.\\nFirst, increasing both the train and eval capacity factors (CF) improves quality as seen by comparing\\nacross the segmented blocks of Table 8. For instance, top-1 routing improves by +0.011 neg. log\\nperp. when increasing from 1.0 !1.25 train CF and top-2 routing improves +0.009 increasing\\nfrom 1.25!2.0 train CF. To provide context for these numbers: tripling the size of a dense model\\n(Dense-L to Dense-XL) yields a +0.090 neg. log perp. boost. Therefore, these CF boosts are \\x18\\n1=10thof that magnitude. But this comes at a cost. Increasing the capacity factor linearly increases\\nthe einsums costs, memory for activations, all2all communication costs, and model-parallelism\\nallreduce communication costs for expert layers10.\\nSecond, there are small gains of top- (n+1) over top-nrouting given a ﬁxed capacity factor (Table 8).\\nFor instance, top-2 routing improves +0.004 over top-1 at train CF of 1.25 or about 1=20ththe boost\\nof a dense model tripling. This revises an earlier recommendation from Fedus et al. (2021). The\\nprimary difference between these experimental setups was scale of compute. Fedus et al. (2021)\\ntrained 220M-FLOP matched models for 50B tokens. We ﬁnd at an 8x larger scale of training\\n(1B-FLOP matched models for 100B tokens) there is instead a small gain to route to more than\\none expert. Furthermore, at the larger experimental scale, the speed difference of top- nversus top-\\n(n+ 1) routing is negligible. Speed differences were observed in Fedus et al. (2021) because the\\nrouter computation was a larger fraction of the total model computation.\\n9As an exercise to the reader, verify the operational intensity of the ﬁrst expert computation isb\\x01h\\nb+h\\x01ewithb\\nbatch size, hhidden dimension, enumber of experts.\\n10all2all andallreduce costs depend on the number of devices, batch size, dmodel and capacity factor,\\nbut not on the number of experts.\\n14\\nAlgorithm Train CF Eval CF Neg. Log Perp. (\")\\nDense-L — — -1.474\\nDense-XL — — -1.384\\nTop-1 0.75 0.75 -1.428\\nTop-1 0.75 2.0 -1.404\\nTop-2 0.75 0.75 -1.424\\nTop-2 0.75 2.0 -1.402\\nTop-1 1.0 1.0 -1.397\\nTop-1 1.0 2.0 -1.384\\nTop-2 1.0 1.0 -1.392\\nTop-2 1.0 2.0 -1.378\\nTop-1 1.25 1.25 -1.378\\nTop-1 1.25 2.0 -1.373\\nTop-2 1.25 1.25 -1.375\\nTop-2 1.25 2.0 -1.369\\nTop-2 2.0 2.0 -1.360\\nTop-2 2.0 3.0 -1.359\\nTop-3 2.0 2.0 -1.360\\nTop-3 2.0 3.0 -1.356\\nTable 8: Comparing capacity factors (CF) and routing algorithms. Increasing both train and eval\\nCF improves performance. Increasing or decreasing the eval CF gives an additional lever if you have\\nmore or less compute at eval time. Next, there are smaller gains of top- (n+ 1) over top-nrouting\\nacross capacity factors. Because the quality improves, but the speed slows as the CF increases, the\\nPareto efﬁcient CF must be determined by the speciﬁc hardware system.\\nThe speciﬁc hardware-software system will determine the optimal nand capacity factor. For in-\\nstance, if the system supports fast all2all andallreduce communications, larger capacity\\nfactors and larger nin top-nrouting may be optimal. However, if the all2all and/or allreduce\\ncommunications are slow, smaller capacity factors may dominate. In our case, the hardware-\\nsoftware stack is the TPU and Mesh Tensorﬂow. We record the training speed of both our ST-MoE-L\\nand ST-MoE-32B model in Table 9 as we increase the train capacity factor. As the models scale,\\na higher capacity factor makes the models increasingly slower. The ST-MoE-L does not require\\nmodel parallelism (it ﬁts within accelerators memory, which implies no additional allreduce\\ncommunications) making it better suited for high capacity factors than our ST-MoE-32B model. For\\nour largest model, we therefore continue to use the smaller train capacity factor of 1.25 advocated\\nby Fedus et al. (2021) for Pareto efﬁciency, differing from other work which use a larger and more\\nexpensive 2.0 capacity factor (Lepikhin et al., 2020; Du et al., 2021).\\nModel Train CF Step Time (s) ( #)\\nST-MoE-L 1.25 2.397\\nST-MoE-L 2.0 2.447 ( +7%)\\nST-MoE-32B 1.25 4.244\\nST-MoE-32B 2.0 4.819 ( +14%)\\nTable 9: Proﬁling sparse models on TPUs. Increasing the train capacity factor from 1.25 to 2.0\\nincreases the step-time by +7% for the large (1B) model but by +14% for our 32B model. As the\\nmodel size increases, we ﬁnd the small quality gains of higher train capacity factors from Table 8\\nare more than offset by the signiﬁcant 14% slow-down. Note: the step time between ST-MoE-L and\\nST-MoE-32B are not comparable because they used a different number of cores.\\nOur results in this section focus on top- nrouting, but we also experimented with a variety of other\\nrouting techniques in Appendix J. We found most performed similarity or worse compared to top- n\\nrouting. However we found Batch Prioritized Routing (BPR), introduced in Riquelme et al. (2021),\\nsigniﬁcantly helps performance for capacity factors less than one (Appendix D). We recommend\\n15\\nBPR for larger models where all2all andallreduce are more expensive and lower capacity\\nfactors are optimal.\\n6 E XPERIMENTAL RESULTS\\nGiven our improvements to training stability, ﬁne-tuning and model design, we start by validating\\na sparse model approximately FLOP-matched to T5-Large (Raffel et al., 2019). We conclude this\\nsection by designing and training a 269B sparse parameter model (FLOP matched to a 32B dense\\nmodel) which achieves state-of-the-art quality across a wide set of NLP tasks.\\nWe studied the SuperGLUE (Wang et al., 2019) benchmark throughout this work which consists\\nof tasks including sentiment analysis (SST-2), word sense disambiguation (WIC), sentence similar-\\nity (MRPC, STS-B, QQP), natural language inference (MNLI, QNLI, RTE, CB), question answer-\\ning (MultiRC, RECORD, BoolQ), coreference resolution (WNLI, WSC) and sentence completion\\n(COPA) and sentence acceptability (CoLA). We often observe good performance on SuperGLUE to\\ncorrelate with (but not guarantee) performance across many NLP tasks. We also include a divers set\\nof additional benchmarks. The CNN-DM (Hermann et al., 2015) and BBC XSum (Narayan et al.,\\n2018) datasets are used to measure the ability to summarize articles. Question answering is probed\\nwith the SQuAD dataset (Rajpurkar et al., 2016) as well as on grade-school science questions in\\nARC Easy and ARC Reasoning Challenge (Clark et al., 2018). And as in Roberts et al. (2020), we\\nevaluate the knowledge of our models by ﬁne-tuning on three closed-book question answer datasets:\\nNatural Questions (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013) and Trivia QA\\n(Joshi et al., 2017). Closed-book simply refers to questions posed with no supplemental reference or\\ncontext material. To gauge the model’s common sense reasoning we evaluate it on the Winogrande\\nSchema Challenge (Sakaguchi et al., 2020). And ﬁnally, we test our model’s natural language infer-\\nence capabilities on the Adversarial NLI Benchmark (Nie et al., 2019).\\n6.1 ST-M OE-L\\nFor simplicity and to cover dozens of tasks easily, we train on mixtures of the tasks listed rather than\\nseparately ﬁne-tuning a model on each task. However, because the tasks vary in size considerably,\\nequally sampling per the number of examples would over-sample large tasks and under-sample small\\nones. We therefore mix each task in proportion to the number of examples in its ‘train’ split (up to\\nsome maxnumexamples=65536 ) as in Raffel et al. (2019). This means that tasks containing\\nmore than 65536 training examples are weighted as if they only contain maxnumexamples .\\nTable 10 summarizes the quality of a dense T5-Large (L) model and sparse model with approxi-\\nmately the same number of FLOPs pre-trained for 500k steps with a 1M batch size (524B tokens)\\non the C4 dataset (Raffel et al., 2019). The sequence length for the encoder was 512 and 114 for\\nthe decoder. We observe improvements on the validation (dev) sets across a wide array of tasks ex-\\namining natural language understanding, question answering, and summarization. As seen in Fedus\\net al. (2021), striking gains are observed in closed book question answering (Roberts et al., 2020).\\nAlso, in support of the overﬁtting hypothesis presented in Section 4.1, we observe two of the smallest\\ntasks CB and WSC (250 and 259 training examples, respectively), are the only ones where the sparse\\nmodel does not yield gains over its dense counterpart. This again suggests that improved forms of\\nregularization for sparse models may unleash greater performance.\\n6.2 ST-M OE-32B\\nWith quality validated at the scale of T5-Large, we seek to push the capabilities of sparse models\\nthrough the ST-MoE-32B. When designing this, we sought a balance between FLOPs and parame-\\nters. High-FLOP sparse models were previously unstable in Fedus et al. (2021) in our setting (i.e.\\nencoder-decoder models, Adafactor optimizer), but the router z-loss enabled us to proceed. For com-\\nputational efﬁciency, we expanded the hidden size of the experts ( dffin Table 11 below)11. Finally,\\nwe increased the dkvto 128 for better performance on our hardware. The most salient changes are\\nfewer overall parameters and more FLOPs per token relative to both Switch-C and Switch-XXL. Our\\n11allreduce activation communications introduced through model parallelism are independent of the\\nhidden size, but not the model dimension, making it a good choice to increase.\\n16\\nName Metric Split Dense-L ( \") ST-MoE-L (\") Gain (%)\\nSQuADv2 F1 dev 94.0 94.5 +1%\\nSQuADv2 acc dev 87.6 88.1 +1%\\nSuperGLUE avg dev 85.1 87.4 +3%\\nBoolQ acc dev 87.1 88.6 +2%\\nCopa acc dev 83.0 91.0 +10%\\nRTE acc dev 91.0 92.1 +1%\\nWiC acc dev 70.4 74.0 +5%\\nMultiRC F1 dev 83.9 86.0 +3%\\nWSC acc dev 95.2 93.3\\x002%\\nReCoRD acc dev 85.7 88.9 +4%\\nCB acc dev 100 98.2\\x002%\\nXSum ROUGE-2 dev 19.9 21.8 +10%\\nCNN-DM ROUGE-2 dev 20.3 20.7 +2%\\nWinoGrande (XL) acc dev 75.4 81.7 +8%\\nANLI (R3) acc dev 54.3 57.3 +6%\\nARC-Easy acc dev 63.5 75.4 +19%\\nARC-Challenge acc dev 50.2 56.9 +13%\\nClosed Book TriviaQA acc dev 28.1 33.8 +20%\\nClosed Book NatQA acc dev 27.2 29.5 +8%\\nClosed Book WebQA acc dev 30.5 33.2 +9%\\nTable 10: Fine-tuning performance of FLOP-matched dense and sparse models. Comparison\\nof the dense-L baseline and the sparse FLOP-matched version (higher numbers better). We observe\\nconsistent gains across diverse tasks, using approximately the same amount of computation. The\\nonly two tasks without improvement from the sparse model are the two smallest: CB with 250\\ntraining examples and WSC with 259.\\nST-MoE-32B has “only” 269B parameters and is approximately FLOP-matched to a dense Trans-\\nformer with 32B parameters. The reduced parameter count from Switch-C and Switch-XXL eases\\nthe burden for both serving and ﬁne-tuning. Finally, we use the sparse-dense stacking described in\\nAppendix C.\\nWe pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the\\ndataset from GLaM (Du et al., 2021) summarized in Appendix E. We use 1M tokens per batch,\\nthe Adafactor optimizer with default hyperparameters, and a learning rate warm-up of 10k steps\\nfollowed by inverse square root decay. Our model follows the initialization scheme proposed in\\nFedus et al. (2021).\\nTable 12 evaluates our ST-MoE-32B model against previous state-of-the-art approaches using\\ninference-only (zero-shot, one-shot) as well as ﬁne-tuning. On SuperGLUE, our model improves\\nupon the prior state-of-the-art model, achieving an average score of 91.2 on the test server (93.2\\nvalidation accuracy) which is over one percentage point beyond estimated human capability. For\\nboth summarization datasets, XSum and CNN-DM, our model achieves state-of-the-art without ad-\\nditional changes to training or ﬁne-tuning (Raffel et al., 2019; Liang et al., 2021). ST-MoE-32B\\nimproves the current state-of-the-art on the test server submissions for both ARC Easy (92.7 !\\n94.8) and ARC Challenge (81.4 !86.5). On two of the three closed book QA tasks, we improve\\nover the prior state-of-the-art. Closed book WebQA achieves a 47.4 accuracy (prior best of 42.8 from\\nRoberts et al. (2020) and exceeds results from the zero-shot performance of the ERNIE 3.0 Titan\\n260B dense parameter model (Wang et al., 2021)). Closed book NatQA improves to 41.9 accuracy\\n(prior best of 41.5 from Karpukhin et al. (2020)). We ﬁnd signiﬁcant improvements on adversari-\\nally constructed datasets (ANLI R3 and WinoGrande XL). ANLI R3 (Nie et al., 2019) improves the\\nstate-of-the-art to 74.7 (prior best of 53.4).\\nWe note some weaknesses in our model. ST-MoE-32B has lackluster performance on the small\\nSQuAD dataset, with an exact match score of 90.8 which falls short of the older benchmark set by\\nthe T5-XXL of 91.3. Furthermore, while setting a new state-of-the-art for SuperGLUE in aggregate,\\n17\\nModel Parameters FLOPs/seq dmodel FFN GEGLU dffdkv\\nDense-L 0.8B 645B 1024 X 2816 64\\nT5-XXL 11.1B 6.3T 4096 X 10240 64\\nSwitch-XXL 395B 6.3T 4096 X 10240 64\\nSwitch-C 1571B 890B 2080 6144 64\\nST-MoE-L 4.1B 645B 1024 X 2816 64\\nST-MoE-32B 269B 20.2T 5120 X 20480 128\\nModel Num. Heads Num. Layers Num. Experts Expert Layer Freq. Sparse-Dense\\nDense-L 16 27 – –\\nT5-XXL 64 24 – –\\nSwitch-XXL 64 24 64 1=4\\nSwitch-C 32 15 2048 1=1\\nST-MoE-L 16 27 32 1=4 X\\nST-MoE-32B 64 27 64 1=4 X\\nTable 11: Model comparisons. A comparison of the Dense-L and T5-XXL, the two largest Switch\\nTransformer variants (Switch-XXL and Switch-C), and the ST-MoE-L and ST-MoE-32B. dmodel\\nrefers to the model hiddenstate size and dffis the internal size of the FFN layer. dkvis the dimension\\nof each attention head. Expert Layer Freq. is the fraction of FFN layers replaced with a sparse layer.\\nSparse-Dense refers to the architectural variant described in Appendix C.\\nPrevious Best (\") Ours ( \")\\nName Metric Split Zero-Shot One-Shot Fine-Tune Fine-Tune\\nSQuADv2 F1 dev 68.3e70.0e96:2a96.3\\nSQuADv2 acc dev 62.1e64.6e91.3a90.8\\nSuperGLUE avg test – – 90.9 91.2\\nBoolQ acc dev/test 83.0e82.8e92.0 92.4\\nCopa acc dev/test 91.0d92.0e98.2 99.2\\nRTE acc dev/test 68.8e71.5e94.1 93.5\\nWiC acc dev/test 50.5e52.7e77.9 77.7\\nMultiRC F1 dev/test 72.9d72.9d88.6 89.6\\nWSC acc dev/test 84.9e83.9e97.3 96.6\\nReCoRD acc dev/test 90.3e90.8e96.4 95.1\\nCB acc dev/test 46.4d73.2e99.2 98.0\\nXSum ROUGE-2 test – – 24.6h27.1\\nCNN-DM ROUGE-2 test – – 21.6a21.7\\nWinoGrande XL acc dev 73.4e73.2d– 96.1\\nANLI R3 acc test 40.9e40.8e53.4 74.7\\nARC-Easy acc test 71.9e76.6e92.7g95.2\\nARC-Challenge acc test 51.4 53.2 81.4g86.5\\nCB TriviaQA em dev 68.0e74.8e61.6b62.3\\nCB NatQA em test 21.5e23.9e41:5c41.9\\nCB WebQA em test 38.0f25.3 42:8b47.4\\nTable 12: ST-MoE-32B versus previous best for inference-only techniques and ﬁne-tuned mod-\\nels.A split of “dev/test” refers to dev split for Zero-Shot and One-Shot and test split for Fine-Tune\\nquality. Data not available ﬁlled in with “–”. Superscript letters denote the result:a: Raffel et al.\\n(2019)b: Roberts et al. (2020)c: Karpukhin et al. (2020),d: Brown et al. (2020),e: Du et al. (2021),\\nf: Wang et al. (2021),g: UniﬁedQA + ARC MC/DA + IR,h: Zhang et al. (2020).\\n18\\ncertain tasks, including small ones like CB, WSC, fail to improve. Finally, on closed book Trivia\\nQA, our model improves over the ﬁne-tuned baseline with SSM from Roberts et al. (2020), but fails\\nto produce gains over both GPT-3 and GLAM.\\nWhile not the focus of this paper, we present the quality differential between recent advances in\\ninference-only techniques like few-shot learning and ﬁne-tuning on these tasks (GPT-3 (Brown\\net al., 2020), GLAM (Du et al., 2021) and Gopher (Rae et al., 2021)). As expected and observed\\npreviously, ﬁne-tuning outperforms zero/one-shot learning, but has the disadvantage of requiring\\nadditional training and different models for each task.\\n7 T RACING TOKENS THROUGH THE MODEL\\nThus far we have presented quantitative measures and performance metrics. We change tack to\\nexplore qualitative features by visualizing how tokens are routed among the experts. We do so by\\npassing a batch of tokens to the model and manually inspecting token assignment at each layer. We\\nconsider our ST-MoE-L model pre-trained either on the monolingual C4 corpus (Raffel et al., 2019)\\nor on the multilingual mC4 corpus (Xue et al., 2020). On both the encoder and the decoder, the\\nmodel has six sparse layers, each with 32 experts.\\nPreliminaries\\nThe span corruption objective is to recover spans of variable-length contiguous segments\\nmasked out in the inputs. This is formatted as:\\nInputs: I went to<extra id0>to buy<extra id1>\\nTargets:<extra id0>the store<extra id1>milk\\nIn our encoder-decoder architecture, the inputs will be passed to the encoder and targets to\\nthe decoder.\\nEach group of tokens is routed jointly with load balancing across experts incentivized by an auxiliary\\nloss as proposed in Shazeer et al. (2017) (see Appendix A for details). Tokens compete for expert\\nassignment against other tokens in their group, rather than the entire batch, and expert specialization\\nis heavily inﬂuenced by the distribution of tokens in each group. The notion of groups is introduced\\nto limit the cost of dispatching and gathering the correct tokens to the correct experts.\\n7.1 E NCODER EXPERTS EXHIBIT SPECIALIZATION\\nOur ﬁrst observation is that, at each layer, at least one expert specializes in sentinel tokens (mask to-\\nkens that represent blanks to ﬁll-in). Additionally, some encoder experts exhibit clear specialization,\\nwith some experts primarily operating on punctuation, verbs, proper names, counting, etc. Table 13\\npresents a few notable example of specialization across encoder experts. And while we ﬁnd many\\ninstances of specialization, these have been speciﬁcally extracted from many examples without a\\nclear semantic or syntactic specialization.\\n7.2 D ECODER EXPERTS LACK SPECIALIZATION\\nIn contrast, expert specialization is far less noticeable in the decoder . Not only are sentinel to-\\nkens routed somewhat uniformly across decoder experts (see Table 14), but we also do not observe\\nmeaningful specialization (semantics or syntax) in decoder experts.\\nWe hypothesize that this lack of meaningful expert specialization is caused by the distribution of\\ntarget tokens induced by the span corruption objective . In particular, (a)a smaller number of tokens\\nare routed jointly in the decoder due to longer sequence lengths in the encoder (e.g. group size\\nis 2048 in the encoder vs 456 in the decoder in our setup) and (b)a higher proportion of tokens\\nare sentinel tokens in the decoder. As a result, target tokens in each group typically cover a smaller\\nsemantic space (compared to the encoder), perhaps explaining the lack of expert specialization in the\\ndecoder. This intricate interplay between the architecture and the training objective invites further\\n19\\nExpert specialization Expert position Routed tokens\\nSentinel tokens Layer 1 been<extra id4><extra id7>ﬂoral to\\n<extra id10><extra id12><extra id15>\\n<extra id17><extra id18><extra id19>...\\nLayer 4 <extra id0><extra id1><extra id2>\\n<extra id4><extra id6><extra id7>\\n<extra id12><extra id13><extra id14>...\\nLayer 6 <extra id0><extra id4><extra id5>\\n<extra id6><extra id7><extra id14>\\n<extra id16><extra id17><extra id18>...\\nPunctuation Layer 2 , , , , , , , , , - , , , , , ). )\\nLayer 6 , , , , , : . : , & , & & ? & - , , ? , , , . <extra id27>\\nConjunctions and articles Layer 3 The the the the the the the the the The the the\\nthe the the The the the the\\nLayer 6 a and and and and and and and or and a and .\\nthe the if ? a designed does been is not\\nVerbs Layer 1 died falling identiﬁed fell closed left posted lost felt\\nleft said read miss place struggling falling signed died\\nfalling designed based disagree submitted develop\\nVisual descriptions Layer 0 her over her know dark upper dark outer\\ncolor, spatial position center upper blue inner yellow raw mama\\nbright bright over open your dark blue\\nProper names Layer 1 A Mart Gr Mart Kent Med Cor Tri Ca Mart\\nR Mart Lorraine Colin Ken Sam Ken Gr Angel A\\nDou Now Ga GT Q Ga C Ko C Ko Ga G\\nCounting and numbers Layer 1 after 37 19. 6. 27 I I Seven 25 4, 54 I two dead we\\nwritten and numerical forms Some 2012 who we few lower each\\nTable 13: Notable examples of specialization in encoder experts . We ﬁnd experts that specialize\\nin punctuation, conjunctions & articles, verbs, visual descriptions, proper names, counting & num-\\nbers. Across all layers (not shown), we observe experts that primarily operate on sentinel tokens\\n(marked as<extra idx>). Note that a SentencePiece model (Kudo and Richardson, 2018) will\\nsplit a token if it doesn’t exist in the vocabulary, e.g. Kenneth may become Ken, ne, th.\\nresearch on better leveraging sparsity and expert specialization in the decoder. Alternatively, future\\nwork could study simply removing the experts in the decoder layer, which also confers beneﬁts\\nduring autoregressive decoding (Kudugunta et al., 2021a).\\nLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Uniform (32-experts)\\nEncoder 2.2 1.8 1.6 1.7 1.7 1.2 3.5\\nDecoder 3.4 3.4 3.4 3.4 3.4 3.4 3.5\\nTable 14: Entropy of routed sentinel tokens across encoder and decoder layers. We support our\\nqualitative observation that encoder experts specialize, but decoder expert don’t by computing the\\nentropy over the routing for sentinel tokens. The encoder routing entropy is low, but the decoder\\nrouter is high entropy, and nearly equal to uniform routing. Because each layer has 32-experts, a\\ncompletely uniform distribution has entropy of 3.5.\\n20\\n7.3 M ULTILINGUAL EXPERTS SPECIALIZE , BUTNOT BY LANGUAGE\\nWe next consider a multilingual sparse model pretrained on a mixture of different languages and\\ninspect the expert specialization in the encoder. As in the monolingual case, we ﬁnd strong evidence\\nof expert specialization. Table 15 presents some examples of experts specializing in sentinel tokens,\\nnumbers, conjunctions & articles and proper names.\\nExpert specialization Routed tokens\\nSentinel tokens to<extra id6>to til<extra id9>\\n<extra id10>to<extra id14><extra id17>\\n<extra id19><extra id20><extra id21>...\\nNumbers $50 comment .10.2016 ! 20 20 3 ! 5 1. ! 91 ? n ´e ?\\n2 17 4 17 11 17 8 & 11 & 22:30 02 2016. ) iOS\\nConjunctions & Articles of of of their their of any this this your your am von\\nthis of Do of of This these our 的的于的在的在的\\nle les Le la di la sur sur 136 sur ののするのというのし\\nPrepositions & Conjunctions For for or for for or for from because https during https\\n并与和par c Pour `a a par tr `e pour pour pour pour pour c とやのに\\nでででなので - and and + c between and and\\nProper names Life Apple iOS A IGT 众莫HB\\nF HB A K A OPP OK HB A Gia C Gia C P Scand Wi\\nG H Z PC G Z ハイ PC G Ti CPU PC PC A キット OS\\nTable 15: Examples of specialization in multilingual experts (encoder) . Multilingual ex-\\nperts also exhibit specialization, which sometimes spans across different languages (e.g. ”for” and\\n”pour”). Experts trained on multilingual mixtures do not exhibit language specialization.\\nOne might expect experts to specialize in languages, which appears as a natural criterion for divvying\\nup batches of data among experts. However, we ﬁnd no evidence of language specialization (see\\nTable 15). Routers instead pass tokens from English, Japanese, French and Chinese indiscriminately\\nand the experts appear to be multilingual . But this lack of language specialization is less surprising\\nwhen considering the mechanism of token routing and load balancing. Since each group of tokens\\nmay only contain one, to at most a few, languages (a group usually consists of 2-4 sequences in\\nour setup), then all experts are encouraged to handle tokens from all languages. We experimented\\nwith a global load balance loss, however, this usually results in worse load-balance and worse model\\nperformance, so we leave further improving multilingual expert models as an area of open work\\n(Section 9).\\nOur visualization reveals apparent specialization learned in our models (Tables 13, 15) for the en-\\ncoder layers. Other expert specializations were also observed in the appendix of Shazeer et al.\\n(2017). However, this leads to an interesting question of how architectures that eliminate learned\\nrouting Roller et al. (2021); Zuo et al. (2021) appear to perform well. An extensive study of the\\nscaling properties of learned versus random routing could prove helpful as future work and help\\nguide us to a better understanding of routing behavior.\\n8 R ELATED WORK\\nMixture-of-Experts (MoE) date back at least three decade history to the work of Jacobs et al. (1991);\\nJordan and Jacobs (1994). In initial concepts, the MoE deﬁned the entire neural network akin to\\nensemble methods. But later Eigen et al. (2013) extended the idea of including MoE as a component\\nas part of deeper networks. Shazeer et al. (2017) then scaled this idea to a 137B parameter model to\\nachieve state-of-the-art in machine translation. Most of the later work (including ours) follows this\\nMoE as a component approach.\\nScale in natural language processing. The remarkable success of scale in natural language pro-\\ncessing (Kaplan et al., 2020; Brown et al., 2020) has reinvigorated MoE research evidenced by a\\n21\\nsurge of recent work (Lepikhin et al., 2020; Fedus et al., 2021; Yang et al., 2021; Kim et al., 2021;\\nDu et al., 2021; Artetxe et al., 2021; Zuo et al., 2021; Clark et al., 2022). Sparse expert models\\nhave been proposed as a method to achieve the results of large-scale dense models, more efﬁciently.\\nFedus et al. (2021) showed a 4x pre-train speed-up over T5-XXL (Raffel et al., 2019) and Du et al.\\n(2021) matched the quality of GPT-3 (Brown et al., 2020) using only 1=3of the energy. And in the\\nspan of the last twelve months, a milestone of efﬁciently training trillion parameter deep neural net-\\nworks has been achieved by multiple groups (Fedus et al., 2021; Yang et al., 2021; Du et al., 2021),\\nand most recently, Lin et al. (2021) introduced techniques to train a 10T parameter model. One side\\nnote is that the recent signiﬁcant successes of sparse expert models have often been in settings with\\na lot of data and no distribution shift – two examples being language modeling/span corruption and\\nmachine translation (Shazeer et al., 2017; Lepikhin et al., 2020; Kim et al., 2021; Fedus et al., 2021).\\nIn contrast, discrepancies between strong pre-training quality and poor ﬁne-tuning quality for sparse\\nmodels have been observed in Fedus et al. (2021); Narang et al. (2021); Artetxe et al. (2021), but we\\nexpect advances in regularization techniques to continue to improve downstream quality.\\nTowards better routing algorithms. BASE layers (Lewis et al., 2021) recasts token routing as a\\nlinear assignment problem – removing the need for load balancing auxiliary losses. This work also\\ndemonstrated the efﬁcacy of a single expert layer. Clark et al. (2022) studies in depth the scaling\\nproperties of a few different routing algorithms and propose their own variant of BASE layers that\\nuses an optimal transport formulation. Yang et al. (2021) introduces the M6-T architecture and\\nexpert prototyping which splits experts into different groups and applies ktop-1 routing procedures\\n(contrasting with the top- krouting commonly used elsewhere). Hazimeh et al. (2021) proposed a\\ncontinuously differentiable sparse gate with demonstrated improvements over vanilla top- kgating.\\nOther work (Bengio et al., 2016) considered casting the routing selection as a reinforcement learning\\nproblem. More radical versions remove learning the routing entirely. Hash layers (Roller et al.,\\n2021) shows random ﬁxed routing (per hash functions) led to competitive performance with learned\\nrouting. Zuo et al. (2021) also proposed an algorithm which randomly selects experts during training\\nand inference and found gains of 2 BLEU points over Switch Transformers and competitive scores\\nwith the larger models of Kim et al. (2021). Finally, Fan et al. (2021) designs an architecture with\\nexplicit language-speciﬁc sublayers (rather than allowing arbitrary routing as done in Lepikhin et al.\\n(2020)) to yield gains of +1 BLEU.\\nSparse expert models in other modalities. MoE and sparse experts model have also advanced\\nresults in modalities aside from language. Riquelme et al. (2021) designed a 15B parameter V-MoE\\nto match state-of-the-art ImageNet (Deng et al., 2009) models with fewer computational resources.\\nLou et al. (2021) similarly showed a beneﬁt over dense vision models by using MoE layers across\\nboth image patch and channel dimensions. Additionally, Automatic Speech Recognition has been\\nimproved by the SpeechMoE variants (You et al., 2021a;b). Kumatani et al. (2021) reduced word\\nerror rates using MoE models in Sequence-to-Sequence Transformer and Transformer Transducer.\\nImproving deployment of sparse models. Initial expert designs (including this work) route each\\ntoken separately to experts at that layer. One issue is that these type of architectures may be burden-\\nsome to serve since it requires sufﬁcient memory for storing the parameters. Distillation was shown\\nin Fedus et al. (2021) to be moderately effective, but recent approaches modiﬁed the routing to in-\\nstead route full sentences or tasks (Kudugunta et al., 2021b; Zuo et al., 2021) which then permits\\nextraction of sub-networks at time of serving (e.g. deploy only the network associated with the new\\ntask). As an alternative to distillation, Kim et al. (2021) considers directly pruning away experts not\\nessential to the task of interest.\\nMultitask learning with MoE. We conclude our tour of recent MoE research with successes in\\nmultitask settings. Ma et al. (2018) recommended using a separate gating or router network for each\\ntask, an idea that may soon be revisited for Transformer architectures. Finally, Gururangan et al.\\n(2021) recommends even greater modularity of language models and conditionally activates experts\\nbased on the domain/task label or by an inferred label.\\n9 D ISCUSSION\\nWhile this work is on sparse models, these models intersect with many other interesting topics\\nin machine learning such as adaptive computation, low-precision training, scaling principles, and\\n22\\nneural network architecture advances. Our discussion therefore covers a broader range of topics\\nsurfaced during this research.\\nUnpredictable dynamics when pre-training on multilingual data. We often observe that the\\nsame model pre-trained on multilingual data will yield smaller pre-training speed-ups and be more\\nunstable. One hypothesis is that this is due to the variance of sequences per group across batches.\\nAs a reminder, we encourage tokens in a group to be load-balanced. There are usually only 2-8\\nsequences per group (higher becomes expensive) where each sequence is written in a single lan-\\nguage. Therefore, at most 2-8 languages must be balanced across experts – even when training with\\nover 100 languages. This leads to high variance across groups and batches, resulting in chaotic and\\nunpredictable routing. In a follow-up experiment (just highlighted for brevity), we pre-trained on\\na mixture of English C4 plus a small fraction of a ﬁne-tuning task which similarly resulted in an\\nunstable model.\\nThe robustness of sparse models. Despite a paper focused on the details of sparse model-\\nparticulars, zooming out we ﬁnd them to be robust to a wide set of hyperparameters and architectural\\nchanges. Sparse models obtain great performance under a variety of routing algorithms, dropping\\nhigh fractions of tokens, and different hyperparameters. While we did point out the importance of\\ntuning the batch size and learning rate for ﬁne-tuning, our intuition, in-line with Kaplan et al. (2020),\\nis that the real winner is scale. For instance, Table 8 shows larger gains to be had by simply increas-\\ning the capacity factor (i.e. FLOPs) rather than by more sophisticated routing (i.e. algorithms).\\nAdaptive computation. Sparse models are a subclass of adaptive computation models since each\\ninput gets different computation applied to it. In sparse models a token is routed to the expert(s)\\nof its choosing. When capacity factors are less than one, the model learns to not apply computa-\\ntion to certain tokens. This has shown promise in computer vision (Riquelme et al., 2021) and our\\nlanguage experiments (Appendix D). We envision future models expanding this through heteroge-\\nneous experts (e.g. each expert applies differing computation). Intuitively, different input examples\\nwill likely require different amounts of processing depending on difﬁculty. Future models in this\\ndirection will be efﬁciently enabled through emerging computing infrastructures (Dean, 2021).\\nGeneralizing ﬁndings from small to large scale. A key issue we faced throughout our work was\\nidentifying small scale models and training setups that reﬂect larger scale experiments. This was\\nevident in our stability studies in Section 3 where experiments had to be run with XL sized models\\nto surface relevant dynamics. For our architecture and routing algorithm experiments, we often ﬁnd\\nimprovements vanish, or even reverse, when models are trained for longer or made larger. As one\\nexample, the top- nﬁndings of Fedus et al. (2021) were reversed in our 8x larger-scale experiments\\npresented here, which revealed small boosts of top- (n+ 1) routing over top- nrouting (see Table 8).\\nTraining models with even lower precision. The best method we found to stabilize our models\\nwithout hurting (and sometimes improving) quality was the router z-loss. This is an auxiliary loss\\nthat encourages the model logits to have values smaller in absolute magnitude. Given the max range\\nof numbers float32 andbfloat16 can support (\\x183e38), this leads us to believe most of this\\nrange is not needed, and compressing it actually might improve model training dynamics. Therefore,\\nfuture precision formats might take into account more compressed exponential ranges to train certain\\nclasses of models.\\nDesigning new operations with more multiplicative interactions. Section 3.1 shows that op-\\nerations with more multiplicative interactions than additions, or those that don’t accumulate over\\nmany numbers, improve model performance. We test this further by injecting more multiplicative\\ninteractions into expert layers which speedup pre-training by 4% without any change to step-time\\n(Appendix C). We think this hints at promising architectural improvements for models and could be a\\ngood design principle. Recently depthwise convolutions, which only accumulate 3-5 elements, have\\nalso been shown to greatly improve Transformer performance (So et al., 2021). These operations\\nare especially exciting as elementwise multiplications typically do not introduce any communication\\noverhead when using model parallelism (which makes operations like depthwise convolutions and\\nour multiplicative interactions very efﬁcient). While we did note these methods to increase model\\ninstabilities in Section 3.1, using the router z-loss in our models prevented any further instabilities.\\n23\\nConstrain activations to alleviate other undesirable model scaling dynamics. We observed\\ntwo additional sources of training instability. (1)Encoder-decoder models are more unstable than\\ndecoder only models (for ﬁxed amount of FLOPs). Encoder-decoder models have a higher ratio\\nof attention layers (e.g. more exponential functions) due to having both self-attention and enc-dec\\nattention layers for each FFN on the decoder. (2)Deeper models are more unstable than shallower\\nmodels for a ﬁxed amount of FLOPs. Deeper models also introduce more exponential functions\\nthrough additional attention layers. We hypothesize that a contributing factor to both of these obser-\\nvations is simply the increased number of exponential functions found in the network. Future work\\ncould look at resolving these training dynamics by adding z-loss penalties to the attention softmaxes\\nfor non-sparse models, especially since we observed adding them didn’t change model quality.\\nDense and sparse models depend differently on hyperparameters. Our ﬁne-tuning analysis\\nin Section 4.3 shows optimal ﬁne-tuning hyperparameters differ signiﬁcantly between dense and\\nsparse models. In certain settings, ﬁne-tuning hyperparamters that worked well for the dense model\\nmasked any improvements from the sparse model (despite large pre-training speedups). For new\\nmodel classes, we recommend researchers and practitioners to extensively test key hyperparameters\\nbefore prematurely abandoning a method.\\n10 C ONCLUSION\\nWe temper the over-exuberance for scale in Fedus et al. (2021) by showing how a model with 1/5th\\nthe size, but with a better balance of computation (FLOPs) to parameters – is a more effective sparse\\nlearner. Furthermore, this improves the usability of sparse models since it can be deployed with less\\nmemory overhead. Using our sparse model variant, we achieve SOTA across a wide range of the\\nmost competitive public benchmarks. We hope this work shows the power of model sparsity and\\naccelerates the adoption of such models.\\nACKNOWLEDGEMENTS\\nWe would like to thank Alex Passos, Ekin Cubuk, Margaret Li, Noah Constant, Oriol Vinyals,\\nBasil Mustafa, Joan Puigcerver, Diego de Las Casas, Mike Lewis, and Ryan Sepassi for detailed\\ncomments and feedback on early versions of the draft. We also thank the Google Brain Team for\\nuseful discussions throughout the course of this work.\\n24\\nREFERENCES\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria\\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui\\nChen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo,\\nJeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. Efﬁcient large\\nscale language modeling with mixtures of experts, 2021.\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation\\nin neural networks for faster models, 2016.\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural\\nlanguage processing , pages 1533–1544, 2013.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv preprint arXiv:2005.14165 , 2020.\\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,\\nBogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Uniﬁed scaling laws for\\nrouted language models. arXiv preprint arXiv:2202.01169 , 2022.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457 , 2018.\\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\\ngated convolutional networks. In International conference on machine learning , pages 933–941.\\nPMLR, 2017.\\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: In-\\nvestigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung ,\\nvolume 23, pages 107–124, 2019.\\nJeff Dean. Introducing pathways: A next-generation ai architecture. Google AI Blog , 2021.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,\\npages 248–255. Ieee, 2009.\\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise\\nquantization, 2021.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,\\nZongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy\\nMeier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen,\\nand Claire Cui. Glam: Efﬁcient scaling of language models with mixture-of-experts, 2021.\\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\\nmixture of experts. arXiv preprint arXiv:1312.4314 , 2013.\\n25\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Man-\\ndeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond english-centric\\nmultilingual machine translation. Journal of Machine Learning Research , 22(107):1–48, 2021.\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: Better text generation via ﬁlling in\\nthe.arXiv preprint arXiv:1801.07736 , 2018.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961 , 2021.\\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. Demix\\nlayers: Disentangling domains for modular language modeling, 2021.\\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\\nRahul Mazumder, Lichan Hong, and Ed H. Chi. Dselect-k: Differentiable selection in the mixture\\nof experts with applications to multi-task learning, 2021.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition, 2015.\\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\\narXiv:1606.08415 , 2016.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.\\nIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\\nin Neural Information Processing Systems , volume 28, pages 1693–1701. Curran Asso-\\nciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\\nafdec7005cc9f14302cd0474fd0f3c96-Paper.pdf .\\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\\n1735–1780, 1997.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp.\\nInInternational Conference on Machine Learning , pages 2790–2799. PMLR, 2019.\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. In International conference on machine learning , pages 448–\\n456. PMLR, 2015.\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\\nlocal experts. Neural computation , 3(1):79–87, 1991.\\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.\\nNeural computation , 6(2):181–214, 1994.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361 , 2020.\\nVladimir Karpukhin, Barlas Ouz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen tau Yih. Dense passage retrieval for open-domain question answering, 2020.\\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu,\\nAmr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efﬁ-\\ncient moe training for multitask multilingual models, 2021.\\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 , 2018.\\n26\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efﬁcient inference.\\narXiv preprint arXiv:2110.03742 , 2021a.\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efﬁcient inference.\\narXiv preprint arXiv:2110.03742 , 2021b.\\nKenichi Kumatani, Robert Gmyr, Felipe Cruz Salinas, Linquan Liu, Wei Zuo, Devang Patel, Eric\\nSun, and Yu Shi. Building a great multi-lingual teacher with sparsely-gated mixture of experts for\\nspeech recognition, 2021.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 7:453–466, 2019.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt\\ntuning. arXiv preprint arXiv:2104.08691 , 2021.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\\nSimplifying training of large, sparse models. arXiv preprint arXiv:2103.16716 , 2021.\\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv\\npreprint arXiv:2101.00190 , 2021.\\nXiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, and\\nTie-Yan Liu. R-drop: Regularized dropout for neural networks, 2021.\\nJunyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang,\\nYong Li, Wei Lin, Jingren Zhou, and Hongxia Yang. M6-10t: A sharing-delinking paradigm for\\nefﬁcient multi-trillion parameter pretraining, 2021.\\nYuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. Sparse-mlp: A fully-mlp architecture\\nwith conditional computation, 2021.\\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relation-\\nships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining , pages 1930–1939,\\n2018.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\\ntraining. arXiv preprint arXiv:1710.03740 , 2017.\\nVinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.\\nInIcml, 2010.\\nSharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Kar-\\nishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modiﬁcations\\ntransfer across implementations and applications? arXiv preprint arXiv:2102.11972 , 2021.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum-\\nmary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint\\narXiv:1808.08745 , 2018.\\nArvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and\\nJames Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint\\narXiv:1511.06807 , 2015.\\n27\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversar-\\nial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599 ,\\n2019.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\\ninstructions with human feedback. 2022.\\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\\nnetworks. In International conference on machine learning , pages 1310–1318. PMLR, 2013.\\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,\\nDavid So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv\\npreprint arXiv:2104.10350 , 2021.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kun-\\ncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Men-\\nsch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yu-\\njia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Au-\\nrelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training go-\\npher, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683 , 2019.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250 , 2016.\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr ´e Su-\\nsano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\\narXiv preprint arXiv:2106.05974 , 2021.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\\nparameters of a language model? arXiv preprint arXiv:2002.08910 , 2020.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse\\nmodels. arXiv preprint arXiv:2106.04426 , 2021.\\nSascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. A simple\\nrecipe for multilingual grammatical error correction. arXiv preprint arXiv:2106.03830 , 2021.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\\nsarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence , volume 34, pages 8732–8740, 2020.\\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\\ntraining of deep neural networks. Advances in neural information processing systems , 29:901–\\n909, 2016.\\nNoam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\\narXiv:1911.02150 , 2019.\\n28\\nNoam Shazeer. Glu variants improve transformer, 2020.\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\\nInInternational Conference on Machine Learning , pages 4596–4604. PMLR, 2018.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538 , 2017.\\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep\\nlearning for supercomputers. In Advances in Neural Information Processing Systems , pages\\n10414–10423, 2018.\\nSam Shleifer, Jason Weston, and Myle Ott. Normformer: Improved transformer pretraining with\\nextra normalization. arXiv preprint arXiv:2110.09456 , 2021.\\nDavid R So, Wojciech Ma ´nke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Primer:\\nSearching for efﬁcient transformers for language modeling. arXiv preprint arXiv:2109.08668 ,\\n2021.\\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\\nDropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning\\nResearch , 15(1):1929–1958, 2014. URL http://www.cs.toronto.edu/ ˜rsalakhu/\\npapers/srivastava14a.pdf .\\nNassim Nicholas Taleb. Antifragile: Things that gain from disorder , volume 3. Random House\\nIncorporated, 2012.\\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan\\nNarang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efﬁciently: Insights from\\npre-training and ﬁne-tuning transformers. arXiv preprint arXiv:2109.10686 , 2021.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\\nprocessing systems , pages 5998–6008, 2017.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language\\nunderstanding systems. In Advances in Neural Information Processing Systems , pages 3266–\\n3280, 2019.\\nShuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Jun-\\nyuan Shang, Yanbin Zhao, Chao Pang, et al. Ernie 3.0 titan: Exploring larger-scale knowledge en-\\nhanced pre-training for language understanding and generation. arXiv preprint arXiv:2112.12731 ,\\n2021.\\nSamuel Williams, Andrew Waterman, and David Patterson. Rooﬂine: an insightful visual perfor-\\nmance model for multicore architectures. Communications of the ACM , 52(4):65–76, 2009.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-\\nlation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\\nBarua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv\\npreprint arXiv:2010.11934 , 2020.\\nAn Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jia-\\nmang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, and Hongxia Yang. M6-t:\\nExploring sparse expert models and beyond, 2021.\\n29\\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe: Scaling to large acoustic models with\\ndynamic routing mixture of experts, 2021a.\\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe2: Mixture-of-experts model with im-\\nproved routing, 2021b.\\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. arXiv preprint\\narXiv:1910.07467 , 2019.\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with extracted\\ngap-sentences for abstractive summarization, 2020.\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\\narXiv preprint arXiv:1810.12885 , 2018.\\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and\\nJianfeng Gao. Taming sparsely activated transformer with stochastic experts, 2021.\\n30\\nA T OKEN LOAD BALANCE DESCRIPTION\\nThe auxiliary load balancing loss from Shazeer et al. (2017) is also used to here to balance tokens\\nacross experts. Assume we have Nexperts indexed by i= 1toNand a batchBwithTtokens. The\\nauxiliary loss is computed as the scaled dot-product between vectors f and P,\\nloss=\\x0b\\x01N\\x01NX\\ni=1fi\\x01Pi (7)\\nwherefiis the fraction of tokens dispatched to expert i,\\nfi=1\\nTX\\nx2B1fargmaxp(x);ig (8)\\nandPiis the fraction of the router probability allocated for expert i,2\\nPi=1\\nTX\\nx2Bpi(x) (9)\\nSince we seek uniform routing of the batch of tokens across the Nexperts, we desire both vectors\\nto have values of 1=N. The auxiliary loss of Equation 7 encourages uniform routing since it is\\nminimized under a uniform distribution. The objective can also be differentiated as the P-vector is\\ndifferentiable, but the f-vector is not. The ﬁnal loss is multiplied by expert count Nto keep the loss\\nconstant as the number of experts varies since under uniform routingPN\\n1(fi\\x01Pi) =PN\\n1(1\\nN\\x011\\nN) =\\n1\\nN. Finally, a hyperparameter \\x0bis a multiplicative coefﬁcient for these auxiliary losses; throughout\\nthis work we use an \\x0b= 10\\x002which was sufﬁciently large to ensure load balancing while small\\nenough to not to overwhelm the primary cross-entropy objective.\\nB R OUTER Z-L OSSTRAINING DYNAMICS\\nFigure 7 plots the router z-loss from Equation 5 across a coefﬁcient sweep where the best value of\\ncz= 0:001is plotted in green for the encoder and decoder.\\n0 25000 50000 75000 100000 125000 150000\\nStep0510152025303540Z-Loss0.0\\n1e-4\\n1e-3\\n1e-2\\n1e-1\\n0 25000 50000 75000 100000 125000 150000\\nStep0102030405060Z-Loss0.0\\n1e-4\\n1e-3\\n1e-2\\n1e-1\\nFigure 7: Sweeping loss coefﬁcient ( cz) for Router Z-Loss. We plot the router z-losses over the\\ncourse of pre-training without router z-loss (blue) and with increasing values of cz(we selected\\ncoefﬁcient associated with green curve for all later experiments). With values of 1e-2 , or larger,\\nthe z-loss shrinks near to zero. The left plot shows an encoder layer and the right plot shows a\\ndecoder layer.\\n2A potential source of confusion: pi(x)is the probability of routing token xto expert i.Piis the probability\\nfraction to expert iacross all tokens in the batchB.\\n31\\nC I MPROVED ARCHITECTURAL MODIFICATIONS\\nWe consider a few small architecture variations here. The ﬁrst modiﬁcation was adding additional\\nFFN layers (feed-forward network, see Table 1 for more details) immediately before or after each\\nMoE layer (referred to as Sparse-Dense). Table 16 reveals the effectiveness of an FFN layer im-\\nmediately preceding or following each sparse layer and that these extra FFN layers help less when\\nadded elsewhere in the network. Guaranteeing all tokens have at least one FFN applied to them\\nbetween each attention layer appears useful.\\nModel Neg. Log Perp. (\") \\x01\\nDense model (baseline) -1.474 -\\nDense model w/ extra FFN layers -1.452 0.022\\nSparse model (baseline) -1.383 -\\nSparse model w/ extra FFN layer after each sparse layer -1.369 0.014\\nSparse model w/ extra FFN layer before each sparse layer -1.369 0.014\\nSparse model w/ extra FNN layers placed randomly in the network -1.376 0.007\\nTable 16: A dense FFN immediately before or after each sparse layer improves quality . Insert-\\ning an extra dense FFN immediately before or after each sparse layer improves quality 2x as much\\nas placing the dense layers (randomly) elsewhere in the network. All of the non-baseline models\\nhave the same amount of FFN layers added for fair comparisons. Note that improving perplexity\\nbecomes harder as the model gets better.\\nSecond, we introduce an additional bias in the expert layers. All our models use the GELU-Linear\\nFFN (Shazeer, 2020), rather than the ReLU FFN:\\nFFN ReLU(x) = ( ReLU (xW 1))W2\\nFFN GEGLU (x) = ( GELU (xW 11)\\x0cxW 12)W2\\nThe additive bias is a learned weight ( B) added after the ﬁrst matrix multiplication in the FFN layer\\nof shape [batch;d ff]. The multiplicative bias (also referred to as a scale parameter) is a learned\\nweight of the same shape, but does an elementwise multiplication. We initialize the additive bias to\\nzeros and the multiplicative bias to ones.\\nFFN GEGLU +Add Bias (x) = [( GELU (xW 11)\\x0cxW 12) +B]W2\\nFFN GEGLU +Mult Bias (x) = [( GELU (xW 11)\\x0cxW 12)\\x0cB]W2\\nTable 17 shows the results of our different methods. Both the additive and multiplicative biases are\\nessentially free: cheap to compute, adds few new parameters, and incurs no additional communi-\\ncation costs with model and expert parallelism. When using our router z-loss from Section 3.1, we\\nobserve no instabilities from the multiplicative bias. We do see that the multiplicative interactions\\nimprove performance, achieving a 4% speedup in convergence time over our strong sparse baseline.\\nThis hints that a promising avenue for future architectural research is ﬁnding new ways of adding\\nmore multiplicative interactions into networks.\\nModel Neg. Log. Perp. (\") \\x01\\nDense Baseline -1.474 -\\nSparse Baseline -1.369 -\\nSparse +Additive Bias -1.371 -0.002\\nSparse +Multiplicative Bias -1.361 0.008\\nTable 17: More multiplicative interactions improve sparse model quality. Both the additive and\\nthe multiplicative bias add virtually no parameters or compute.\\nFinally, motivated by the work of Roller et al. (2021), we explored similar methods, but did not ﬁnd\\nimprovements in our setting. We tried routing using the word embedding exclusively, as well as\\n32\\nan additional input to the layer embedding for routing decisions. We toggled stopping the gradient\\nthrough the word embedding or allowing it to have gradients propagated from the router. Using only\\nthe word embedding hurt quality, while using it in addition to the normal layer hidden activation was\\ninitially positive, but after pre-training for 50B+ tokens on models of scale 1B+ dense parameters it\\nhad a neutral effect. Appendix J has further details on the experiments with negative results.\\nD B ATCH PRIORITIZED ROUTING FOR LOWER CAPACITY FACTORS\\nSurprisingly, top-1 and top-2 routing work well with CF less than 1.0 despite token routing being\\ndone in a left to right order over the sequence. If Ntokens are sent to an expert with only Mspaces\\nthenN >M tokens will dropped. The ordering of the dropping is important: we drop tokens going\\nleft to right (e.g. tokens earlier in the sentence will be routed ﬁrst over the end tokens). This is done\\nto avoid the model cheating. If we dropped tokens in another ordering, the model gets information\\non what tokens are occurring later in the sequence based on if tokens are being dropped or not.\\nBatch Prioritized Routing (BPR) from Riquelme et al. (2021) was introduced in Vision Transformers\\n(Dosovitskiy et al., 2020) for image classiﬁcation. Our work explores BPR with top-1 routing in the\\ncontext of language modeling. BPR aims to have a global view of all tokens to determine which\\ntokens should be dropped instead of the left-to-right ordering. The algorithm works by looking at\\nall N tokens getting sent to Expert iand then only routing the Mones with the highest probabilities\\nfrom the router. Table 18 shows that BPR top-1 routing improves performance over top-2 routing,\\nespecially when capacity factors are less than 1.0. We leave it to future work to try top- nBPR\\nrouting, which will hopefully yield larger improvments for higher capacity factors.\\nImportantly, BPR routing can only be done on the encoder side of the encoder-decoder model. On\\nthe encoder side there are not autoregressive predictions and all tokens can see each other. If you use\\nBPR on the decoder, it learns to cheat by using future token information to improve current token\\npredictions.\\n33\\nAlgorithm Train CF Eval CF Neg. Log. Perp. (\")\\nDense — — -1.474\\nDense-L — — -1.384\\nBPR Top-1 0.5 0.5 -1.433\\nBPR Top-1 0.5 2.0 -1.416\\nTop-1 0.75 0.75 -1.428\\nTop-1 0.75 2.0 -1.404\\nTop-2 0.75 0.75 -1.424\\nTop-2 0.75 2.0 -1.402\\nBPR Top-1 0.75 0.75 -1.409\\nBPR Top-1 0.75 2.0 -1.397\\nTop-1 1.0 1.0 -1.397\\nTop-1 1.0 2.0 -1.384\\nTop-2 1.0 1.0 -1.392\\nTop-2 1.0 2.0 -1.378\\nBPR Top-1 1.0 1.0 -1.386\\nBPR Top-1 1.0 2.0 -1.379\\nTop-1 1.25 1.25 -1.378\\nTop-1 1.25 2.0 -1.373\\nTop-2 1.25 1.25 -1.375\\nTop-2 1.25 2.0 -1.369\\nBPR Top-1 1.25 1.25 -1.376\\nBPR Top-1 1.25 2.0 -1.375\\nTable 18: Batch Prioritized Top-1 Routing (BPR) performance. BPR top-1 routing improves\\nquality when capacity factors are \\x141. However, once the capacity factor reaches 1.25, the improve-\\nments greatly diminish and it underperforms top-2 routing. Future work can try BPR with top-2\\nrouting, which should hopefully further improve the performance.\\nE P RE-TRAINING DATASET DETAILS\\nThe pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and\\nthe dataset introduced in GLaM (Du et al., 2021).\\nDataset Tokens (B) Weight in Mixture\\nFiltered C4 183 0.17\\nFiltered Webpages 143 0.34\\nWikipedia 3 0.05\\nConversations 174 0.23\\nForums 247 0.02\\nBooks 390 0.17\\nNews 650 0.02\\nTable 19: Data and mixture weights in the training set . We sample from different dataset sources\\nwith probability proportional to “weight in mixture”. The number of tokens listed are in billions (B).\\nFor more details on the C4 corpus see Raffel et al. (2019) and for the other datasets see Du et al.\\n(2021).\\n34\\nF F ULL FINE-TUNING SENSITIVITY DATA\\nTable 20 contains the raw data for Figure 6 measuring the ﬁne-tuning protocol sensitivity. Dense\\nand Sparse are encoder-decoder models FLOP matched to T5-Large that were pre-trained for 500k\\nsteps with a batch size of 1M tokens on the C4 corpus.\\nModel Learning Rate Batch Size Reset Optimizer Slot Vars SuperGLUE (\")\\nDense 1e-3 1M 84.8\\nDense 1e-3 1M X 84.3\\nDense 5e-4 1M 84.8\\nDense 5e-4 1M X 84.2\\nDense 1e-4 1M 84.0\\nDense 1e-4 1M X 84.8\\nDense 1e-3 262k 84.9\\nDense 1e-3 262k X 83.7\\nDense 5e-4 262k 84.9\\nDense 5e-4 262k X 84.0\\nDense 1e-4 262k 85.1\\nDense 1e-4 262k X 85.0\\nDense 1e-3 65k 83.7\\nDense 1e-3 65k X 82.5\\nDense 5e-4 65k 84.4\\nDense 5e-4 65k X 84.1\\nDense 1e-4 65k 84.9\\nDense 1e-4 65k X 84.6\\nSparse 1e-3 1M 86.9\\nSparse 1e-3 1M X 85.9\\nSparse 5e-4 1M 86.1\\nSparse 5e-4 1M X 83.5\\nSparse 1e-4 1M 84.3\\nSparse 1e-4 1M X 84.3\\nSparse 1e-3 262k 86.2\\nSparse 1e-3 262k X 85.2\\nSparse 5e-4 262k 85.5\\nSparse 5e-4 262k X 84.8\\nSparse 1e-4 262k 85.1\\nSparse 1e-4 262k X 85.5\\nSparse 1e-3 65k 85.8\\nSparse 1e-3 65k X 85.5\\nSparse 5e-4 65k 86.5\\nSparse 5e-4 65k X 85.1\\nSparse 1e-4 65k 85.6\\nSparse 1e-4 65k X 84.5\\nTable 20: Fine-tuning protocol sensitivity. We vary the batch size, learning rate and whether we\\nreset the optimizer slot variables for both dense and sparse models. Resetting the optimizer state\\nduring ﬁne-tuning hurts performance. We observe a difference in optimal batch size and learning\\nrate for sparse vs. dense models. Certain hyperparameter ﬁne-tuning settings make the sparse and\\ndense models perform almost exactly the same, showing the importance of correctly tuning the\\nhyperparameters.\\n35\\nG O PTIMALLY SETTING THE ROUTING THRESHOLD\\nTop-nRouting Algorithm\\n1.Route each token xto the expert with the highest router probability (gate1(x)).\\n2.Normalize the top- nexpert router scores for each token x, so gatei=gatei(x)Pn\\ni=1gatei(x).\\n3.Route the token to the other n-1 experts (indexed by i) with probability min (1:0;gatei(x)\\nthreshold).\\nThreshold is a predeﬁned hyperparameter that is typically set to 0.2.\\nWe describe the MoE hyperparameters and how they should change as the routing algorithm\\nchanges. The MoE top-2 routing algorithm (Shazeer et al., 2017; 2018; Lepikhin et al., 2020) works\\nas follows: ﬁrst the router ﬁnds the expert that is assigned the higher router score (gate1) and always\\nsends the token to that expert. The token is also sent to its second highest expert with probability\\nmin(1:0;gate2=threshold ). The threshold is a hyperparameter that is typically set to 0.2, and gate2is\\nthe token’s router probability for the second highest expert. Note that gate1and gate2get normalized\\nby the sum of their two scores, so they sum to one.\\nWe trivially extend the top-2 algorithm to work for top- nrouting here. Take the scores of the top- n\\nexperts per token and sum them, then renormalize each expert router score based on that sum. If the\\nspeciﬁc renormalized expert score has a higher value than the threshold (e.g. 0.2), then the token will\\nbe routed, otherwise it will be routed with probabilityscore\\nthreshold. At a high level this only routes the\\ntoken to the next n-1 experts if their scores are not too much lower than the highest scored expert.\\nFor top-3 routing vs top-2, the sum that the expert scores are normalized by is larger, therefore\\nwe experimented with decreasing the threshold. Our experimental results are shown in Table 21.\\nInterestingly, we do observe the top-3 routing to slightly beneﬁt from the lower threshold, while the\\nopposite is true for top-2 routing.\\nWe also experimented with an absolute threshold policy instead of a relative one. This is where the\\nnextn-1 tokens will be routed only if their router score is great than some pre-deﬁned value (e.g.\\n0.2). We found it can achieve as good of performance if the threshold value is tuned.\\nAlgorithm Train CF Threshold Neg. Log. Perp. (\")\\nDense — — -1.474\\nDense-L — — -1.384\\nTop-2 3.0 0.2 -1.354\\nTop-2 3.0 0.05 -1.356\\nTop-3 3.0 0.2 -1.351\\nTop-3 3.0 0.05 -1.349\\nTable 21: Performance of top-2 and top-3 routing with different thresholds. Top-3 routing does\\nslightly better with lower thresholds than top-2 routing.\\nH M ESH LAYOUT FOR DATA, MODEL AND EXPERT PARALLELISM WITH\\nFEWEXPERTS\\nWe use data and model parallelism partitioning with Mesh-Tensorﬂow (Shazeer et al., 2018). The\\npartitioning strategy works by ﬁrst forming a logical 2D mesh of size dxm, with the rows corre-\\nsponding to the data dimension ( d) and the columns as the model dimension ( m) and the product\\nequal to the total number of cores, n=dxm. This mesh is only an abstraction. Each logical core\\nmust be mapped to a physical core, which is optimized through performance tuning.\\nAs a refresher, each row in the mesh will have its own unique slice of the data and each column will\\nhave a unique slice of the model weights. The ﬁnal gradient allreduce communication occurs\\nacross each individual column. The model parallelism allreduce communications occur across\\neach row in the mesh. One constraint from this approach is that the number of rows must evenly\\n36\\nModel DimensionData Dimension\\nOuter Data DimensionInner Data Dimension\\nModel DimensionExperts >= Data Dimension Experts < Data DimensionFigure 8: Data and model parallelism meshes used for distributing models. In this example there\\nare a total of 32 processors (e.g. n= 32 ).(Left) A valid 2D mesh if the number of experts is greater\\nthan or equal to the data parallelism dimension. The data dimension has 8 rows ( d) and the model\\ndimension has 4 columns ( m).(Right) A valid 3D mesh when we have fewer experts than the data\\nparallelism dimension. The batch dimension is factorized into two new dimensions: inner data and\\nouter data dimensions. Now we have 1 expert per inner data dimension ( i). The 8 data rows in the\\nleft ﬁgure become 4 in the outer batch ( o) and 2 in the inner batch ( i) with 2 experts instead of 8.\\ndivide the number of data sequences and the number of columns must evenly divide the model\\ndimensions being partitioned.\\nBut if we have fewer thandexperts then this layout will not work. To allow for fewer experts than\\ndata parallelism rows in our mesh, we factorize the data dimension into two new dimensions: inner\\n(i) and outer ( o) whereixo=dand the number of experts equals i. This transforms the logical 2D\\nmesh of shape dxminto a 3D mesh of shape oxixm. See Figure 8 for a visualization of both\\nmeshes12.\\nI N OTE ON COMMUNICATION COSTS FOR DISTRIBUTED MODELS\\nCommunication operations ( allreduce andall2all ) can signiﬁcantly impact sparse model\\ntraining throughput (see Table 1 for a description of the communication operations). allreduce\\ncalls are executed along model and batch dimensions, typically dominated by the model dimension\\nallreduce calls that sum results of partial matrix multiplication operations from the workers.\\nThese calls are needed when matrix multiplications are partitioned across multiple cores (e.g. model\\nparallelism). The gradient summation allreduce calls can be amortized away by training models\\nwith larger batch sizes since the gradient accumulation allreduce communication cost is inde-\\npendent of the batch size. To alleviate the memory issues of larger batch sizes, microbatches can\\nbe used. Microbatches do this by splitting the batch into nevenly divisible chunks and computing\\ngradients on each sequentially, then summing.\\nTo increase the allreduce throughput, more workers may need to be assigned to the model di-\\nmension (instead of batch dimension). However, increasing the number of workers may reduce\\ncompute per worker resulting in higher communication overheads that cancel some of the gains\\nfrom higher communication throughput from allreduce . For the results in this paper, ﬁrst we\\nexplored various model partitioning strategies. Next the shapes of the pre-training jobs were al-\\nlocated based on performance benchmarking which showed the lowest cumulative communication\\noverheads in allreduce andall2all .\\n12See Mesh Tensorﬂow for more details on the inner and outer batch: https://github.com/\\ntensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py\\n37\\nJ N EGATIVE RESULTS\\nWe conclude with some ideas that yielded negative results in our setting.\\nAdding information if tokens were dropped to the router. We experimented with having the\\nexpert layer have information of whether the token was routed or dropped in the previous expert\\nlayers. We implemented this through counting the number of times a token was routed in all pre-\\nvious expert layers, having embeddings for each possible value and then adding this to the router\\nembedding. We found that this made no difference in performance.\\nAdding explicit expert positional information. We experimented with adding explicit positional\\ninformation into the outputs of the expert layer. We wanted to see if it either improved performance\\nor sped up convergence during the beginning of training when expert layers were drastically chang-\\ning. We did this through adding an embedding corresponding to what expert each token was sent\\n(including an embedding if the token was dropped), but this did not improve performance.\\nAdding pre-training noise to ﬁx pre-training and ﬁne-tuning discrepancies. To help ﬁx the\\npre-training perplexity and ﬁne-tuning gap we tried pre-training the sparse models with a variety of\\ndifferent types of noise. The goal was to help pre-training match the ﬁne-tuning conditions where\\ndropout is used and more tokens can be dropped. Some of the noise types we tried adding during\\npre-training were dropout, dropping out full experts for a batch of tokens, and adding an entropy\\nmaximization auxiliary loss to the router. Unfortunately, all of the methods either hurt the pre-\\ntraining quality too much or didn’t end up helping the ﬁne-tuning.\\nLoad balancing in top-n routing over lower n-1 experts. In the standard top- nMoE formaliza-\\ntion there is only loading balancing over the top expert a token is sent to. We experimented with\\nadding an auxiliary load balancing term to the other n\\x001experts in top- nrouting, but found this to\\nprovide minimal beneﬁts.\\nMixing pre-training and ﬁne-tuning data to prevent overﬁtting. To help combat the overﬁtting\\nof sparse models during ﬁne-tuning, we tried mixing in pre-training span corruption data at varying\\namounts (e.g. 1%, 5%, 25%, ...) during ﬁne-tuning. This ended up not helping the ﬁne-tuning\\nperformance, but did increase the training loss.\\n38'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss.\n",
      "B ROUTER Z-LOSS TRAINING DYNAMICS\n",
      "The router z-loss is a regularization term added to the total loss to encourage the model logits\n",
      "to have values smaller in absolute magnitude. The z-loss is computed as the scaled dot-product\n",
      "between the vector of logits and the vector of ones,\n",
      "Lz(x) =1\n",
      "BBX\n",
      "i=10\n",
      "@logNX\n",
      "j=1ex(i)\n",
      "j1\n",
      "A2\n",
      "(10)\n",
      "whereBis the number of tokens, Nis the number of experts, and x2RB\u0002Nare the logits going\n",
      "into the router. The z-loss is multiplied by a hyperparameter czto weight it as part of the total\n",
      "loss optimized.\n",
      "The z-loss is added to the total loss optimized,\n",
      "Ltot=LCE+cBLB+cz\n"
     ]
    }
   ],
   "source": [
    "question = \"summarise the document in a few sentences wihtout equations the key objective of this paper: \"\n",
    "prompt =\"<s> [INST] \"+  question + chunks[0][:101000] + \"[/INST]\"\n",
    "\n",
    "inference_params = dict(temperature=0, max_tokens=200, top_k = 50, top_p= 0.95)\n",
    "\n",
    "# Model Predict\n",
    "model_prediction = Model(\"https://clarifai.com/mistralai/completion/models/mixtral-8x7B-Instruct-v0_1\").predict_by_bytes(prompt.encode(), \n",
    "                                                                                                                         input_type=\"text\", \n",
    "                                                                                                                         inference_params=inference_params)\n",
    "\n",
    "print(model_prediction.outputs[0].data.text.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clarifai.client.model import Model\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "clarifai_pat = os.getenv('CLARIFAI_PAT')\n",
    "\n",
    "inference_params = dict(temperature=0, max_tokens=1, top_k = 50, top_p= 0.95)\n",
    "\n",
    "def ask_model(prompt):\n",
    "    inference_params = dict(temperature=0, max_tokens=200, top_k = 50, top_p= 0.95)\n",
    "\n",
    "    # Model Predict\n",
    "    model_prediction = Model(\"https://clarifai.com/mistralai/completion/models/mixtral-8x7B-Instruct-v0_1\").predict_by_bytes(prompt.encode(), \n",
    "                                                                                                                            input_type=\"text\", \n",
    "                                                                                                                            inference_params=inference_params)\n",
    "\n",
    "    return(model_prediction.outputs[0].data.text.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not a doctor, but I can tell you that melanoma is a type of skin cancer that can be serious. It's important to get any suspicious moles or skin lesions checked by a healthcare professional as soon as possible. The level of danger posed by a melanoma can depend on various factors, such as its size, depth, and location, as well as the individual's overall health. Therefore, I would recommend consulting with a healthcare provider to determine the level of danger in your specific situation.\n"
     ]
    }
   ],
   "source": [
    "inference_params = dict(temperature=0, max_tokens=100, top_k = 50, top_p= 0.95)\n",
    "\n",
    "\n",
    "\n",
    "situation = \"I am a boy, 6 year old, and i have a melanoma on my shin. \"\n",
    "question=\" Does this skin lesion is dangerous? (1) Not danger, (2) Moderate danger, (3) High danger, (4) Extreme danger.\"\n",
    "temp = \"<s> [INST] \"+  situation + question + \"[/INST]\" \n",
    "temp = \"<s> [INST] \"+  \"knowing that your are not a doctor,please answer the question\" +situation + question + \"[/INST]\" \n",
    "\n",
    "\n",
    "answer = ask_model(prompt= temp)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3) High danger. Melanoma is a type of skin cancer that can be very serious, especially if it spreads to other parts of the body. It is important to get medical treatment right away if a melanoma is found. The survival rate for melanoma is higher when it is found and treated at an early stage.\n"
     ]
    }
   ],
   "source": [
    "inference_params = dict(temperature=0, max_tokens=100, top_k = 50, top_p= 0.95)\n",
    "\n",
    "\n",
    "\n",
    "situation = \"I am a boy, 6 year old, and i have a melanoma on my shin. \"\n",
    "question=\" Does this skin lesion is dangerous? (1) Not danger, (2) Moderate danger, (3) High danger, (4) Extreme danger.\"\n",
    "temp = \"<s> [INST] \"+  situation + question + \"[/INST]\" \n",
    "temp =  situation + question \n",
    "temp = \"a 6 year old boy has a melanoma on the leg. is it dangerous? answer only with the following options (1) Not danger, (2) Moderate danger, (3) High danger, (4) Extreme danger.\"\n",
    "temp = \"<s> [INST] \"+  temp + \"[/INST]\" \n",
    "\n",
    "\n",
    "answer = ask_model(prompt= temp)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not a doctor, but I can tell you that melanoma is a serious form of skin cancer, and it's unusual for a 6-year-old child to have it. It's important to seek medical attention immediately if you suspect your child has melanoma.\n",
      "\n",
      "Here are the steps you should take:\n",
      "\n",
      "1. Make an appointment with a pediatric dermatologist or a pediatrician as soon as possible.\n",
      "2. Do not try to remove the melanoma yourself.\n",
      "3. Protect the area from the sun to prevent further damage.\n",
      "4. Keep an eye on the melanoma and note any changes in size, shape, or color.\n",
      "5. Provide your child's medical history, including any family history of skin cancer.\n",
      "\n",
      "The doctor will examine the melanoma and may perform a biopsy to confirm the diagnosis. If melanoma is confirmed, the\n"
     ]
    }
   ],
   "source": [
    "temp = \"a 6 year old boy has a melanoma on the leg. what should i do?\"\n",
    "temp = \"<s> [INST] \"+  temp + \"[/INST]\" \n",
    "\n",
    "\n",
    "answer = ask_model(prompt= temp)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A beauty mole, also known as a birthmark or congenital nevus, is a common type of pigmented skin lesion that is present at birth or appears in the first few weeks of life. The majority of beauty moles are harmless and do not require any treatment. However, it is important to monitor the mole for any changes in size, shape, or color, as well as any symptoms such as itching, bleeding, or pain, as these could be signs of melanoma, a type of skin cancer.\n",
      "\n",
      "If the beauty mole is located in an area that is prone to injury or if it is causing the child any discomfort, you may want to consider having it evaluated by a healthcare professional. They can determine if the mole is harmless or if further evaluation or treatment is needed.\n",
      "\n",
      "In general, it is a good idea to perform regular skin self-examinations and to teach your child how to do the same as\n"
     ]
    }
   ],
   "source": [
    "temp = \"a 6 year old boy has a beauty mole on the leg. what should i do?\"\n",
    "temp = \"<s> [INST] \"+  temp + \"[/INST]\" \n",
    "\n",
    "\n",
    "answer = ask_model(prompt= temp)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not a doctor, but I can tell you that a squamous cell carcinoma diagnosis in a 6-year-old boy is extremely rare and concerning. It is crucial to consult with a pediatric oncologist or a dermatologist specializing in pediatric skin cancers as soon as possible. They will be able to provide the appropriate medical advice, treatment options, and follow-up care.\n",
      "\n",
      "In the meantime, ensure the affected area is protected from the sun to prevent further damage. Keep the area clean and avoid touching or scratching it.\n",
      "\n",
      "Please remember that this advice is not a substitute for professional medical care. It is essential to consult with a healthcare professional for accurate information tailored to the child's specific condition.\n"
     ]
    }
   ],
   "source": [
    "inference_params = dict(temperature=0, max_tokens=100, top_k = 50, top_p= 0.95)\n",
    "\n",
    "temp = \"i have a Squamous cell carcinoma . what should i do? Please give a short answer with few key action to do\"\n",
    "\n",
    "temp = \"a 6 year old boy has Squamous cell carcinoma on the leg. what should i do?\"\n",
    "temp = \"<s> [INST] \"+  temp + \"[/INST]\" \n",
    "\n",
    "\n",
    "answer = ask_model(prompt= temp)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
